---
title: "On the Bias of Complete- and Available-Case Meta-Regressions with Missing Covariates"
author: 
  - Jacob M. Schauer, Northwestern University
  - Karina DÃ¬az, Columbia University
  - Jihyun Lee, University of Texas
  - Therese D. Pigott, Georgia State University
csl: ../addons/apa.csl
output:
  bookdown::pdf_document2: 
    fig_caption: yes
    includes:
      in_header: ../addons/style.sty
    toc: false
  bookdown::word_document2: 
    reference_docx: ../addons/styles_word.docx
bibliography: ../addons/cca_references.json
---


# Introduction

Meta-regression is a useful tool for studying important sources of variation between effects in a meta-analysis [@borensteinIntroductionMetaanalysis2009; @tiptonHistoryMetaregressionTechnical2019].
Analyses of these models in the absence of missing data have been studied thoroughly in the literature [e.g.,
@berkeyRandomeffectsRegressionModel1995; @hedgesCombiningIndependentEstimators1983; @hedgesRobustVarianceEstimation2010; @konstantopoulosFixedEffectsVariance2011;
@viechtbauerAccountingHeterogeneityRandomeffects2007]
However, it is common for meta-analytic datasets to be missing data [@pigottReviewMethodsMissing2001]. 
In the context of meta-regression, issues with missing data frequently involve missing covariates [@pigottMissingPredictorsModels2001; @tiptonCurrentPracticesMetaregression2019].

Precisely how to proceed with a meta-regression with missing covariates remains something of an open question.
Statistical guidance suggests that analyses ought to consider the mechanism that causes covariates to be missing [@pigottMissingPredictorsModels2001; @pigottHandlingMissingData2019].
However, it appears that doing so is less common in practice for meta-analyses.
A recent review found that meta-regressions with missing data tend to take one of two strategies [@tiptonCurrentPracticesMetaregression2019]. 
An analyst may conduct a *complete-case analysis* that excludes any effects for which a relevant covariate is missing (i.e., only analyze complete cases). 
However, if there are very few such effects, a common approach is to use *shifting units of analysis*, which we refer to in this article as a *shifting-case analysis* [@cooperResearchSynthesisMetaanalysis2017]. 
Under a shifting-case analysis, analysts fit a series of meta-regression models on subsets of relevant covariates, so that each model selectively omits certain covariates.

Estimators used in both complete-case and shifting units analyses ignore the fact that covariates are missing from the data.
Ignoring missing data can potentially lead to biased estimates [see @littleStatisticalAnalysisMissing2002; @grahamMissingData2012].
Some research has pointed out these issues in meta-analysis [@pigottHandlingMissingData2019].
However, precisely how much bias can arise in a complete- or shifting-case analysis is not immediately clear, nor is there exhaustive guidance on when these analyses produce unbiased estimates.

This article examines the potential bias of complete- and available-case analyses. 
The following section provides a demonstration of these methods on [INSERT DATA SET].
We then introduce a statistical framework for studying bias for incomplete data meta-regressions that incorporates a model for whether or not an data point is observed.
Using this framework, we describe conditions under which complete- and available-case analyses are unbiased.
When these conditions are not met, we derive an approximation for the bias of complete- and available-case analyses using standard models for missingness and examine the magnitude of bias.



# Case Study

[HOLD FOR ILLUSTRATION OF ANALYSIS METHODS WITH REAL DATA]


# Model and Notation

Suppose a meta-analysis involves $k$ effects estimated from studies. 
For the $i$th effect, let $T_i$ be the estimate of the effect parameter $\theta_i$, and let $v_i$ be the estimation error variance of $T_i$. 
Denote a vector of covariates that pertain to $T_i$ as $X_i =[1, X_{i1}, \ldots, X_{ip}]$. Note that the first element of $X_i$ is a 1, which corresponds to an intercept term in a meta-regression model, and that $X_{ij}$ for $j = 1, \ldots p$ corresponds to different covariates.
The meta-regression model can be expressed as:
\begin{equation}
T_i | X_i, v_i, \eta = X_i \beta + u_i + e_i 
\label{eq:full_data_reg}
\end{equation}
Here, $\beta \in \mathbb{R}^{p+1}$ is the vector of regression coefficients. 
The estimation errors $e_i$ are typically assumed to be normally distributed with mean zero and variance $V[e_i] = v_i$, which is true of some effect sizes, and is an accurate large-sample approximation for others [@cooperHandbookResearchSynthesis2019].
The term $u_i$ represents the random effect such that $u_i \perp e_i$ and $V[u_i] = \tau^2$. 

This is the standard random effects meta-regression model, and it is also consistent with subgroup analysis models [@hedgesFixedRandomeffectsModels1998; @cooperHandbookResearchSynthesis2019]. 
The vector $\eta = [\beta, \tau^2]$ refers to the parameters of model. 
Under a fixed-effects model, it is assumed that $\tau^2 = 0$, in which case $\eta = \beta$, and $u_i \equiv 0$.

A common assumption in random effects meta-regression is that the random effects $u_i$ are independent and normally distributed with mean zero and variance $\tau^2$ [@hedgesFixedRandomeffectsModels1998; @hedgesRandomEffectsModel1983; @lairdStatisticalMethodsCombining1990; @viechtbauerBiasEfficiencyMetaanalytic2005]: 
\[
u_i \sim N(0, \tau^2).
\]
In that case, the distribution $p(T | X, v, \eta)$ can be written as
\begin{equation}
p(T_i | X_i, v_i, \eta) = \frac{1}{\sqrt{2\pi(\tau^2 + v_i)}} e^{-\frac{(T_i - X_i\beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full_data_prob}
\end{equation}
Thus, the joint likelihood for all $k$ effects can be wrtiten as:
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = [2\pi(\tau^2 + v_i)]^{-k/2} e^{-\sum_{i=1}^k \frac{(T_i - X_i \beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full_data_prob_vec}
\end{equation}
where $\mathbf{T} \in \mathbb{R}^k$ is the vector of effect estimates, $\mathbf{v} \in \mathbb{R}^k$ is the vector of estimation variances, and $\mathbf{X} \in \mathbb{R}^{k \times (p+1)}$ is the matrix of covariates where each row of $\mathbf{X}$ is simply the row vector $X_i$. 
Note that the functions in both (\ref{eq:full_data_prob}) and (\ref{eq:full_data_prob_vec}) assume that *all* of the $p$ covariates are observed. 
Equation (\ref{eq:full_data_prob_vec}) is referred to as the *complete-data likelihood function* [@gelmanBayesianDataAnalysis2014; @littleStatisticalAnalysisMissing2002].
We note that a meta-regression with no missing data will be accurate if the complete-data model is correctly specified. 
Thus, to illustrate the properties of incomplete data meta-regression, we assume that the complete-data model is correctly specified.

The vector of regression coefficient estimates for the complete-data model when there is no missing data is typically estimated by
\begin{equation}
\hat{\beta} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{T}
\label{eq:betahat_cd}
\end{equation}
Here, $\mathbf{W} = \text{diag}[1/(v_i + \tau^2)]$ is the diagonal matrix of weights.
The covariance matrix of $\hat{\beta}$ is given by
\begin{equation}
V[\hat{\beta}] = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} 
\label{eq:v_betahat_cd}
\end{equation}

This model can be expanded to account for dependent effect sizes by assuming that $T_i \in \mathbb{R}^{k_i}$ is a vector of $k_i$ effects from the same study, $e_i$ is vector of estimation errors, $u_i$ is a vector of random effects, and $e_i + u_i$ has covariance matrix $\Sigma_i$.
In this model, $X_i$ is a matrix of covariates for each effect in $T_i$.
While we focus on results for independent effect sizes in this article, the results hold for dependent effect sizes.

Not all relevant variables may be observed in a meta-analytic dataset.
Let $R_i$ be a vector of response indicators that correspond with effect $i$. 
This article concerns missing covariates, and we assume that $T_i$ and $v_i$ are observed for every effect of interest in a meta-analysis. 
Thus, each element $R_{ij}$ of $R_i$ corresponds to a covariate $X_{ij}$. 
The $R_{ij}$ take a value of either 0 or 1: $R_{ij} = 1$ indicates the corresponding $X_{ij}$ is observed and $R_{ij} = 0$, indicates a that the corresponding $X_{ij}$ is not observed.
Note that $R_i \in \mathcal{R} \equiv \{0,1\}^p$ is a vector of 0s and 1s of length $p$.
For instance, $X_{i2}$ were missing, then $R_{i2} = 0$.

Denote $O = \{(i, j): R_{ij} = 1\}$ as the indices of covariates that are observed and $M = \{(i, j): R_{ij} = 0\}$ be the set of indices for missing covariates. 
Then, the complete-data model can be written as 
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = p(\mathbf{T} | \mathbf{X}_{O}, \mathbf{X}_{M}, \mathbf{v}, \eta). 
\label{eq:full_data_prob_mis}
\end{equation}
Note that the complete-data model depends on entries of $\mathbf{X}_{M}$, which are unobserved. 

When meta-analytic datasets are missing covariates, analyses involve incomplete data. 
In practice, meta-regressions of incomplete data have largely relied on one of two approaches: complete-case analyses and available-case analyses [@pigottHandlingMissingData2019; @pigottMissingPredictorsModels2001; @tiptonCurrentPracticesMetaregression2019; @tiptonHistoryMetaregressionTechnical2019]. 
A complete-case analysis (CCA) includes only effects for which all covariates of interest are observed.
Available-case meta-regressions typically amount to including a subset of relevant covariates that are completely observed [@pigottHandlingMissingData2019; @tiptonCurrentPracticesMetaregression2019]. 
Analyses of this sort have been referred to as *shifting units of analysis* in the meta-analytic literature [@cooperResearchSynthesisMetaanalysis2017]. 
In this article we refer to this analytic approach as a *shifting-case analysis* (SCA).



## Complete-Case Estimators

A common approach in meta-regression with missing covariates is to use a complete-case analysis. 
This approach simply omits rows in the data for which any covariate is missing. 
Thus, this analysis method only uses effects and covariates for which $R_i = [1, \ldots, 1] = \mathbbm{1}$. 

Let $C = \{i : R_i = \mathbbm{1}\}$ index all relevant effects $i$ such that $R_i = \mathbbm{1}$, so that  $\mathbf{X}_C$ is the matrix of covariates such $R_i = \mathbbm{1}$, $\mathbf{T}_C$ is the corresponding subset of effect estimates, and $\mathbf{W}_C$ is the corresponding subset of weights. 
The complete-case analysis estimates coefficients $\beta$ with:
\begin{equation}
\hat{\beta}_C
  = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \mathbf{T}_C
\label{eq:beta_c}
\end{equation}



## Shifting-Case Estimators

When there are multiple covariates of interest, each of which has some missingness, there may only be a few effects for which all covariates of interest are observed. 
When that happens, a complete case analysis can be unfeasible. 
A common solution to this in meta-analysis is to use an available-case analysis [@pigottHandlingMissingData2019].
In practice, an *available-case* meta-regression is often equivalent to a shifting-case analysis, referred to in the literature as *shifting units of analysis* [@cooperResearchSynthesisMetaanalysis2017; @tiptonCurrentPracticesMetaregression2019].

Shifting-case analyses involve fitting multiple regression models, each including a subset of the covariates of interest. 
Sometimes this even takes the form of regressing effect estimates on one covariate at a time [@pigottHandlingMissingData2019]. 
A given regression in a shifting-case analysis conditions on a set of missingness patterns $R \in \mathcal{R}_j$ where $\mathcal{R}_j \subset \mathcal{R}$.
These patterns are such that $R_{ij} = 1$ for those covariates $X_{ij}$ that are observed *and* included in the analyses, but $R_{ik} \in \{0, 1\}$ for covariates $X_{ik}$ that are excluded from the analysis; that is, excluded covariates may be observed or unobserved.

As an example, suppose there are two covariates of interest $X_{i1}$ and $X_{i2}$. 
A shifting-case analysis might first regress $T_i$ on observed values of $X_{i1}$. 
This regression would include observations for which both $X_{i1}$ and $X_{i2}$ are observed (i.e., $R_i = [1, 1]$) and observations for which $X_{i1}$ is observed but $X_{i2}$ is missing (i.e., $R_i = [1, 0])$.
This would imply that the regression involves effects $i$ for which $R_i \in \mathcal{R}_j = \{[1, 1], [1, 0]\}$.

Consider a single regression in a shifting case analysis, and let $S$ index the covariates included in that model $S = \{j : j = 0 \text{ or } X_{ij} \text{ in analysis}\}$ and let $E$ be the complement of $S$.
Note that this model is used to estimate $\beta_S$, which is a subset of the full vector of coefficients $\beta$. 
In the example above, where $T_i$ is regressed on only $X_{i1}$, $\beta_S = [\beta_0, \beta_1]$.
Denote $\mathcal{R}_j$ as the set of missingness patterns such that all included covariates are observed: $\mathcal{R}_j = \{R \in \mathcal{R}: R_S = \mathbbm{1}\}$.
Note that $\mathcal{R}_j$ contains missingness patterns $R$ such that all the included covariates are observed, but any excluded covariates may be either observed or unobserved.
Finally, let $U(S)$ be the set of effects for which $X_{iS}$ are observed $U(S) = \{i : R_i \in \mathcal{R}_j\}$.
Then, the shifting-case estimators for $\beta_S$ are given by:
\begin{equation}
\hat{\beta}_S = 
(\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1} \mathbf{X}_{US}^T \mathbf{W}_U \mathbf{T}_U
\label{eq:beta_s}
\end{equation}
where $\mathbf{X}_{US}$ contains the columns of $\mathbf{X}$ that pertain to the covariates that are included in the model, and the rows for which all of those covariates are observed.
The matrix $\mathbf{W}_U$ contains the rows of $\mathbf{W}$ for which $X_{iS}$ are observed (and similarly for $\mathbf{T}_U$).


## Missingness Mechanisms

Both the complete- and shifting-case estimators are analyses of incomplete data. 
Analyses of incomplete data require some assumption about why data are missing, which is referred to as the missingness *mechanism*.
The mechanism by which missingness arises is typically modeled through the distribution of $R$. 
Let $\psi$ denote the parameter (or vector of parameters) that index the distribution of $R$ so that the probability mass function of $R$ can be written as $p(R | T, X, v, \psi)$.
Assumptions about the missingness mechanism are therefore equivalent to assumptions about $p(R | T, X, v, \psi)$.

@rubinInferenceMissingData1976 defined three types of mechanisms in terms of the distribution of $R$.
Data could be missing completely at random (MCAR), which means that the probability that a given value is missing is independent of all of the observed or unobserved data:
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) \propto \psi
\]
MCAR implies that probability that a given value is missing is unrelated to anything observed or unobserved.

Covariates could be missing at random (MAR), which implies the distribution of missingness depends only on observed data:.
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) = 
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O,\mathbf{v}, \psi)
\]
MAR differs from MCAR in that missingness might be related to observed values. 
As an example, if with larger standard errors are less likely to report the racial composition of their samples, then missingness would depend on the (observed) estimation error variances. 
Data missing according to this mechanism would violate an assumption of MCAR, since missingness is related to an observed value.

Finally, data are said to be missing not at random (MNAR) if the distribution of $R$ depends on unobserved data in some way.
In the context of the meta-regression data, this would imply that $R$ is related to $\mathbf{X}_M$, so that the probability of a covariate not being observed depends on the value of the covariate itself.
For instance, data would be MCAR if studies with larger standard errors and a greater proportion of minorities are less likely to report the racial composition of their samples because the likelihood that racial composition is not reported will depend on the composition itsef.

A related concept in missing data is that of *ignorability*, which means that the missingness pattern does not contribute any additional information.
When missing data are ignorable, it is not necessary to know (or estimate) $\psi$ in order to conduct inference on $\eta$ [@gelmanBayesianDataAnalysis2014; @grahamMissingData2012; @littleStatisticalAnalysisMissing2002; @vanbuurenFlexibleImputationMissing2018].
In practice, missing data are ignorable if they are MAR and if $\psi$ and $\eta$ are distinct.






# Conditional Incomplete Data Meta-Regression


Because both complete- and available-case analyses depend on the value of $R_i$, they can be seen as models that condition on missingness. 
Models that condition on missingness are not necessarily identical to the complete-data model, which is the model of interest, because the complete-data model does not condition on $R_i$.
Yet, complete- and available-case analyses proceed as if the complete-data and conditional models are equivalent. 
Doing so ignores the sources and impacts of missingness, and can lead to inaccurate results.

The complete-data model can be related to the conditional models through the distribution of missingness $R_i$. 
This approach is referred to as a *selection model* in the missing data literature [@gelmanBayesianDataAnalysis2014; @littleStatisticalAnalysisMissing2002; @vanbuurenFlexibleImputationMissing2018].
We can write the selection model for meta-regression with missing covariates as:
\begin{equation}
p(T_i | X_i, v_i, R_i \in \mathcal{R}_j, \eta, \psi) 
  = \frac{p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta)}{p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta)} 
\label{eq:selection_model}
\end{equation}
where $\psi$ indexes the distribution of $R | T, X, v$.
Here, $\mathcal{R}_j$ refers to the relevant subset of $\mathcal{R}$ on which the analysis conditions; for a complete-case analysis, $\mathcal{R}_j = \{\mathbbm{1}\}$.

Equation (\ref{eq:selection_model}) describes the conditional model as a function of the complete-data model $p(T_i | X_i, v_i, \eta)$ and a selection model $p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)$ that gives the probability that a given set of covariates are observed.
The denominator on the right hand side of (\ref{eq:selection_model}) is the probability of observing the missingness pattern $r$ given the estimation error variance $v_i$ and the observed and unobserved covariates in the vector $X_i$, and can be written as
\begin{equation}
p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta) = \int p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta) dT_i
\label{eq:pr_xv}
\end{equation}

Note that when the complete-data model in (\ref{eq:full_data_prob}) is not equivalent to the conditional model in (\ref{eq:selection_model}), the resulting coefficient estimators in a meta-regression can be biased. 
To see this, we can write:
\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] 
  = E[T_i | X_i, v_i] + \delta_{ij} 
  = X_i \beta + \delta_{ij}
\label{eq:bias_delta}
\end{equation}
Here, we wee that the expectation of $T_i$ conditional on $R_i$ can be written as the complete-data expectation $X_i \beta$ plus a bias term $\delta_{ij}$. 
If $\delta_{ij} \neq 0$, it follows that conditioning on $R_i$ induces bias in the distribution of $T_i$ used in an analysis. 
Because the complete-case estimator in (\ref{eq:beta_c}) and the shifting-case estimator in (\ref{eq:beta_s}) are weighted averages of the $T_i$, the following sections show that they can be biased if $\delta_{ij} \neq 0$.
The precise magnitude of the $\delta_{ij}$ will depend on the selection model in (\ref{eq:selection_model}) and hence on the missingness mechanism.

A standard approach for modeling missingness mechanisms for covariates is to assume $R_i$ follows some log-linear distribution [@agrestiCategoricalDataAnalysis2013]. 
Various authors have described approaches to modelling $R$ for missing covariates in generalized linear models that include logistic and multinomial logistic models [@ibrahimIncompleteDataGeneralized1990; @ibrahimMissingCovariatesGeneralized1999; @lipsitzConditionalModelIncomplete1996]. 
Thus one class of models for missingness would involve the logit probability of observing some missingness patterns $R_i \in \mathcal{R}_j \subset \mathcal{R}$:
\begin{equation}
\text{logit}[p(R_i \in \mathcal{R}_j | T_i, X_i, v_i)] = \sum_{m = 0}^{m_j} \psi_{mj} f_{mj}(T_i, X_i, v_i) 
\label{eq:r_loglinear}
\end{equation}
where $f_{0j}(T_i, X_i, v_i) = 1$, so that $\psi_{0j}$ would be the intercept term for the logit model for the set of missingness patterns $\mathcal{R}_j$.
While log-linear models are not the only applicable or appropriate model for missingness, we make this assumption at points throughout this article in order to demonstrate conditions under which conditional meta-regressions are inaccurate, and how inaccurate they can be.



## Approximate Bias for Log-linear Selection Models

As argued above, the bias of complete-case estimators $\hat{\beta}_C$ or shifting-case estimators $\hat{\beta}_S$ will depend in some way on the bias $\delta_{ij}$ induced in $T_i$ by conditioning on $R_i \in \mathcal{R}_j$.
The magnitude and direction of $\delta_{ij}$ will in turn depend on the missingness mechanism.

It is possible to derive an approximation for $\delta_{ij}$ when $R_i | T_i, X_i, v_i$ under certain conditions when $R_i$ follows a log-linear distribution. 
If $p(T_i | X_i, v_i)$ is the standard fixed- or random effects meta-regression model in equation (\ref{eq:full_data_prob}), and $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i) = H_j(T_i, X_i, v_i)$ follows the log-linear model in (\ref{eq:r_loglinear}), then
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional_bias}
\end{equation}
where $H_j(X_i \beta, X_i, v_i)$ is equivalent to $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i)$ evaluated at $T_i = X_i\beta$ and 
\[
f_{mj}'(X_i\beta, X_i, v_i) = \left.\frac{\partial f_{mj}}{\partial T_i}\right\rvert_{T_i = X_i\beta}
\]
is the derivative of $f_{mj}$ with respect to $T_i$ evaluated at $T_i = X_i\beta$.

While the following sections will examine possible values that $\delta_{ij}$ may take under different selection models, we can gain some insight on bias by examining (\ref{eq:conditional_bias}).
The expression for $\delta_{ij}$ depends on three main quantities.
First, $\delta_{ij}$ is an increasing of $H_j(X_i\beta, X_i, v_i)$, which is the probability that $R_i \not\in \mathcal{R}_j$.
This implies that the bias will be greater as the probability of omitting an observation increases.
Second, $\delta_{ij}$ increases in $\tau^2 + v_i$, which means that it will be larger when $T_i$ vary more around the regression line.
Finally, $\delta_{ij}$ depends on $\psi_{mj} f_{mj}'(X\beta, X, v)$. 
Note that $f_{mj}'$ is the derivative of $f_{mj}$ with respect to $T$. 
If $f_{mj}$ does not depend on $T$, $f_{mj}' = 0$. 
Thus, $\delta_{ij}$ depends on the components of the selection model that are functions of $T$ and how important those components are.


<!-- Then an approximation for $E[T | X, v, R = r]$ is as follows: -->
<!-- \begin{align*} -->
<!-- E[T | X, v, R = r]  -->
<!--   & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_j \psi_j f_j(T, X, v)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)} \log\left(A + e^{\sum_j \psi_j f_j(T, X, v)}\right)} dT \\ -->
<!--   & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_j \psi_j f_j(T, X, v) - \log\left(A + e^{\sum_j \psi_j f_j(X\beta, X, v)}\right)\right.}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} \\ -->
<!--   & \qquad\qquad \frac{\left.- G(\sum_{j \in \mathcal{T}} \psi_j f_j(X, v))(T - X\beta) + O(T^2)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} dT \\ -->
<!--   & \approx \int \frac{T \exp\left\{-\frac{1}{2(\tau^2 + v)}\left(T^2 - 2TX\beta - 2T (\tau^2 + v)\sum_{j \in \mathcal{T}} \psi_j f_j(X, v)\right.\right.}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} \\ -->
<!--   & \qquad\qquad \frac{\left.\left.+ 2T(\tau^2 + v) G(\sum_{j \in \mathcal{T}} \psi_j f_j(X, v)) + \ldots\right)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} dT \\ -->
<!--   & = X\beta + (1 - G)(\tau^2 + v)\sum_{j \in \mathcal{T}} \psi_j f_j(X, v) -->
<!-- \end{align*} -->



# Bias in Complete-Case Analyses

Complete-case analyses only include effects for which all relevant covariates are observed. 
The complete-case coefficient estimator $\hat{\beta}_C$ given in equation (\ref{eq:beta_c}) conditions on $R_i = \mathbbm{1}$.
As noted above, conditioning on $R_i$ can induce bias, however there are conditions under which the complete case analysis will lead to unbiased coefficient estimates. 
First, if the covariates are MCAR, so that $R_i \perp T_i, X_i, v_i$, then 
\[
p(R_i = \mathbbm{1} | T_i, X_i, v_i, \psi) 
  = p(R_i = \mathbbm{1} | \psi)  
\]
and hence 
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}, \eta) 
  = \frac{p(R_i = \mathbbm{1} | \psi) p(T_i | x_i, v_i, \eta)}{p(R_i = \mathbbm{1} | \psi)}
  = p(T_i | x_i, v_i, \eta)
\label{eq:cc_mcar}
\end{equation}
Thus, the complete-data and conditional models are equivalent.
Assuming $X$ is MCAR, then $\hat{\beta}_C$ in (\ref{eq:beta_c}) will be unbiased for $\beta$.
This is consistent with broader results on analyses of MCAR data.

A complete-case analysis is also unbiased under slightly less restrictive assumptions. 
Suppose that $R_i \perp (X_i, T_i) | v_i$, then the complete-data model and the conditional model are equivalent:
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}) 
  = \frac{p(R_i = \mathbbm{1} | v_i, \psi) p(T_i | x_i, v_i, \eta)}{p(R_i = \mathbbm{1} | v_i, \psi)}
  = p(T_i | x_i, v_i, \eta)
\label{eq:cc_mar}
\end{equation}
Under this assumption, $\hat{\beta}_C$ will be unbiased.

The assumption that $R_i \perp (X_i, T_i) | v_i$ implies that if missingness only depends on the estimation error variances, then a complete case analysis may be unbiased. 
This is a weaker assumption than MCAR, which requires $R_i \perp (T_i, X_i, v_i)$.
Intuitively, if both $X_i$ and $T_i$ are conditionally independent of $R_i$, then so is the relationship between $X_i$ and $T_i$.

For most effect size indices, variances $v_i$ are functions of the sample sizes within studies $n_i$. 
Some effect sizes, such as the $z$-transformed correlation coefficient, have variances $v_i$ that depend entirely on the sample size of a study, while for other effect sizes this is approximately true, such as the standardized mean difference. 
For such effect sizes, this assumption implies that missingness depends only on the sample size of the study. 
This may be true, for instance, if smaller studies are less likely to report more fine-grained demographic information regarding their sample out of concern for the privacy of the subjects who participated in the study (and that no other factors affect missingness).

An even weaker assumption can also lead to unbiased estimation with complete cases. 
When $R_i \perp T_i | X_i, v_i$, we can write:
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}) 
  = \frac{p(R_i = \mathbbm{1} | X_i, v_i) p(T_i | X_i, v_i)}{p(R_i = \mathbbm{1} | X_i, v_i)}
  = p(T_i | X_i, v_i)
\label{eq:cc_mnar}
\end{equation}
Thus, if $R_i \perp T_i | X_i, v_i$ then the complete-case model will be the same as the complete-data model. 
Note that $R_i$ can still depend on $X_i$, including $X_{ij}$ that are not observed, which means that the data are MNAR. 

<!-- This result is consistent with the bias approximation in equation (\ref{eq:conditional_bias}).  There, the bias was a function of $f_{ij}'$, which is the derivative of $f_{ij}(T, X, v)$ with respect to $T$.  -->
<!-- If $f_{ij}$ does not depend on $T$, so that $f_{ij}(T, X, v) \equiv f_{ij}(X, v)$ then the derivative will be zero, and hence $\psi_{ij}$ will not contribute to the bias. -->
<!-- Thus, only parts of the missingness model that depend on $T$ will contribute to bias. -->

However, when $R_i$ is not independent of $X_i$ or $T_i$ (given $v_i$), then analyses can be biased. 
Let $\mathcal{R}_1 = \{\mathbbm{1}\}$ so that the complete-case analysis conditions on $R_i \in \mathcal{R}_1$.
Based on equation (\ref{eq:bias_delta}), the bias of $\hat{\beta}_C$ will depend on the $\delta_{i1}$. 
If we let $\Delta = [\delta_{11}, \ldots, \delta_{k1}]$ be the vector of $\delta_{i1}$ and let $\Delta_C$ be the subset of $\Delta$ for which all covariates are observed (i.e., $R_i = \mathbbm{1}$). 
Then the bias of the complete-case analysis can be written as 
\begin{equation}
\text{Bias}[\hat{\beta}_C] = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \Delta_C
\label{eq:cc_bias_delta}
\end{equation}
The bias in equation (\ref{eq:cc_bias_delta}) is a weighted average of individual biases $\delta_{i1}$.
Hence, the bias will be larger if the $\delta_{i1}$ are larger.

Precisely how large the bias in (\ref{eq:cc_bias_delta}) is will depend on the distribution of $R_i$ and its relationship to effect estimates $T_i$ and their covariates $X_i$.
When $R_i$ follows the log-linear model in (\ref{eq:r_loglinear}), the approximate bias can be written as 
\begin{equation}
\text{Bias}[\hat{\beta}_C]
  = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{H}_{1C}  \mathbf{f}_{1C} \psi_1
\label{eq:cc_bias_loglinear}
\end{equation}
where 
\[
\mathbf{H}_1 = \text{diag}[H_1(X_i\beta, X_i, v_i)]
\]
is a $k \times k$ diagonal matrix where entries refer to the probability that an observation is *not* a complete case, 
\[
\mathbf{f}_1 = \left[f_{01}'(X_i^T\beta, X_i, v_i), \ldots, f_{m_1 1}'(X_i^T\beta, X_i, v_i) \right]
\]
is a $k \times m_1$ matrix of derivatives, and $\psi_1 = [\psi_{01}, \ldots, \psi_{m_1 1}]^T$ is a vector of parameters that index the selection model.
Note that the bias in (\ref{eq:cc_bias_loglinear}) involves $\hat{H}_{1C}$ which contains the rows of $\mathbf{H}_1$ for which $R_i = \mathbbm{1}$; similarly for $\mathbf{f}_{1C}$.

While (\ref{eq:cc_bias_loglinear}) provides a general expression for the approximate bias of $\hat{\beta}_C$, it can be a little difficult to interpret.
Loosely, we can see that the bias depends on the probability that observations are omitted due to missingness $\mathbf{H}_{1C}$, as well as some function of the components of the log-linear model $\mathbf{f}_{1C} \psi_1$.
To better intuit this bias, we provide a simple example in the following section.


## Example: Complete-Case Analysis with a Single Binary Covariate 

Suppose the model of interest includes a single binary covariate covariate $X_{i1} \equiv X_i \in \{0, 1\}$, so that the complete data model is 
\begin{equation}
T_i = \beta_0 + \beta_1 X_i + u_i + e_i
\label{eq:cc_example}
\end{equation}
where $\beta_0, \beta_1$ are the regression coefficients of interest.
Note that $\beta_0$ is the average effect when $X_i = 0$ and $\beta_1$ is the contrast in mean effects for when $X_i = 1$ versus when $X_i = 0$.

Because $X_i$ is a scalar, so is $R_i$; $R_i = 0$ indicates that $X_i$ is missing, $R_i = 1$ indicates that $X_i$ is observed. 
A complete-case analysis would include only effects $i$ for which $X_i$ is observed (i.e., $R_i = 1$).
The complete-case estimator for $\beta_0$ is given by a weighted sum of $T_i$ among the effects for which $X_i = 0$ and $R_i = 1$:
\begin{equation}
\hat{\beta}_{0C} = \frac{\sum_{i: X_i = 0, R_i = 1} w_i T_i}{\sum_{i: X_i = 0, R_i = 1} w_i}
\label{eq:b0c_ex}
\end{equation}
The complete-case estimator for $\beta_1$ is given by the difference between the (weighted) mean effect for $X_i = 1$ versus $X_i = 0$:
\begin{equation}
\hat{\beta}_{1C} = \frac{\sum_{i: X_i = 1, R_i = 1} w_i T_i}{\sum_{i: X_i = 1, R_i = 1} w_i} - \hat{\beta}_{0C}
\label{eq:b1c_ex}
\end{equation}

Assume that the model for missingness is log-linear, and that missingness depends on the size of the effect $T_i$ and the value of $X_i$:
\begin{equation}
\text{logit}[p(R_i = 1 | T_i, X_i, v_i)] 
  = \psi_0  + \psi_1 T_i + \psi_2 X_i
\label{eq:cc_loglinear}
\end{equation}
Under this model, $H_{1}(X_i\beta, X_i, v_i)$ depends only on $X_i$ and not $v_i$, so we can write $H_1(X_i) = p(R \neq \mathbbm{1} | T_i, X_i, v_i)\rvert_{T_i = X_iT \beta}$.
As well, $f_{11}(T_i, X_i, v_i) = T_i$ and $f_{21}(T_i, X_i, v_i) = X_i$. 
Given the result in equation (\ref{eq:conditional_bias}), we can write
\begin{equation}
\delta_{i1}
   \approx H_1(X_i)(v_i + \tau^2)\psi_1
\label{eq:cc_ex_delta}
\end{equation}

Under this selection model, the bias of the complete-case estimator for $\beta_0$ is:
\begin{equation}
\text{Bias}[\hat{\beta}_{0C}] 
  \approx H_1(0)(\bar{v}_0 + \tau^2)\psi_1
\label{eq:cc_bias_b0}
\end{equation}
where $\bar{v}_0$ is the average estimation error variance $v_i$ among effects for which $X_i = 0$ and $R_i = 1$.

The expression in (\ref{eq:cc_bias_b0}) depends on three key quantities, and is an increasing function of each of those quantities.
First, the bias increases in $H_1(0)$, which is essentially the probability that a case is missing any covariates; as covariates are more likely to be missing, the bias will be greater.
Second, it is increasing in $\bar{v}_0 + \tau^2$, the average variation of $T_i$ for which $X_i = 0$; the greater the variation, the greater the bias.
Finally, the bias depends on $\psi_1$, which is the relationship between an $X_i$ being observed and the size of $T_i$.

To gain better insight into equation (\ref{eq:cc_bias_b0}), suppose $v_i \approx v = \bar{v}_0$ so that each study has roughly the same estimation error variance. 
If we assume $T_i$ is on the scale of a standardized mean difference, $v_i \approx 4/n_i$ where $n_i$ is the total sample size used to compute $T_i$. 
Various researchers have described conventions for the magnitude of $\tau^2$ that range from $\tau^2 = v/4$ to $\tau^2 = v$ [@hedgesPowerStatisticalTests2001; @hedgesPowerStatisticalTests2004; @hedgesStatisticalAnalysesStudying2019].
Thus, we can write $\tau^2 + v = 4(1 + r)/n$ from some constant $r$ that ranges from 0 to 1. 

The parameter $\psi_1$ is a log-odds ratio, which reflects the odds of a complete case for $T_i$ versus $T_i - 1$.
There are various conventions for the size of an odds ratio that depend on base rates $P[R = \mathbbm{1} | T]$.
Conventions used by @cohenStatisticalPowerAnalysis1988 have been interpreted as implying that a "small" odds ratio is about 1.49, a "medium" odds ratio is about 3.45, and a "large" odds ratio is about 9.0. 
@fergusonEffectSizePrimer2009 suggests 2.0, 3.0, and 4.0 for small, medium, and large odds ratios, while @chenHowBigBig2010 provide a range of conventions for different base rates, and their tables are roughly consistent with about 1.5 being a small odds ratio, 2.4 being medium, and 4.1 being large. 
@haddockUsingOddsRatios1998 suggests any odds ratio over 3.0 would be considered quite large.
Thus, consider a range of odds ratios from about 1.5 to 4.5.

However, the actual size of $\psi_1$ will depend on the scale of $T_i$.
A difference of $T_i - T_j = 1$ is considered quite large for standardized mean differences. 
A less extreme difference $D_T = |T_i - T_j|$ for a standardized mean difference would be no larger than the size of an individual $T_i$.
Conventions for standardized mean differences imply that a "small" effect would be about $T_i = 0.2$, a "medium" effect would be $T_i = 0.5$, and a "large" effect would be $T_i = 0.8$ [@cohenStatisticalPowerAnalysis1988].
Thus, meaningful values of $D_T$ might feasibly range from 0.2 to 1.0.
These conventions for odds ratios and $D_T$ would imply that relevant values of $|\psi_1|$ might range from 0.4 to over 7.5.

Based on these conventions, Figure \ref{fig:delta} shows the potential (approximate) bias of $\hat{\beta}_{0C}$ for this example. 
Each panel corresponds to a given within-study variance $v = 4/n$ and residual heterogeneity $\tau^2$. 
Panels plot the bias contributed by a single case $\delta_i$ as a function of the probability of missingness $H_1(0)$ ($x$-axis) and $\psi_1$ (color).
The panels on the bottom few rows and left most columns show that if both $\psi_1$ is small and $\tau^2 + v$ is small, then $\delta_i$ will be less than 0.05. 
However if $\tau^2 + v_i$ is larger and the probability of a complete case is strongly related to $T_i$ (i.e., $\psi_1$ is large), then the bias can be greater than 0.2 or even 0.5 (Cohen's $d$).

It is worth noting that Figure \ref{fig:delta} gives the bias for when $T_i$ is positively correlated with $X_i$ being fully observed, and hence $\psi_1 > 0$. 
This need not be the case, and $\psi_1 < 0$. 
When $\psi_1 > 0$, then the bias of $\hat{\beta}_{0C}$ is negative, and would be be a mirror image of those in Figure \ref{fig:delta}.
Larger, more negative values of $\psi_1$ would lead to a greater downward bias.

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/delta_plot_cts}
\end{center}
\caption{This figure plots the bias of the intercept estimate $\hat{\beta}_{0C}$ ($y$-axis) of the example. Bias is shown as a function of the average sampling variance $v$, residual heterogeneity $\tau^2$, the probability of missingness $H_1(0)$ ($x$-axis), and the correlation between missingness and the effect size as measured by $\psi_1$ (color). Note that $\psi_1$ is a log-odds ratio for effect sizes on the scale of Cohen's $d$.}
\label{fig:delta}
\end{figure}


The bias of $\hat{\beta}_{1C}$ under selection model (\ref{eq:cc_loglinear}) is given by:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}]
  \approx \left[H(1)(\bar{v}_1 + \tau^2) - H(0)(\bar{v}_0 + \tau^2)\right]\psi_1 
\label{eq:cc_bias_beta1_example}
\end{equation}
where $\bar{v}_1$ is the mean $v_i$ among effects for which $X_i = 1$ and $R_i = 1$. 
As with $\hat{\beta}_{0C}$, the bias of $\hat{\beta}_{1C}$ is an increasing function of $\psi_1$. 
If $T_i$ has a strong positive correlation with $R_i$, then $\psi_1$ will be larger and so will the bias of $\hat{\beta}_{1C}$.

When all studies have approximately the same estimation error variance so that $v_i \approx v$ and $\bar{v}_0 \approx \bar{v}_1$, then the bias of $\hat{\beta}_{1C}$ is approximately:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}] 
  \approx \left[p(R = 0 | X = 1) - p(R = 0 | X = 0)\right] (v + \tau^2) \psi_1 
\label{eq:cc_bias_b1_simp}
\end{equation}
The expression in (\ref{eq:cc_bias_b1_simp}) depends on three quantities. 
Like $\hat{\beta}_{0C}$, the bias of $\hat{\beta}_{1C}$ is an increasing function of $\tau^2 + v$ and $\psi_1$.
The bias of $\hat{\beta}_{1C}$ also increases as a function of $p(R = 0 | X = 1) - p(R = 0 | X = 0)$, which can be thought of as a difference in observation rates between $X_i = 1$ and $X_i = 0$.
This difference will be greater if $R$ and $X$ are strongly correlated.
Taken together, the bias of $\hat{\beta}_{1C}$ will be greatest when there are fewer complete cases, missingness is strongly related to the size of effects and the value of the covariate $X$. 

To gain insight into the bias of $\hat{\beta}_{1C}$, consider the values of $\psi_1 \in [0.4, 7.5]$ and $\tau^2 + v = 4(1 + r)/n$ discussed above. 
The difference $p(R = 0 | X = 1) - p(R = 0 | X = 0)$ is just a difference in conditional probabilities. 
For reference, because both $R_i$ and $X_i$ are binary, then $p(R = 0 | X = 1) - p(R = 0 | X = 0)$ would be equal to the correlation between $R$ and $X$ (assuming equal marginals in a $2 \times 2$ table).
Thus, $|p(R = 0 | X = 1) - p(R = 0 | X = 0)|$ might feasibly range from 0 to as large as 0.5.

Figure \ref{fig:b1} shows the potential bias of $\hat{\beta}_{1C}$ for this example assuming the values discussed above.
Each panel corresponds to a given amount of heterogeneity $\tau^2 + v$, and within panels the bias is shown as a function of the difference $p(R = 0 | X = 1) - p(R = 0 | X = 0)$ ($x$-axis) and $\psi_1$ (color).
Figure \ref{fig:b1} highlights that the relationship between $R_i$ and $T_i$ ($\psi_1$) and between $R_i$ and $X_i$ ($x$-axes) can affect the magnitude of the bias. 
If $R_i$ is strongly correlated with both $X_i$ and $T_i$ the bias can be as large as $d$ = 0.3 or 0.4. 
However, the less $R_i$ depends on $T_i$ or $X_i$, the lower the bias is. 

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/bias_beta1_ex1}
\end{center}
\caption{This figure plots the bias of $\hat{\beta}_{1C}$ ($y$-axis). Each panel corresponds to a given value of residual heterogeneity $\tau^2$ and estimation error variance $v$. Within panels, the bias of $\hat{\beta}_{1C}$ is plotted as function of differential observation rates ($p(R = 0 | X = 1) - p(R = 0 | X = 0)$), which is analogous to the correlation between the value of $X$ and whether it is observed. Bias is also shown as a function of $\psi_1$ which is the relationship between the probability of observing $X$ and the effect size $T$. Bias is shown on the scale of Cohen's $d$ and $\psi_1$ is on the scale of a log-odds ratio.}
\label{fig:b1}
\end{figure}






<!-- - Continuous covariates? -->

<!-- For a single continuous covariate, note that  -->
<!-- \[ -->
<!-- \hat{\beta}_1 = \frac{\sum w_i (T_i - \bar{T}_\cdot)(X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- \] -->
<!-- Given the selection model above, the bias of $\hat{\beta}_1$ can be expressed as: -->
<!-- \[ -->
<!-- \psi_1 \frac{\sum (1 - G(X_i))(X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- + \psi_3 \frac{\sum (1 - G(X_i)) X_i (X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- \] -->

<!-- It is not immediately clear that these simplify. Perhaps we leave this result out? -->



# Bias in Shifting-Case Analyses

Shifting-case analyses are a common approach in meta-regression when there are very few complete cases. 
These analyses involve fitting multiple regression models, where each model omits some of the covariates of interest.
In this sense, shifting-case analyses can be thought of as a set of regression models. 
Consider one model from that set, which estimates regression coefficients for some subset $S$ of the relevant covariates using the estimator $\hat{\beta}_S$ in equation (\ref{eq:beta_s}).
Recall that $E$ refers to the set of covariates omitted from the model, and that the estimator $\hat{\beta}_S$ conditions on a set of missingness patterns $R_i \in \mathcal{R}_j$. 
The set of missingness patterns $\mathcal{R}_j$ is such that $R_{iS} = 1$ so that all included covariates are observed.



To understand the conditions under which $\hat{\beta}_S$ is unbiased, we can write a shifting-case model as:
\begin{equation}
p(T_{i} | X_{iS}, v_i, R_i \in \mathcal{R}_j) = \frac{p(R_i \in \mathcal{R}_j | T_i, X_{iS}, v_i) p(T_i | X_{iS}, v_i)}{p(R_i \in \mathcal{R}_j | X_{iS}, v_i)}
\label{eq:sca_model}
\end{equation}

The model in (\ref{eq:sca_model}) is slightly different from the models in the previous sections in that all of the functions depend on the covariates included in a given regression $X_{iS}$ rather than the complete set of relevant covariates $X_i$.
Thus, the function $p(T_i | X_{iS}, v_i)$ can be thought of as a partial-data model, since it omits relevant covariates. 
The partial-data model $p(T_i | X_{iS}, v_i)$ need not be equivalent to the complete-data model $p(T_i | X_i, v_i)$ because the former conditions only on $X_{iS}$ and not the full set of covariates $X_i$.
These models would only be equivalent if $T_i \perp X_{iE} | X_{iS}, v_i$. 
That is, unless the excluded covariates are completely unrelated to effect size (given the covariates included in the SCA model), then $\hat{\beta}_S$ will be biased even if no $X_{iS}$ are missing.

The model in (\ref{eq:sca_model}) suggests a very strict set of conditions for which $\hat{\beta}_S$ is unbiased. 
First, missingness must be independent of effect sizes. 
This arises if $R_i \perp T_i | X_{iS}, v_i$ or $R_i \perp (T_i, X_{iS}) | v_i$. 
This is a similar assumption as that made for unbiased complete-case analyses, and amounts to effect sizes (and potentially covariates) being independent of missingness.

Second, any excluded covariates must be completely irrelevant given the included covariates: $T_i \perp X_{iE} | X_{iS}, v_i$. 
This amounts to $\beta_j = 0$ for all $j \in E$.
A related assumption is that $(T, X_{iS}) \perp X_{iE} | v$, which would imply that the complete data likelihood involves no interactions between $X_{iS}$ and $X_{iE}$ and that $X_{iS}$ and $X_{iE}$ are orthogonal.
Note that conditions on omitted covariates *and* omitted observations must hold in order for $\hat{\beta}_S$ to be unbiased.

When assumptions about omitted variables and cases are not met, $\hat{\beta}_S$ will be biased. 
Just how biased will depend on a number of factors, including the amount of missingness, the missingness mechanism, and the relevance of any excluded covariates.
The bias can be expressed as:
\begin{equation}
\text{Bias}[\hat{\beta}_S] = (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{UE} \beta_E + (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \Delta_{jU}
\label{eq:bias_sca}
\end{equation}
where $\mathbf{X}_{UE}$ is the matrix of omitted covariates and $\beta_E$ comprises the coefficients for the omitted covariates. The term $\Delta_j$ is a vector of biases due to missingness $\Delta_j = [\delta_{1j}, \ldots, \delta_{kj}]$ and $\Delta_{jU}$ is the subset of $\Delta_j$ for which $R_i \in \mathcal{R}_j$.
Note that the $\delta_{ij}$ are the biases due solely to missingness as in equation (\ref{eq:conditional_bias}):
\[
\delta_{ij} = E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] - X_i \beta
\]

The expression in (\ref{eq:bias_sca}) shows that a shifting-case analysis suffers from two sources of bias. 
The first source, captured in the first term in (\ref{eq:bias_sca}), is a function of the coefficients for the excluded covariates $\beta_E$.
This is referred to in the statistical and econometric literature as *omitted variable bias* [e.g., @farrarMulticollinearityRegressionAnalysis1967; @melaImpactCollinearityRegression2002]. 
Omitted variable bias arises even if no $X_{iS}$ are missing, and is related to the issue of multicollinearity in linear models.
In fact, if the columns in $\mathbf{X}_{US}$ and $\mathbf{X}_{UE}$ are orthogonal, so that the omitted variables are independent of the included variables, then the omitted variable bias will be zero. 
When the omitted variables are not orthogonal to the included variables, the bias will be nonzero, and it will depend in large part on the contribution of the omitted variables $\mathbf{X}_{UE} \beta_E$. 
The estimator $\hat{\beta}_S$ will have greater bias if the coefficients for the omitted variables $\beta_E$ are larger and the omitted covariates $\mathbf{X}_{UE}$ are correlated with the included covariates $\mathbf{X}_{US}$.

The second term in (\ref{eq:bias_sca}) captures the bias due to ignoring observations missing $X_{iS}$.
This *missingness bias* is a function of $\Delta{jU}$, which is itself a vector of biases for each effect, and it can be understood in terms of its individual components $\delta_{ij}$.
Because the $\delta_{ij}$ are of the same form for the complete-case versus shifting-case models, the missing data bias for a shifting-case analysis is governed by similar factors as the complete-case analyses, and are quite possibly similar in magnitude. 
Based on (\ref{eq:conditional_bias}), $\delta_{ij}$ will be positive if $T_i$ is positively correlated with whether $R_i \in \mathcal{R}_j$, and $\delta_{ij}$ will be greater in magnitude when that correlation is larger.

Taken together, the bias of shifting-case analysis can be greater than the bias of a complete-case analysis.
This occurs if the omitted variable bias and the missingness bias are in the same direction (e.g., both are positive).
For both biases to be in the same direction, correlation between $T_i$ and the omitted variables $X_{iE}$ must be in the same direction as the correlation between $T_i$ the probability that $X_{iS}$ is observed.
If, however, the omitted variable and missingness biases are in opposite directions, this can reduce the bias of a shifting-case estimator.
It is worth noting, however, that it will almost always be impossible to confirm the direction of biases, since they depend on potentially unobserved covariates.




## Example: Shifting-Cases Analysis with Two Binary Covariates

Suppose $X_i = [1, X_{i1}, X_{i2}]$ and $X_{i1}$ and $X_{i2}$ are binary covariates such that 
\begin{equation}
T_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_2 + u_i + e_i
\label{eq:sca_ex}
\end{equation}
If there is missingness in both $X_1$ and $X_2$, then $R_i \in \{0, 1\}^2$ so that $R_i = [1,1]$ indicates both covariates are observed, and $R_i = [1, 0]$ indicates only $X_{i1}$ is observed. 
If missingness is such that $R_i = [1, 1]$ for very few effect estimates, then a shifting units analysis might involve regressing $T_i$ on the observed values of $X_{i1}$ and then on the observed values of $X_{i2}$. 

The first regression would take only rows for which $X_{i1}$ is observed, so that $R \in \{[1,1], [1,0]\}$ and the excluded $X_{i2}$ could be either 0 or 1. 
The shifting-case estimators follow from equation (\ref{eq:}):
\[
\hat{\beta}_{0S} = \frac{\sum_{X_1 = 0} w_i T_i}{\sum_{X_1 = 0} w_i}, 
\qquad \hat{\beta}_{1S} = \frac{\sum_{X_1 = 1} w_i T_i}{\sum_{X_1 = 1} w_i} - \hat{\beta}_{0S}
\]

Assume that missigness follows the following log-linear model:
\begin{equation}
\text{logit}[p(R_i = [a, b] | T_i, X_i, v_i)] = \psi_{0ab} + \psi_{1ab} T_i + \psi_{2ab} X_{i1} + \psi_{3ab} X_{i2} 
\label{eq:logit_sca}
\end{equation}
Here, the probability that an observation is included in the first SCA model depends on $T_i$, $X_{i1}$, and $X_{i2}$. 
It also depends on the parameters $\psi_{mab}$ such that $a = 1$ (i.e., $\psi_{010}, \psi_{011}$, etc.).
The first SCA model that includes only $X_{i1}$ conditions on $R_i \in \mathcal{R}_1 = \{[1,0], [1, 1]\}$.
The observed and missing $X_{i2}$ take values of either 0 or 1. 

Given the selection model in (\ref{eq:logit_sca}), the bias of the SCA estimators can be written as:
\begin{align}
\text{Bias}[\hat{\beta}_0]
  & = \beta_2 \frac{\sum_{X_1 = 0, X_2 = 1} w_i}{\sum_{X_i = 0} w_i} + \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i}}{\sum_{X_{i1} = 0} w_i}  \\
\text{Bias}[\hat{\beta}_1]
  & = \beta_2 \left(\frac{\sum_{X_{i1} = 1, X_{i2} = 1} w_i}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0, X_{i2} = 1} w_i}{\sum_{X_{i1} = 0} w_i}\right) + \left(\frac{\sum_{X_{i1} = 1} w_i \tilde{\delta}_{i}}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i}}{\sum_{X_{i1} = 0} w_i}\right)
\end{align}

Note that both the bias of $\hat{\beta}_{0S}$ and $\hat{\beta}_{1S}$ depend on two terms. 
The first term in each expression is the omitted variable bias.
The second term in each expression is the missingness bias.
To gain some insight into these expressions, consdier the omitted variables bias. 
When effects are estimated with roughly the same precision, so that $w_i \approx w$, then the omitted variable biases reduce to 
\begin{align}
\text{Omitted Var. Bias}[\hat{\beta}_{0S}]
  & = \beta_2 P[X_2 = 1 | X_1 = 0] \\
\text{Omitted Var. Bias}[\hat{\beta}_{1S}]
  & = \beta_2 \left(P[X_2 = 1 | X_1 = 1] - P[X_2 = 1 | X_1 = 0] \right)
\end{align}

To get a better sense of these expressions, consider the bias for $\hat{\beta}_1$. This will depend on two quantities. The first is $\beta_2$, which is the contribution of $X_2$ to the complete-data model.
The second is the difference $P[X_2 = 1 | X_1 = 1] - P[X_2 = 1 | X_1 = 0]$. 
Because both $X_1$ and $X_2$ are binary, this difference is equivalent to their Pearson correlation (assuming equal marginals). 
If $X_1 \perp X_2$, then their correlation is zero, and $\hat{\beta}_1$ will be unbiased.
But if $X_1$ and $X_2$ are correlated, the bias of $\hat{\beta}_1$ will depend on how strongly correlated they are, and how big $\beta_2$ is.

Figure \ref{fig:omitted_bias} shows the omitted variable bias of $\hat{\beta}_0$ (left plot) and $\hat{\beta}_1$ as a function of $\beta_2$. 
Both the bias and $\beta_2$ are shown on the scale of Cohen's $d$. 
In the left plot $\pi_{01} = P[X_2 = 1 | X_1 = 0]$ is the proportion of $X_2 = 1$ when $X_1 = 0$. 
In the right plot, $\rho_{12} = P[X_2 = 1 | X_1 = 1] - P[X_2 = 1 | X_1 = 0]$, which is roughly the correlation between $X_1$ and $X_2$.
Note that because $\rho_{12}$ can be intuited as (roughly) a Pearson correlation, the values in the figure include 0, 0.1 (i.e., a "small" correlation), 0.2 (medium correlation), and 0.5 (large correlation) [@cohenStatisticalPowerAnalysis1988].

The figure shows that if $\beta_2 = 0$ so that $X_2$ is irrelevant given $X_1$, that both $\hat{\beta}_0$ and $\hat{\beta}_1$ will be unbiased.
However, when $\beta_2$ is nonzero, both estimators will be biased.
If $X_1$ and $X_2$ are highly correlated, or if $X_2 = 1$ when $X_1 = 0$ with high probability, the bias of both estimators will about as large as a "small" effect (i.e., $d = 0.2$) when $\beta_2$ is larger than 0.2. 
For $\hat{\beta}_1$ the bias will be less than about $d = 0.05$ when $|\beta_2| \leq 0.1$ or if $\rho_{12} < 0.5$. 

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/omitted_var_bias}
\end{center}
\caption{This figure shows the omitted variable bias of $\hat{\beta}_0$ and $\hat{\beta}_1$ for the model in (\ref{eq:sca_ex}) as a function of the omitted variable $\beta_2$. The bias ($y$-axis) and $\beta_2$ ($x$-axis) are on the scale of Cohen's $d$. The bias displayed is soley due to omitting $X_2$ from (\ref{eq:sca_ex}). In the left plot, lines are colored according to $\pi_{01} = P[X_2 = 1 | X_1 = 0]$. In the right plot, lines are colored according to $\rho_{12} = P[X_2 = 1 | X_1 = 1] - P[X_2 = 1 | X_1 = 0]$.}
\label{fig:omitted_bias}
\end{figure}

Figure \ref{fig:omitted_bias} does not take into account any bias induced by missingness. 
Note that the expression for missingness bias for $\hat{\beta}_{0S}$ is a weighted average of $\tilde{\delta}_i$ among effects for which $X_{i1} = 0$. 
Similarly, $\tilde{\delta}_i$ is a weighted average of $\delta_{ij}$. 
This when $w_i \approx w$ and $\delta_{ij} \approx \delta$, the missingness bias for $\hat{\beta}_{0S}$ will be about the same as the missingness bias for $\hat{\beta}_{0C}$. 
Figure \ref{fig:delta} shows possible values of this bias on the scale of Cohen's $d$.

Likewise, the missingness bias of $\hat{\beta}_{1S}$ is a difference of weighted averages of $\delta_{ij}$. Assuming $\delta_{ij} \approx \delta$ and $w_i \approx w$, the missingness bias of $\hat{\beta}_{1S}$ is approximately the same as in \ref{eq:}.
Figure \ref{fig:bias_b1_cca} shows possible values of that bias on the scale of Cohen's $d$.

[FIGURE IDEA: Panels for $\psi_1$ and $\beta_2$. Within panels, plot total bias vs. Prob. Missing].






# Discussion

1. Both CCA and SCA will produce biased coefficient estimates unless certain conditions are met. While these sets of conditions are somewhat restrictive for CCA, they are considerably more restrictive for SCA.

2. The bias of CCA/SCA when these conditions are not met can be substantial. 

3. An important part of the bias is that it will depend on unknown parameters and unobserved data. This means that for CCA/SCA to be unbiased, we would need to assume those conditions hold. Because it will be impossible to say if they hold, or if they don't, it will be difficult to say just how biased a given analysis is. [PERHAPS TIE IN EXAMPLE].

4. We would recommend avoiding SCA unless there are strong reasons to suspect both conditions are true. We would avoid using CCA unless there is very little missing data.

5. There are alternatives available. EM (Ibrahim), EM with MNAR (Lipsits), MI, FIML. Their application in MR is less broad due to a lack of easy-to-integrate software for most meta-analysts. For MI, the properties of various imputation techniques has not been fully studied for MR models.

6. There is still a large amount of work to be done on missing data in MA.

\clearpage

\appendix


# Approximate Bias for Log-Linear Selection Models

**Proposition:** Suppose $p(T_i | X_i, v_i)$ is the standard fixed- or random effects meta-regression model in equation (\ref{eq:full_data_prob}), and suppose $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i) = H_j(T_i, X_i, v_i)$ follows the log-linear model in (\ref{eq:r_loglinear}). Then:

\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] \approx X_i\beta + H_j(X_i\beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional_expectation}
\end{equation}
where $f_{mj}'(X_i \beta, X_i, v_i) = \frac{\partial}{\partial T_i} f_{mj}(T_i, X_i, v_i)\rvert_{T_i = X_i^T \beta}$.
Therefore, the bias of the conditional expectation is given by:
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional_bias}
\end{equation}

**Proof:**

In this proof, we drop the subscript $i$ for sake of simplicity. 
Denote
\begin{align*}
H_j(X\beta, X, v) \equiv H_j(X, v) 
  & = P[R =\not\in \mathcal{R}_j | T, X, v] \rvert_{T = X\beta} \\
G_j(X, v) 
  & = 1 - H_j(X, v) \\
g_j(X, v)
  & = P[R \in \mathcal{R}_j | X, v] \text{ as in (\ref{eq:pr_xv}).}
\end{align*}

Then an approximation for $E[T | X, v, R \in \mathcal{R}_j]$ is as follows:
\begin{align*}
  & \text{omitting subscripts, Taylor series for exponents in } P[R \in \mathcal{R}_j | T, X, v] \text{ at } T = X  \beta\\
E[T | X, v, R \in \mathcal{R}_j] 
  & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij}(T, X, v)\right\}}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)} \left(1 + e^{\sum_i \psi_{ij} f_{ij}(T, X, v)}\right)} dT \\
  & = \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij} (X \beta, X, v) \right. \\
  & \qquad\qquad + \sum_i \psi_{ij} f_{ij}'(X \beta, X, v)(T - X \beta) - \log\left(1 + e^{\sum_i \psi_{ij} f_{ij} (X  \beta, X, v)}\right) \\
  & \qquad\qquad \left. - G_j(X, v)(\sum_i \psi_{ij} f_{ij}'(X  \beta, X, v))(T - X\beta) + O(T^2)\right\} dT \\
  & \approx \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{1}{2(\tau^2 + v)}\left(T^2 - 2TX\beta - 2T (\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X \beta, X, v)\right.\right. \\
  & \qquad\qquad \left.\left.+ 2T(\tau^2 + v) G_j(X, v)(\sum_{i \in \mathcal{D}} \psi_{ij} f_{ij}'(X  \beta, X, v)) + \ldots\right)\right\} dT \\
  & = X\beta + H_j(X, v)(\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X  \beta, X, v)  \qquad\qquad\qquad \blacksquare
\end{align*}

Note that this uses a first order Taylor expansion of the log-linear model at $T = X \beta$, and thus assumes the $f_{mj}$ are differentiable.
The approximation will be more accurate if $\tau^2 + v_i$ are small.
A more accurate approximation is possible if the $f_{mj}$ are linear in $T_i$. 
In that case, only an approximation of the denominator of the log-linear model is required.



# Issues with Conditional Inference on Incomplete Data

Both complete- and shifting-case analyses ignore information, which can lead to biased meta-regression estimates. 
Complete-case analyses omit effects with missing covariates, so that $\mathbf{X}_A \equiv \mathbf{X}_C$ is a matrix containing the rows of $\mathbf{X}$ that are not missing any covariates.
Shifting-case analyses omit covariates *and* missing data, so that $\mathbf{X}_A \equiv \mathbf{X}_S$ contains a subset of the columns of $\mathbf{X}$ and only rows from that subset that are not missing any variables.

Neither analytic approach typically makes any adjustments for the information they exclude. 
They therefore estimate regression coefficients by 
\[
\hat{\beta} = (\mathbf{X}_A^T \mathbf{W}_A \mathbf{X}_A)^{-1} \mathbf{X}_A^T \mathbf{W}_A \mathbf{T}_A
\]
where $A$ corresponds to the relevant subset of $\mathbf{X}$ as described above, and $\mathbf{T}_A$ and $\mathbf{W}_A$ are the relevant subsets of $\mathbf{T}$ and $\mathbf{W}$ used in the analysis.

These estimates will be biased under many conditions, and that bias can be expressed by:
\begin{equation}
(\mathbf{X}_A^T \mathbf{W}_A \mathbf{X}_A)^{-1} \mathbf{X}_A^T \mathbf{W}_A E[\mathbf{T}_A | \mathbf{X}_A, \mathbf{v}_A, R \in \mathcal{R}_j] - \beta
\label{eq:bias_gen}
\end{equation}
where $\mathcal{R}_j$ is the missing data pattern(s) upon with the analytic approach conditions.

Equation (\ref{eq:bias_gen}) shows that the bias induced by complete- or shifting-case analyses will be a function of the conditional expectation $E[T_i | X_i, v_i, R_i]$, and how it differs from $E[T_i | X_i, v_i] = X_i \beta$.
One way to convceive of this is to note that the conditional expectation $E[T_i | X_i, v_i, R_i \in \mathcal{R}_j]$ can be expressed as
\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] = X_i\beta + \delta_i
\label{eq:condbias}
\end{equation}
where $\delta_i$ corresponds to the bias induced for observation $i$ by conditioning on $R_i$. 
If we denote $\mathbf{\delta} = [\delta_1, \ldots, \delta_k]^T$, and $\delta_A$ as the relevant subset of $\delta$, then we can express the bias in conditional estimates of regression coefficients as:
\begin{equation}
(\mathbf{X}_A^T \mathbf{W}_A \mathbf{X}_A)^{-1} \mathbf{X}_A^T \mathbf{W}_A \mathbf{\delta}_A 
\label{eq:bias_expression}
\end{equation}
The conditional expectation $E[T_i | X_i, v_i, R_i]$, and hence $\mathbf{\delta}$ will depend on the selection model given in equation (\ref{eq:selection_model}), which in turn depends on the missingness mechanism $p(R_i | T_i, X_i, v_i)$.
To compute just how large the bias is requires assumptions about missingness.




For the remainder of this article, we will denote 
\[
(1 - G_j(X_i, v_i)) 
  = H_{ij}, 
\qquad 
\mathbf{f}_{ij}^T \psi_j
  = \sum_{m = 0}^{m_j} \psi_{mj} f_{kj}'(X_i^T\beta, X_i, v_i) 
  = \sum_{m = 1}^{m_j} \psi_{mj} f_{kj}'(X_i^T\beta, X_i, v_i)
\]

The bias $\delta_i$ under the log-linear selection model in equation (\ref{eq:conditional_bias}) depends on a variety of quantities. 
First, $H_{ij}$ is essentially the probability that $R_i \neq r_j$, which means that it can loosely be understood as the probability that a given observation is excluded from the analysis (conditional on $X, v$). 
For example, if $r_j = \mathbb{1}$ as in a complete-case analysis, $H_{ij} \equiv H_i$ would give the probability that a covariate in for the $i$th effect is missing a covariate.
Note that $\delta_i$ is increasing in $H_{ij}$ which means that the greater the probability of missingness, the greater the bias induced by conditioning on missingness.

The second quantity that $\delta_i$ depends on is $v_i + \tau^2$. 
When effects are estimated with low precision or if there is a large amount of residual heterogeneity, then $v_i + \tau^2$ will be larger, and hence so will the bias. 
However, if effects have very small standard errors, then this can reduce the size of $\delta_i$.

Finally, $\delta_i$ depends on the selection model. 
Note that if $f_{mj}(T_i, X_i, v_i)$ does not depend on $T_i$, then its derivative is 0. 
Therefore, $\delta_i$ will be a function of the $\psi_{mj}$ such that $f_{mj}$ depends on $T_i$. 

As an example, consider the following log-linear selection model for complete cases (i.e., $R_i = [1, \ldots, 1] = \mathbbm{1}$):
\begin{equation}
\text{logit}[p(R_i = \mathbbm{1} | T_i, X_i, v_i)] = \psi_0 + \psi_1 T_i
\label{eq:loglinear_example}
\end{equation}
Then $f_{1}(T, X, v) = T$, and the bias $\delta_i$ can be expressed as
\begin{equation}
\frac{1}{1 + \exp\{\psi_0 + \psi_1 X\beta\}} (\tau^2 + v_i) \psi_1
\label{eq:deltai}
\end{equation}
To gain some insight into equation (\ref{eq:deltai}), suppose $T_i$ is on the scale of a standardized mean difference so that $v_i \approx 4/n_i$ where $n_i$ is the total sample size used to compute $T_i$. 
The expression in (\ref{eq:deltai}) depends on three key quantities.
The first term is essentially the probability that a case is missing any covariates.
The second term is the total variation of $T_i$ around the regression lines, which depends on $\tau^2$.
Various researchers have described conventions for the magnitude of $\tau^2$ that range from $\tau^2 = v_i/4$ to $\tau^2 = v_i$ [@hedgesPowerStatisticalTests2001; @hedgesPowerStatisticalTests2004; @hedgesStatisticalAnalysesStudying2019].
Thus, we can write $\tau^2 + v_i = 4(1 + r)/n_i$ from some constant $r$ that ranges from 0 to 1. 

The parameter $\psi_1$ is a log-odds ratio, which reflects the odds of a complete case for $T_i$ versus $T_i - 1$.
There are various conventions for the size of an odds ratio that depend on base rates $P[R = \mathbbm{1} | T]$.
Conventions used by @cohenStatisticalPowerAnalysis1988 have been interpreted as implying that a "small" odds ratio is about 1.49, a "medium" odds ratio is about 3.45, and a "large" odds ratio is about 9.0. 
@fergusonEffectSizePrimer2009 suggests 2.0, 3.0, and 4.0 for small, medium, and large odds ratios, while @chenHowBigBig2010 provide a range of conventions for different base rates, and their tables are roughly consistent with about 1.5 being a small odds ratio, 2.4 being medium, and 4.1 being large. 
@haddockUsingOddsRatios1998 suggests any odds ratio over 3.0 would be considered quite large.
Thus, consider a range of odds ratios from about 1.5 to 4.5.

However, the actual size of $\psi_1$ will depend on the scale of $T_i$.
A difference of $T_i - T_j = 1$ is considered quite large for standardized mean differences. 
A less extreme difference $\Delta_T = |T_i - T_j|$ for a standardized mean difference would be no larger than the size of an individual $T_i$.
Conventions for standardized mean differences imply that a "small" effect would be about $T_i = 0.2$, a "medium" effect would be $T_i = 0.5$, and a "large" effect would be $T_i = 0.8$ [@cohenStatisticalPowerAnalysis1988].
Thus, meaningful values of $\Delta_T$ might feasibly range from 0.1 to 1.0.
These conventions for odds ratios and $\Delta_T$ would imply that relevant values of $|\psi_1|$ might range from 0.4 to over 15.

Based on these conventions, Figure \ref{fig:delta} shows the potential bias $\delta_i$ for this example. 
Each panel corresponds to a given within-study sample size $n$ and residual heterogeneity $\tau^2$. 
Within plots, the bias contributed by a single case $\delta_i$ is plotted as a function of the probability of missingness $H_i$ ($x$-axis) and $\psi_i$ (color).
The plots on the bottom few rows and left most columns show that if both $\psi_1$ is small and $\tau^2 + v$ is small, then $\delta_i$ will be less than 0.05. 
However if $\tau^2 + v_i$ is larger and the probability of a complete case is strongly related to $T_i$ (i.e., $\psi_1$ is large), then the bias can be greater than 0.2 or even 0.5 (Cohen's $d$).

It is worth noting that Figure \ref{fig:delta} gives the bias for when $T_i$ is positively correlated with $X_i$ being fully observed, and hence $\psi_1 > 0$. 
This need not be the case, and $\psi_1 < 0$. 
in that case, the biases $\delta_i$ would be a mirror image of those in Figure \ref{fig:delta}.
Larger, more negative values of $\psi_1$ would lead to a greater downward bias $\delta_i$.


# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent