---
title: "On the Bias of Complete- and Shifting-Case Meta-Regressions with Missing Covariates"
author: 
  - Jacob M. Schauer, Northwestern University
  - Karina Diaz, Columbia University
  - Jihyun Lee, The University of Texas at Austin
  - Therese D. Pigott, Georgia State University
csl: ../addons/apa.csl
output:
  bookdown::pdf_document2: 
    fig_caption: yes
    includes:
      in_header: ../addons/style.sty
    toc: false
  bookdown::word_document2: 
    reference_docx: ../addons/styles_word.docx
bibliography: ../addons/cca_references.json
---


# Introduction

Meta-regression is a useful tool for studying important sources of variation between effects in a meta-analysis [@borensteinIntroductionMetaanalysis2009; @tiptonHistoryMetaregressionTechnical2019].
Analyses of these models in the absence of missing data have been studied thoroughly in the literature [e.g.,
@berkeyRandomeffectsRegressionModel1995; @hedgesCombiningIndependentEstimators1983; @hedgesRobustVarianceEstimation2010; @konstantopoulosFixedEffectsVariance2011;
@viechtbauerAccountingHeterogeneityRandomeffects2007].
However, it is common for meta-analytic datasets to be missing data [@pigottReviewMethodsMissing2001]. 
In the context of meta-regression, issues with missing data frequently involve missing covariates [@pigottMissingPredictorsModels2001; @tiptonCurrentPracticesMetaregression2019].

Precisely how to proceed with a meta-regression with missing covariates remains something of an open question.
Statistical guidance suggests that analyses ought to consider the mechanism that causes covariates to be missing [@pigottMissingPredictorsModels2001; @pigottHandlingMissingData2019].
However, it appears that doing so is less common in practice for meta-analyses.
A recent review found that meta-regressions with missing data tend to take one of two strategies [@tiptonCurrentPracticesMetaregression2019]. 
An analyst may conduct a *complete-case analysis* (CCA) that excludes any effects for which a relevant covariate is missing (i.e., only analyze complete cases). 
However, if there are very few such effects, a common approach is to use *shifting units of analysis*, which we refer to in this article as a *shifting-case analysis* (SCA) [@cooperResearchSynthesisMetaanalysis2017]. 
Under a shifting-case analysis, analysts fit a series of meta-regression models on subsets of relevant covariates, so that each model selectively omits certain covariates.

Estimators used in both complete-case and shifting units analyses in some sense ignore effects for which a covariate is missing.
Ignoring missing data can potentially lead to biased estimates [see @littleStatisticalAnalysisMissing2002; @grahamMissingData2012].
Despite authors pointing out such issues in meta-analysis, these methods continue to enjoy widespread use [@pigottHandlingMissingData2019].
Existing literature on this discussion has yet to detail precisely how much bias can arise in a complete- or shifting-case analysis, nor is there exhaustive guidance on when these methods produce unbiased estimates.
In short, there is an understanding that these methods *can* induce bias, but less is known about *how much* and *under what conditions*.

This article examines the potential bias of complete- and shifting-case analyses. 
The following section provides a demonstration of these methods on data concerning a meta-analysis of substance abuse interventions [@tanner-smithAdolescentSubstanceUse2016].
We then introduce a statistical framework for studying bias for incomplete data meta-regressions that incorporates a model for whether or not a data point is observed.
Using this framework, we describe conditions under which complete- and shifting-case analyses are unbiased.
When these conditions are not met, we derive an approximation for the bias of complete- and shifting-case analyses using standard models for missingness and examine the magnitude of bias.



# Example: Substance Abuse Interventions

```{r, message = F, echo = F, error = F, warning = F}
library(tidyverse)

adt <- readRDS("../../data/adt_data.RDS")
dd <- adt %>%
  group_by(studyid, groupid1, groupid2) %>%
  arrange(desc(estimingpost)) %>%
  slice(1) %>%
  ungroup() %>%
  select(studyid, es_g, se_g, 
         g1hrs = g1hrsperweek, g2hrs = g2hrsperweek) %>%
  mutate(v_g = se_g^2, 
         g1hi = (g1hrs > 1.5), 
         g2hi = (g2hrs > 1.5), 
         g1_obs = ifelse(is.na(g1hi), 0, 1), 
         g2_obs = ifelse(is.na(g2hi), 0, 1), 
         g1_obs = factor(g1_obs), 
         g2_obs = factor(g2_obs))

n_study <- dd %>% distinct(studyid) %>% nrow()
n_effect <- dd %>% nrow()

mipat <- mice::md.pattern(dd %>% select(g1hi, g2hi), plot = FALSE)
counts <- as.integer(dimnames(mipat)[[1]][1:4])
mitab <- tibble(g1_intensity = rep(c("Observed", "Missing"), each = 2), 
                g2_intensity = rep(c("Observed", "Missing"), 2), 
                counts = counts) %>%
  mutate(percent = counts/n_effect) %>%
  select(`Group 1 Hi-Intensity` = g1_intensity,
         `Group 2 Hi-Intensity` = g2_intensity, 
         Count = counts, 
         Percent = percent)

# RVE
library(robumeta)

rv_mod_hiint <- robu(formula = es_g ~ g1hi + g2hi, 
                     data = dd, 
                     studynum = studyid, 
                     var.eff.size = v_g,
                     # rho = 0.5,
                     small = TRUE)

rv_mod_hiint_g1 <- robu(formula = es_g ~ g1hi, 
                        data = dd, 
                        studynum = studyid,
                        var.eff.size = v_g, 
                        small = TRUE)

rv_mod_hiint_g2 <- robu(formula = es_g ~ g2hi, 
                        data = dd, 
                        studynum = studyid,
                        var.eff.size = v_g, 
                        small = TRUE)

cc <- rv_mod_hiint$reg_table %>%
  select(labels, estimate = b.r, se = SE, p = prob) %>%
  bind_rows(., 
            tibble(labels = "tau2", 
                   estimate = 0.08))

sc1 <- rv_mod_hiint_g1$reg_table %>%
  select(labels, estimate_sc1 = b.r, se_sc1 = SE, p1 = prob)  %>%
  bind_rows(., 
            tibble(labels = "tau2", 
                   estimate_sc1 = 0.06))

sc2 <- rv_mod_hiint_g2$reg_table %>%
  select(labels, estimate_sc2 = b.r, se_sc2 = SE, p2 = prob)  %>%
  bind_rows(., 
            tibble(labels = "tau2", 
                   estimate_sc2 = 0.09))

tabcc <- left_join(
  left_join(cc, sc1), 
  sc2
) %>%
  mutate_if(is.numeric, round, 2) %>%
  replace_na(list(se = "", 
                  p = "", 
                  estimate_sc1 = "--", 
                  se_sc1 = "", 
                  p1 = "",
                  estimate_sc2 = "--", 
                  se_sc2 = "", 
                  p2 = "")) %>%
  mutate(Term = c("Intercept", "Group 1 Hi-Int.", "Group 2 Hi-Int.", "Variance Comp. $\\tau^2$")) %>%
  unite(se, se, p, sep = ", p = ") %>%
  unite(se_sc1, se_sc1, p1, sep = ", p = ") %>%
  unite(se_sc2, se_sc2, p2, sep = ", p = ") %>%
  unite(complete_case, estimate:se, sep = " (SE = ") %>%
  unite(shifting_case_g1, estimate_sc1:se_sc1, sep = " (SE = ") %>%
  unite(shifting_case_g2, estimate_sc2:se_sc2, sep = " (SE = ") %>%
  select(Term, `Complete-Case` = complete_case, `Shifting-Case Group 1` = shifting_case_g1, `Shifting-Case Group 2` = shifting_case_g2) %>%
  mutate_all(.funs = function(x) ifelse(grepl("\\(", x), paste0(x, ")"), x)) %>%
  mutate_all(.funs = function(x) gsub("SE = ,", ",", x)) %>%
  mutate_all(.funs = function(x) gsub("\\(, p = \\)", "", x)) 
```

@tanner-smithAdolescentSubstanceUse2016 conducted a meta-analysis that examined the effects of substance abuse interventions on future substance use among adolescents. 
The studies included in this meta-analysis involved a variety of different treatment types (e.g., cognitive behavioral therapy, family therapy, and pharmacological therapy) and treatment intensities (measured in hours per week), and were carried out in a variety of contexts, including in-patient and out-patient centers.
Tanner-Smith et al. used meta-regression models to study potential moderators of these effects, and their analyses had to contend with a number of effects that were missing covariates. 
While in practice, models were estimated via the expectation-maximization (EM) algorithm rather than complete- or shifting-case methods, we use a subset of this data in in order to illustrate complete- and shifting-case analyses.

Consider a subset of the Tanner-Smith et al. data comprising `r n_effect` effect estimates of substance abuse interventions from `r n_study` studies. 
These effect estimates involve contrasts between groups in a study that are subjected to different treatment conditions, denoted in the data as *Group 1* and *Group 2*, so that each treatment effect can be thought of as Group 1 minus Group 2.
Each effect estimate corresponds to a given contrast within a study.
Effect sizes are measures on the scale of bias-corrected standardized mean differences.
Often the same group (typically the control group) in a study was used in multiple contrasts, so that effect sizes in this meta-analysis are likely correlated.

Suppose an analysis of interest involves the impact of high- versus low-intensity interventions on treatment effects, where a high-intensity intervention consisted of more than 1.5 hours per week of treatment. 
Then this analysis might use a pair of binary covariates for each effect: one would indicate whether group 1 received a high-intensity intervention (i.e., $X_1 = 1$ if group 1 treatment was high-intensity) and the other would indicate whether group 2 received a high intensity (i.e., $X_{2} = 1$ if group 2 treatment was high-intensity).
The relevant meta-regression model would regress the effect estimates on these two covariates.

In the data, the treatment intensity is missing for some of the effects, and Table \@ref(tab:misspat) summarizes missingness for these covariates.
Table \@ref(tab:misspat) shows that only 37 of the `r n_effect` (50%) have a reported treatment intensity for both groups (i.e., $X_{1}, X_{2}$ are both observed), but that 54 (73%) of effects report Group 1's treatment intensity (i.e., $X_{1}$ is observed) and 41 (55%) effects report Group 2's treatment intensity.
```{r, echo = F}
knitr::kable(mitab, 
             digits = 2,
             format = "latex", 
             booktabs = TRUE,
             escape = FALSE,
             align = 'cccc',
             caption = "\\label{tab:misspat} \\textit{This table displays the total number and percentage of effect sizes that are missing covariates regarding whether Group 1 or Group 2 received high-intensity interventions in the substance abuse intervention meta-analysis.}")
```

A complete-case analysis would include only the 37 effects for which both covariates were observed.
Using robust variance estimation to account for dependence between effect sizes, a complete-case analysis would result in the coefficient estimates and standard errors displayed in the first column of Table \@ref(tab:ccadtexample).
Based on these estimates, when Group 1 receives a high-intensity treatment, we would expect an effect to be larger by $d = 0.44$ (in standard deviation units) than when Group 1 receives a low-intensity treatment, which is statistically significant at the $\alpha = 0.1$ level.
Note that the estimated between-effect variance is $\hat{\tau}^2 = 0.08$.
```{r, echo = F, message = F}
knitr::kable(tabcc, #rv_hiint_tab, 
             digits = 2,
             format = "latex", 
             booktabs = TRUE,
             escape = FALSE,
             align = 'lccccc', 
             caption = "\\label{tab:ccadtexample} \\textit{This table displays the meta-regression results for the model regressing effect sizes on high-intensity indicator variables. The coefficients are estimated using only complete cases (i.e., where both covariates are observed).}")
```


However, the model above is estimated on only half of the data.
Concern over using a small proportion of the data, or a relatively few number of effects often leads meta-analysts to opt for a shifting-case analysis.
An example of a shifting-case analysis would use the 54 effects for which Group 1's treatment intensity is observed (i.e., $X_{1}$ is observed), but only including $X_{1}$ in the model.
Doing so leads to the estimates in second column of Table \@ref(tab:ccadtexample).
Note that the coefficient estimate for Group 1's treatment intensity is still positive, but is roughly 60% the magnitude of the estimate in the complete-case model.

Finally, an analogous model in a shifting-case analysis would include the 55 effects for which Group 2's intensity is observed, and include only that covariate in the model.
The third column of Table \@ref(tab:ccadtexample) shows that this results in a coefficient estimate for Group 2's treatment intensity (0.16) that is in the opposite direction of the estimate from the complete case analysis (-0.21).

It should be noted that all of these estimates ought to be interpreted with caution. 
The complete-case analysis includes only half of the effect sizes, which comprises a missingness rate well beyond what might be considered negligible [@schaferMultipleImputationPrimer1999; @bennettHowCanDeal2001].
The shifting-case analyses include more of the data, but because each shifting-case model omits one of the covariates, these models are not equivalent to the model that includes both covariates [@cooperSynthesizingResearchGuide1998].
The remainder of this article quantifies the bias induced by omitting effect sizes and/or covariates from meta-regressions.


# Model and Notation

Suppose a meta-analysis involves $k$ effects estimated from collection of studies. 
For the $i$th effect, let $T_i$ be the estimate of the effect parameter $\theta_i$, and let $v_i$ be the estimation error variance of $T_i$. 
Denote a vector of covariates that pertain to $T_i$ as $X_i =[1, X_{i1}, \ldots, X_{ip}]$. Note that the first element of $X_i$ is a 1, which corresponds to an intercept term in a meta-regression model, and that $X_{ij}$ for $j = 1, \ldots p$ corresponds to different covariates.
The meta-regression model can be expressed as:
\begin{equation}
T_i | X_i, v_i, \eta = X_i \beta + u_i + e_i 
\label{eq:full-data-reg}
\end{equation}
Here, $\beta \in \mathbb{R}^{p+1}$ is the vector of regression coefficients. 
The estimation errors $e_i$ are typically assumed to be normally distributed with mean zero and variance $V[e_i] = v_i$, which is true of some effect sizes, and is an accurate large-sample approximation for others [@cooperHandbookResearchSynthesis2019].
The term $u_i$ represents the random effect such that $u_i \perp e_i$ and $V[u_i] = \tau^2$. 
This model is equivalent to the standard mixed-effects meta-regression model, and it is also consistent with subgroup analysis models [@hedgesFixedRandomeffectsModels1998; @cooperHandbookResearchSynthesis2019]. 
The vector $\eta = [\beta, \tau^2]$ refers to the parameters of model. 
Under a fixed-effects model, it is assumed that $\tau^2 = 0$, in which case $\eta = \beta$, and $u_i \equiv 0$.

A common assumption in random effects meta-regression is that the random effects $u_i$ are independent and normally distributed with mean zero and variance $\tau^2$ [@hedgesFixedRandomeffectsModels1998; @hedgesRandomEffectsModel1983; @lairdStatisticalMethodsCombining1990; @viechtbauerBiasEfficiencyMetaanalytic2005]: 
\[
u_i \sim N(0, \tau^2).
\]
In that case, the distribution $p(T | X, v, \eta)$ can be written as
\begin{equation}
p(T_i | X_i, v_i, \eta) = \frac{1}{\sqrt{2\pi(\tau^2 + v_i)}} e^{-\frac{(T_i - X_i\beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full-data-prob}
\end{equation}
Thus, the joint likelihood for all $k$ effects can be wrtiten as:
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = (2\pi)^{-k/2} \left[\prod_{i = 1}^k (\tau^2 + v_i)\right] e^{-\sum_{i=1}^k \frac{(T_i - X_i \beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full-data-prob-vec}
\end{equation}
where $\mathbf{T} \in \mathbb{R}^k$ is the vector of effect estimates, $\mathbf{v} \in \mathbb{R}^k$ is the vector of estimation variances, and $\mathbf{X} \in \mathbb{R}^{k \times (p+1)}$ is the matrix of covariates where each row of $\mathbf{X}$ is simply the row vector $X_i$. 
Note that the functions in both \@ref(eq:full-data-prob) and \@ref(eq:full-data-prob-vec) assume that *all* of the $p$ covariates are observed. 
Equation \@ref(eq:full-data-prob-vec) is referred to as the *complete-data likelihood function* [@gelmanBayesianDataAnalysis2014; @littleStatisticalAnalysisMissing2002].
We note that a meta-regression with no missing data will be accurate if the complete-data model is correctly specified. 
Thus, to illustrate the properties of incomplete data meta-regression, we assume that the complete-data model is correctly specified.

The vector of regression coefficient estimates for the complete-data model when there is no missing data is typically estimated by
\begin{equation}
\hat{\beta} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{T}
\label{eq:betahat-cd}
\end{equation}
Here, $\mathbf{W} = \text{diag}[1/(v_i + \tau^2)]$ is the diagonal matrix of weights.
The covariance matrix of $\hat{\beta}$ is given by
\begin{equation}
V[\hat{\beta}] = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} 
\label{eq:v-betahat-cd}
\end{equation}

This model can be expanded to account for dependent effect sizes by assuming that $T_i \in \mathbb{R}^{k_i}$ is a vector of $k_i$ effects from the same study, $e_i$ is vector of estimation errors, $u_i$ is a vector of random effects, and $e_i + u_i$ has covariance matrix $\Sigma_i$.
In this model, $X_i$ is a matrix of covariates for each effect in $T_i$.
The resulting formulas for the complete-data likelihood function and coefficient estimators will be more complex, but they will have a (relatively) similar form as the independent effect size model; indeed, coefficient estimates for the dependent effect size model are also linear transformations of the data.
Because of this, [WORK OUT SOMETHING HERE].

Not all relevant variables may be observed in a meta-analytic dataset.
Let $R_i$ be a vector of response indicators that correspond with effect $i$. 
This article concerns missing covariates, and we assume that $T_i$ and $v_i$ are observed for every effect of interest in a meta-analysis. 
Thus, each element $R_{ij}$ of $R_i$ corresponds to a covariate $X_{ij}$. 
The $R_{ij}$ take a value of either 0 or 1: $R_{ij} = 1$ indicates the corresponding $X_{ij}$ is observed and $R_{ij} = 0$, indicates a that the corresponding $X_{ij}$ is not observed.
Note that $R_i \in \mathcal{R} \equiv \{0,1\}^p$ is a vector of 0s and 1s of length $p$.
For instance, $X_{i2}$ were missing, then $R_{i2} = 0$.

Denote $O = \{(i, j): R_{ij} = 1\}$ as the indices of covariates that are observed and $M = \{(i, j): R_{ij} = 0\}$ be the set of indices for missing covariates. 
Then, the complete-data model can be written as 
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = p(\mathbf{T} | \mathbf{X}_{O}, \mathbf{X}_{M}, \mathbf{v}, \eta). 
\label{eq:full-data-prob_mis}
\end{equation}
Note that the complete-data model depends on entries of $\mathbf{X}_{M}$, which are unobserved. 
<!-- When meta-analytic datasets are missing covariates, analyses involve incomplete data.  -->
<!-- In practice, meta-analysts have frequently turned to CCA and SCA estimators for incomplete data meta-regression [@cooperResearchSynthesisMetaanalysis2017; @pigottHandlingMissingData2019; @pigottMissingPredictorsModels2001; @tiptonCurrentPracticesMetaregression2019; @tiptonHistoryMetaregressionTechnical2019].  -->


## Complete-Case Estimators

A common approach in meta-regression with missing covariates is to use a complete-case analysis [@pigottHandlingMissingData2019; @tiptonCurrentPracticesMetaregression2019]. 
This approach simply omits rows in the data for which any covariate is missing. 
Thus, this analysis method only uses effects and covariates for which $R_i = [1, \ldots, 1] = \mathbbm{1}$. 

Let $C = \{i : R_i = \mathbbm{1}\}$ index all relevant effects $i$ such that $R_i = \mathbbm{1}$, so that  $\mathbf{X}_C$ is the matrix of covariates such $R_i = \mathbbm{1}$, $\mathbf{T}_C$ is the corresponding subset of effect estimates, and $\mathbf{W}_C$ is the corresponding subset of weights. 
The complete-case analysis estimates coefficients $\beta$ with:
\begin{equation}
\hat{\beta}_C
  = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \mathbf{T}_C
\label{eq:beta-c}
\end{equation}



## Shifting-Case Estimators

When there are multiple covariates of interest, each of which has some missingness, there may only be a few effects for which all covariates of interest are observed. 
When that happens, a complete case analysis can be unfeasible. 
A common solution to this in meta-analysis is to use an available-case analysis [@pigottHandlingMissingData2019].
In practice, an *available-case* meta-regression is often equivalent to a shifting-case analysis, referred to in the literature as *shifting units of analysis* [@cooperResearchSynthesisMetaanalysis2017; @tiptonCurrentPracticesMetaregression2019].

Shifting-case analyses involve fitting multiple regression models, each including a subset of the covariates of interest. 
Sometimes this even takes the form of regressing effect estimates on one covariate at a time [@pigottHandlingMissingData2019; @tiptonCurrentPracticesMetaregression2019]. 
In the substance abuse data example, we focused on two covariates of interest $X_{i1}$ and $X_{i2}$. 
The SCA first regressed $T_i$ on observed values of $X_{i1}$. 
This regression included observations for which both $X_{i1}$ and $X_{i2}$ are observed (i.e., $R_i = [1, 1]$) and observations for which $X_{i1}$ is observed but $X_{i2}$ is missing (i.e., $R_i = [1, 0])$.
<!-- Therefore, the regression involves effects $i$ for which $R_i \in \mathcal{R}_1 = \{[1, 1], [1, 0]\}$. -->
We then regressed $T_i$ on $X_{i2}$, which included effects for which $R_i \in \{[1, 1], [0, 1]\}$.
In sum, the SCA demonstrated in the previous section involved two regressions, each of which conditioned on different sets of missingness patterns.

To formalize SCA estimators, consider a single regression in an SCA, and let $S$ index the component of $X_i$ (i.e., the intercept term and relevant covariates) included in that model $S = \{j : j = 0 \text{ or } X_{ij} \text{ in analysis}\}$.
Let $E$ be the complement of $S$ so that $E$ indexes the covariates excluded from the regression.
Then the regression is used to estimate and make inferences about $\beta_S$, which is a subset of the full vector of coefficients $\beta$. 
In the first substance abuse SCA regression, $T_i$ was regressed on only $X_{i1}$, so that $\beta_S = [\beta_0, \beta_1]$.
Denote $\mathcal{R}_j$ as the set of missingness patterns such that all included covariates are observed: $\mathcal{R}_j = \{R \in \mathcal{R}: R_S = \mathbbm{1}\}$.
Note that $\mathcal{R}_j$ contains missingness patterns such that all the included covariates are observed, but any excluded covariates may be either observed or unobserved.
For instance, in the first substance abuse SCA regression of $T_i$ on $X_{i1}$, the analysis included effects such that $R_i \in \mathcal{R}_1 = \{[1,1], [1,0]\}$.
Finally, let $U(S)$ be the set of effects for which $X_{iS}$ are observed $U(S) = \{i : R_i \in \mathcal{R}_j\}$.
Then, the shifting-case estimators for $\beta_S$ are given by:
\begin{equation}
\hat{\beta}_S = 
(\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1} \mathbf{X}_{US}^T \mathbf{W}_U \mathbf{T}_U
\label{eq:beta-s}
\end{equation}
where $\mathbf{X}_{US}$ contains the columns of $\mathbf{X}$ that pertain to the covariates that are included in the model, and the rows for which all of those covariates are observed.
The matrix $\mathbf{W}_U$ contains the rows of $\mathbf{W}$ for which $X_{iS}$ are observed (and similarly for $\mathbf{T}_U$).


## Missingness Mechanisms

Both the complete- and shifting-case estimators are analyses of incomplete data. 
Analyses of incomplete data require some assumption about why data are missing, which is referred to as the missingness *mechanism*.
The mechanism by which missingness arises is typically modeled through the distribution of $R$. 
Let $\psi$ denote the parameter (or vector of parameters) that index the distribution of $R$ so that the probability mass function of $R$ can be written as $p(R | T, X, v, \psi)$.
Assumptions about the missingness mechanism are therefore equivalent to assumptions about $p(R | T, X, v, \psi)$.

@rubinInferenceMissingData1976 defined three types of mechanisms in terms of the distribution of $R$.
Data could be missing completely at random (MCAR), which means that the probability that a given value is missing is independent of all of the observed or unobserved data:
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) = p(\mathbf{R} | \psi)
\]
MCAR implies that probability that a given value is missing is unrelated to anything observed or unobserved, and depends only on the missingness parameter $\psi$.

Covariates could be missing at random (MAR), which implies the distribution of missingness depends only on observed data:.
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) = 
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O,\mathbf{v}, \psi)
\]
MAR differs from MCAR in that missingness might be related to observed values. 
As an example, if with larger standard errors are less likely to report the racial composition of their samples, then missingness would depend on the (observed) estimation error variances. 
Data missing according to this mechanism would violate an assumption of MCAR, since missingness is related to an observed value.

Finally, data are said to be missing not at random (MNAR) if the distribution of $R$ depends on unobserved data in some way.
In the context of the meta-regression data, this would imply that $R$ is related to $\mathbf{X}_M$, so that the probability of a covariate not being observed depends on the value of the covariate itself.
For instance, data would be MNAR if studies with larger standard errors and a greater proportion of minorities are less likely to report the racial composition of their samples because the likelihood that racial composition is not reported will depend on the composition itsef.

A related concept in missing data is that of *ignorability*, which means that the missingness pattern does not contribute any additional information.
When missing data are ignorable, it is not necessary to know (or estimate) $\psi$ in order to conduct inference on $\eta$ [@gelmanBayesianDataAnalysis2014; @grahamMissingData2012; @littleStatisticalAnalysisMissing2002; @vanbuurenFlexibleImputationMissing2018].
In practice, missing data are ignorable if they are MAR and if $\psi$ and $\eta$ are distinct.






# Conditional Incomplete Data Meta-Regression

Because both complete- and available-case analyses depend on the value of $R_i$, they can be seen as models that condition on missingness. 
Models that condition on missingness are not necessarily identical to the complete-data model, which is the model of interest, because the complete-data model does not condition on $R_i$.
Yet, complete- and available-case analyses proceed as if the complete-data and conditional models on missingness are equivalent. 
Doing so ignores the missingness mechanism and its potential impact on the accuracy of analytic results. 

The complete-data model can be related to the conditional models through the distribution of missingness $R_i$. 
This approach is referred to as a *selection model* in the missing data literature [@gelmanBayesianDataAnalysis2014; @littleStatisticalAnalysisMissing2002; @vanbuurenFlexibleImputationMissing2018].
We can write the selection model for meta-regression with missing covariates as:
\begin{equation}
p(T_i | X_i, v_i, R_i \in \mathcal{R}_j, \eta, \psi) 
  = \frac{p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta)}{p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta)} 
\label{eq:selection-model}
\end{equation}
where $\psi$ indexes the distribution of $R | T, X, v$.
Here, $\mathcal{R}_j$ refers to the relevant subset of $\mathcal{R}$ on which the analysis conditions; for a complete-case analysis, $\mathcal{R}_j = \{\mathbbm{1}\}$.

Equation \@ref(eq:selection-model) describes the conditional model as a function of the complete-data model $p(T_i | X_i, v_i, \eta)$ and a selection model $p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)$ that gives the probability that a given set of covariates are observed.
The denominator on the right hand side of \@ref(eq:selection-model) is a normalizing factor that is equivalent to the probability of observing the missingness pattern $\mathcal{R}_j$ given the estimation error variance $v_i$ and the observed and unobserved covariates in the vector $X_i$, and can be written as
\begin{equation}
p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta) = \int p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta) dT_i
\label{eq:pr-xv}
\end{equation}

Note that when the complete-data model in \@ref(eq:full-data-prob) is not equivalent to the conditional model in \@ref(eq:selection-model), the resulting coefficient estimators in a meta-regression can be biased. 
To see this, we can write:
\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] 
  = E[T_i | X_i, v_i] + \delta_{ij} 
  = X_i \beta + \delta_{ij}
\label{eq:bias-delta}
\end{equation}
Here, we see that the expectation of $T_i$ conditional on $R_i$ can be written as the complete-data expectation $X_i \beta$ plus a bias term $\delta_{ij}$. 
If $\delta_{ij} \neq 0$, it follows that conditioning on $R_i$ induces bias in the distribution of $T_i$ used in an analysis. 
Because the complete-case estimator in \@ref(eq:beta-c) and the shifting-case estimator in \@ref(eq:beta-s) are weighted averages of the $T_i$, the following sections show that they can be biased if $\delta_{ij} \neq 0$.
The precise magnitude of the $\delta_{ij}$ will depend on the selection model in \@ref(eq:selection-model) and hence on the missingness mechanism.

A standard approach for modeling missingness mechanisms for covariates is to assume $R_i$ follows some log-linear distribution [@agrestiCategoricalDataAnalysis2013]. 
Various authors have described approaches to modelling $R$ for missing covariates in generalized linear models that include logistic and multinomial logistic models [@ibrahimIncompleteDataGeneralized1990; @ibrahimMissingCovariatesGeneralized1999; @lipsitzConditionalModelIncomplete1996]. 
Thus, one class of models for missingness would involve the logit probability of observing some missingness patterns $R_i \in \mathcal{R}_j \subset \mathcal{R}$:
\begin{equation}
\text{logit}[p(R_i \in \mathcal{R}_j | T_i, X_i, v_i)] = \sum_{m = 0}^{m_j} \psi_{mj} f_{mj}(T_i, X_i, v_i) 
\label{eq:r-loglinear}
\end{equation}
where $f_{0j}(T_i, X_i, v_i) = 1$, so that $\psi_{0j}$ would be the intercept term for the logit model for the set of missingness patterns $\mathcal{R}_j$.
While log-linear models are not the only applicable or appropriate selection model, we make this assumption at points throughout this article in order to demonstrate conditions under which conditional meta-regressions are inaccurate, and how inaccurate they can be.



## Approximate Bias for Log-linear Selection Models

As argued above, the bias of complete-case estimators $\hat{\beta}_C$ or shifting-case estimators $\hat{\beta}_S$ will depend in some way on the bias $\delta_{ij}$ induced in $T_i$ by conditioning on $R_i \in \mathcal{R}_j$.
The magnitude and direction of $\delta_{ij}$ will in turn depend on the missingness mechanism.

It is possible to derive an approximation for $\delta_{ij}$ when $R_i | T_i, X_i, v_i$ under certain conditions when $R_i$ follows a log-linear distribution. 
If $p(T_i | X_i, v_i)$ is the standard fixed- or random effects meta-regression model in equation \@ref(eq:full-data-prob), and $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i) = H_j(T_i, X_i, v_i)$ follows the log-linear model in \@ref(eq:r-loglinear), then
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional-bias}
\end{equation}
where $H_j(X_i \beta, X_i, v_i)$ is equivalent to $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i)$ evaluated at $T_i = X_i\beta$ and 
\[
f_{mj}'(X_i\beta, X_i, v_i) = \left.\frac{\partial f_{mj}}{\partial T_i}\right\rvert_{T_i = X_i\beta}
\]
is the derivative of $f_{mj}$ with respect to $T_i$ evaluated at $T_i = X_i\beta$.
The detailed proof is presented in Appendix A.

While the following sections will examine possible values that $\delta_{ij}$ may take under different selection models, we can gain some insight on bias by examining \@ref(eq:conditional-bias).
The expression for $\delta_{ij}$ depends on three main quantities.
First, $\delta_{ij}$ is an increasing of $H_j(X_i\beta, X_i, v_i)$, which is the probability that $R_i \not\in \mathcal{R}_j$.
This implies that the bias will be greater as the probability of omitting an observation increases.
Second, $\delta_{ij}$ increases in $\tau^2 + v_i$, which means that it will be larger when $T_i$ vary more around the regression line.
Finally, $\delta_{ij}$ depends on $\psi_{mj} f_{mj}'(X\beta, X, v)$. 
Note that $f_{mj}'$ is the derivative of $f_{mj}$ with respect to $T$. 
If $f_{mj}$ does not depend on $T$, $f_{mj}' = 0$. 
Thus, $\delta_{ij}$ depends on the components of the selection model that are functions of $T$ and how important those components are.


<!-- Then an approximation for $E[T | X, v, R = r]$ is as follows: -->
<!-- \begin{align*} -->
<!-- E[T | X, v, R = r]  -->
<!--   & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_j \psi_j f_j(T, X, v)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)} \log\left(A + e^{\sum_j \psi_j f_j(T, X, v)}\right)} dT \\ -->
<!--   & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_j \psi_j f_j(T, X, v) - \log\left(A + e^{\sum_j \psi_j f_j(X\beta, X, v)}\right)\right.}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} \\ -->
<!--   & \qquad\qquad \frac{\left.- G(\sum_{j \in \mathcal{T}} \psi_j f_j(X, v))(T - X\beta) + O(T^2)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} dT \\ -->
<!--   & \approx \int \frac{T \exp\left\{-\frac{1}{2(\tau^2 + v)}\left(T^2 - 2TX\beta - 2T (\tau^2 + v)\sum_{j \in \mathcal{T}} \psi_j f_j(X, v)\right.\right.}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} \\ -->
<!--   & \qquad\qquad \frac{\left.\left.+ 2T(\tau^2 + v) G(\sum_{j \in \mathcal{T}} \psi_j f_j(X, v)) + \ldots\right)\right\}}{g(X, v)\sqrt{2 \pi (\tau^2 + v)}} dT \\ -->
<!--   & = X\beta + (1 - G)(\tau^2 + v)\sum_{j \in \mathcal{T}} \psi_j f_j(X, v) -->
<!-- \end{align*} -->



# Bias in Complete-Case Analyses

Complete-case analyses only include effects for which all relevant covariates are observed. 
The complete-case coefficient estimator $\hat{\beta}_C$ given in equation \@ref(eq:beta-c) conditions on $R_i = \mathbbm{1}$.
As noted above, conditioning on $R_i$ can induce bias, however there are conditions under which the complete case analysis will lead to unbiased coefficient estimates. 
First, if the covariates are MCAR, so that $R_i \perp T_i, X_i, v_i$, then 
\[
p(R_i = \mathbbm{1} | T_i, X_i, v_i, \psi) 
  = p(R_i = \mathbbm{1} | \psi)  
\]
and hence 
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}, \eta, \psi) 
  = \frac{p(R_i = \mathbbm{1} | \psi) p(T_i | X_i, v_i, \eta)}{p(R_i = \mathbbm{1} | \psi)}
  = p(T_i | X_i, v_i, \eta)
\label{eq:cc-mcar}
\end{equation}
Thus, the complete-data and conditional models are equivalent.
Assuming $X$ is MCAR, then $\hat{\beta}_C$ in \@ref(eq:beta-c) will be unbiased estimator of $\beta$.
This is consistent with broader results on analyses of MCAR data in primary study context [@littleStatisticalAnalysisMissing2002].

A complete-case analysis is also unbiased under slightly less restrictive assumptions. 
Suppose that $R_i \perp (X_i, T_i) | v_i$, then the complete-data model and the conditional model are equivalent:
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}, \eta, \psi) 
  = \frac{p(R_i = \mathbbm{1} | v_i, \psi) p(T_i | X_i, v_i, \eta)}{p(R_i = \mathbbm{1} | v_i, \psi)}
  = p(T_i | X_i, v_i, \eta)
\label{eq:cc-mar}
\end{equation}
The assumption that $R_i \perp (X_i, T_i) | v_i$ implies that if missingness only depends on the estimation error variances, then a complete case analysis may be unbiased. 
This is a weaker assumption than MCAR, which requires $R_i \perp (T_i, X_i, v_i)$.
Intuitively, if both $X_i$ and $T_i$ are conditionally independent of $R_i$, then so is the relationship between $X_i$ and $T_i$.
Under this assumption, $\hat{\beta}_C$ will be unbiased.

For most effect size indices, variances $v_i$ are functions of the sample sizes within studies $n_i$. 
Some effect sizes, such as the $z$-transformed correlation coefficient, have variances $v_i$ that depend entirely on the sample size of a study, while for other effect sizes this is approximately true, such as the standardized mean difference. 
For such effect sizes, this assumption implies that missingness depends only on the sample size of the study. 
This may be true, for instance, if smaller studies are less likely to report more fine-grained demographic information regarding their sample out of concern for the privacy of the subjects who participated in the study (and that no other factors affect missingness).

An even weaker assumption can also lead to unbiased estimation with complete cases. 
When $R_i \perp T_i | X_i, v_i$, we can write:
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}, \eta, \psi) 
  = \frac{p(R_i = \mathbbm{1} | X_i, v_i, \psi) p(T_i | X_i, v_i, \eta)}{p(R_i = \mathbbm{1} | X_i, v_i, \psi)}
  = p(T_i | X_i, v_i, \eta)
\label{eq:cc-mnar}
\end{equation}
Thus, if $R_i \perp T_i | X_i, v_i$, then the complete-case model will be the same as the complete-data model. 
Note that $R_i$ can still depend on $X_i$, including $X_{ij}$ that are not observed, which means that the data are MNAR. 

<!-- This result is consistent with the bias approximation in equation \@ref(eq:conditional-bias)).  There, the bias was a function of $f_{ij}'$, which is the derivative of $f_{ij}(T, X, v)$ with respect to $T$.  -->
<!-- If $f_{ij}$ does not depend on $T$, so that $f_{ij}(T, X, v) \equiv f_{ij}(X, v)$ then the derivative will be zero, and hence $\psi_{ij}$ will not contribute to the bias. -->
<!-- Thus, only parts of the missingness model that depend on $T$ will contribute to bias. -->

However, when $R_i$ is not independent of $X_i$ or $T_i$ (given $v_i$), then CCA can be biased. 
Let $\mathcal{R}_1 = \{\mathbbm{1}\}$ so that the CCA conditions on $R_i \in \mathcal{R}_1$.
Based on equation \@ref(eq:bias-delta), the bias of $\hat{\beta}_C$ will depend on the $\delta_{i1}$. 
If we let $\Delta = [\delta_{11}, \ldots, \delta_{k1}]$ be the vector of $\delta_{i1}$ and let $\Delta_C$ be the subset of $\Delta$ for which all covariates are observed (i.e., $R_i = \mathbbm{1}$). 
Then the bias of the complete-case analysis can be written as 
\begin{equation}
\text{Bias}[\hat{\beta}_C] = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \Delta_C
\label{eq:cc-bias-delta}
\end{equation}
The bias in equation \@ref(eq:cc-bias-delta) is a weighted average of individual biases $\delta_{i1}$.
Hence, the bias will be larger if the $\delta_{i1}$ are larger.

Precisely, how large the bias in \@ref(eq:cc-bias-delta) is will depend on the distribution of $R_i$ and its relationship to effect estimates $T_i$ and their covariates $X_i$.
When $R_i$ follows the log-linear model in \@ref(eq:r-loglinear), the approximate bias can be written as 
\begin{equation}
\text{Bias}[\hat{\beta}_C]
  = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{H}_{1C}  \mathbf{f}_{1C} \psi_1
\label{eq:cc-bias-loglinear}
\end{equation}
where 
\[
\mathbf{H}_1 = \text{diag}[H_1(X_i\beta, X_i, v_i)]
\]
is a $k \times k$ diagonal matrix where entries refer to the probability that an observation is *not* a complete case, 
\[
\mathbf{f}_1 = \left[f_{01}'(X_i^T\beta, X_i, v_i), \ldots, f_{m_1 1}'(X_i^T\beta, X_i, v_i) \right]
\]
is a $k \times m_1$ matrix of derivatives, and $\psi_1 = [\psi_{01}, \ldots, \psi_{m_1 1}]^T$ is a vector of parameters that index the selection model.
Note that the bias in \@ref(eq:cc-bias-loglinear) involves $\hat{H}_{1C}$ which contains the rows of $\mathbf{H}_1$ for which $R_i = \mathbbm{1}$; similarly for $\mathbf{f}_{1C}$.

While \@ref(eq:cc-bias-loglinear) provides a general expression for the approximate bias of $\hat{\beta}_C$, it can be a little difficult to interpret.
Loosely, we can see that the bias depends on the probability that observations are omitted due to missingness $\mathbf{H}_{1C}$, as well as some function of the components of the log-linear model $\mathbf{f}_{1C} \psi_1$.
To better intuit this bias, we provide a simple example in the following section.


## Example: Complete-Case Analysis with a Single Binary Covariate 

Suppose the model of interest includes a single binary covariate covariate $X_{i1} \equiv X_i \in \{0, 1\}$, so that the complete data model is 
\begin{equation}
T_i = \beta_0 + \beta_1 X_i + u_i + e_i
\label{eq:cc-example}
\end{equation}
where $\beta_0, \beta_1$ are the regression coefficients of interest.
Note that $\beta_0$ is the average effect when $X_i = 0$ and $\beta_1$ is the contrast in mean effects for when $X_i = 1$ versus when $X_i = 0$.

Because $X_i$ is a scalar, so is $R_i$; $R_i = 0$ indicates that $X_i$ is missing, $R_i = 1$ indicates that $X_i$ is observed. 
A complete-case analysis would include only effects $i$ for which $X_i$ is observed (i.e., $R_i = 1$).
The complete-case estimator for $\beta_0$ is given by a weighted sum of $T_i$ among the effects for which $X_i = 0$ and $R_i = 1$:
\begin{equation}
\hat{\beta}_{0C} = \frac{\sum_{i: X_i = 0, R_i = 1} w_i T_i}{\sum_{i: X_i = 0, R_i = 1} w_i}
\label{eq:b0c-ex}
\end{equation}
The complete-case estimator for $\beta_1$ is given by the difference between the (weighted) mean effect for $X_i = 1$ versus $X_i = 0$:
\begin{equation}
\hat{\beta}_{1C} = \frac{\sum_{i: X_i = 1, R_i = 1} w_i T_i}{\sum_{i: X_i = 1, R_i = 1} w_i} - \hat{\beta}_{0C}
\label{eq:b1c-ex}
\end{equation}

Assume that the selection model is log-linear, and that missingness depends on the size of the effect $T_i$ and the value of $X_i$:
\begin{equation}
\text{logit}[p(R_i = 1 | T_i, X_i, v_i)] 
  = \psi_0  + \psi_1 T_i + \psi_2 X_i
\label{eq:cc-loglinear}
\end{equation}
<!-- \textcolor{blue}{\textit{[Comment-J] (1) Are we assuming $\psi_0=0$ here? (2) Just one thought?: How about changing the model to $logit[p(R_i=0)]$ to have a consistent direction with other components to bias ($H(0)$, $\tau^2 + v$)? And when we say "missingness model", then usually the probability of being missing is modeled. So, this made me pause to think about the opposite direction, especially when Figure 1 is explained later and in the caption of Figure 1. But it is still good with the presented way!}} -->
Note that this is an MNAR mechanism, since it depends on $X_i$. 
Under this model, $H_{1}(X_i\beta, X_i, v_i)$ depends only on $X_i$ and not $v_i$, so we can write $H_1(X_i) = p(R \neq \mathbbm{1} | T_i, X_i, v_i)\rvert_{T_i = X_i^T \beta}$.
As well, $f_{11}(T_i, X_i, v_i) = T_i$ and $f_{21}(T_i, X_i, v_i) = X_i$. 
Given the result in equation \@ref(eq:conditional-bias), we can write
\begin{equation}
\delta_{i1}
   \approx H_1(X_i)(v_i + \tau^2)\psi_1
\label{eq:cc-ex-delta}
\end{equation}

Under this selection model, the bias of the complete-case estimator for $\beta_0$ is:
\begin{equation}
\text{Bias}[\hat{\beta}_{0C}] 
  \approx H_1(0)(\bar{v}_0 + \tau^2)\psi_1
\label{eq:cc-bias-b0}
\end{equation}
where $\bar{v}_0$ is the average estimation error variance $v_i$ among effects for which $X_i = 0$ and $R_i = 1$.

The expression in \@ref(eq:cc-bias-b0) depends on three key quantities, and is an increasing function of each of those quantities.
First, the bias increases in $H_1(0)$, which is essentially the probability that a case is missing any covariates; as covariates are more likely to be missing, the bias will be greater.
Second, it is increasing in $\bar{v}_0 + \tau^2$, the average variation of $T_i$ for which $X_i = 0$; the greater the variation, the greater the bias.
Finally, the bias depends on $\psi_1$, which is the relationship between an $X_i$ being observed and the size of $T_i$.

To gain better insight into equation \@ref(eq:cc-bias-b0), suppose $v_i \approx v = \bar{v}_0$ so that each study has roughly the same estimation error variance. 
If we assume $T_i$ is on the scale of a standardized mean difference, $v_i \approx 4/n_i$ where $n_i$ is the total sample size used to compute $T_i$. 
Various researchers have described conventions for the magnitude of $\tau^2$ that range from $\tau^2 = v/4$ to $\tau^2 = v$ [@hedgesPowerStatisticalTests2001; @hedgesPowerStatisticalTests2004; @hedgesStatisticalAnalysesStudying2019].
Thus, we can write $\tau^2 + v = 4(1 + r)/n$ from some constant $r$ that ranges from 0 to 1. 

The parameter $\psi_1$ is a log-odds ratio, which reflects the odds of a complete case for $T_i$ versus $T_i - 1$.
There are various conventions for the size of an odds ratio that depend on base rates $P[R = \mathbbm{1} | T]$.
Conventions used by @cohenStatisticalPowerAnalysis1988 have been interpreted as implying that a "small" odds ratio is about 1.49, a "medium" odds ratio is about 3.45, and a "large" odds ratio is about 9.0. 
@fergusonEffectSizePrimer2009 suggests 2.0, 3.0, and 4.0 for small, medium, and large odds ratios, while @chenHowBigBig2010 provide a range of conventions for different base rates, and their tables are roughly consistent with about 1.5 being a small odds ratio, 2.4 being medium, and 4.1 being large. 
@haddockUsingOddsRatios1998 suggests any odds ratio over 3.0 would be considered quite large.
Thus, consider a range of odds ratios from about 1.5 to 4.5.

However, the actual size of $\psi_1$ will depend on the scale of $T_i$.
A difference of $T_i - T_j = 1$ is considered quite large for standardized mean differences. 
A less extreme difference $D_T = |T_i - T_j|$ for a standardized mean difference would be no larger than the size of an individual $T_i$.
Conventions for standardized mean differences imply that a "small" effect would be about $T_i = 0.2$, a "medium" effect would be $T_i = 0.5$, and a "large" effect would be $T_i = 0.8$ [@cohenStatisticalPowerAnalysis1988].
Thus, meaningful values of $D_T$ might feasibly range from 0.2 to 1.0.
These conventions for odds ratios and $D_T$ would imply that relevant values of $|\psi_1|$ might range from 0.4 (large $D_T$ with small odds ratio) to over 7.5 (small $D_T$ with large odds ratio).

Based on these conventions, Figure \@ref(fig:delta) shows the potential (approximate) bias of $\hat{\beta}_{0C}$ for this example. 
Each panel corresponds to a given within-study variance $v = 4/n$ and residual heterogeneity $\tau^2$. 
Panels plot the bias contributed by a single case $\delta_i$ as a function of the probability of missingness $H_1(0)$ ($x$-axis) and $\psi_1$ (color).
The panels on the bottom few rows and left most columns show that if both $\psi_1$ is small and $\tau^2 + v$ is small, then $\delta_i$ will be less than 0.05. 
However if $\tau^2 + v_i$ is larger and the probability of a complete case is strongly related to $T_i$ (i.e., $\psi_1$ is large), then the bias can be greater than 0.2 or even 0.5 (Cohen's $d$).

It is worth noting that Figure \@ref(fig:delta) gives the bias for when $T_i$ is positively correlated with $X_i$ being fully observed, and hence $\psi_1 > 0$. 
This need not be the case when $\psi_1 < 0$. 
When $\psi_1 < 0$, then the bias of $\hat{\beta}_{0C}$ is negative, and would be be a mirror image of those in Figure \@ref(fig:delta).
Larger, more negative values of $\psi_1$ would lead to a greater downward bias.

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/delta_plot_cts}
\end{center}
\caption{This figure plots the bias of the intercept estimate $\hat{\beta}_{0C}$ ($y$-axis) of the example. Bias is shown as a function of the average sampling variance $v$, residual heterogeneity $\tau^2$, the probability of missingness $H_1(0)$ ($x$-axis), and the correlation between missingness and the effect size as measured by $\psi_1$ (color). Note that $\psi_1$ is a log-odds ratio for effect sizes on the scale of Cohen's $d$.}
\label{fig:delta}
\end{figure}


The bias of $\hat{\beta}_{1C}$ under selection model \@ref(eq:cc-loglinear) is given by:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}]
  \approx \left[H(1)(\bar{v}_1 + \tau^2) - H(0)(\bar{v}_0 + \tau^2)\right]\psi_1 
\label{eq:cc-bias-beta1-example}
\end{equation}
where $\bar{v}_1$ is the mean $v_i$ among effects for which $X_i = 1$ and $R_i = 1$. 
As with $\hat{\beta}_{0C}$, the bias of $\hat{\beta}_{1C}$ is an increasing function of $\psi_1$. 
If $T_i$ has a strong positive correlation with $p(R_i=1)$, then $\psi_1$ will be larger and so will the bias of $\hat{\beta}_{1C}$.

When all studies have approximately the same estimation error variance so that $v_i \approx v$ and $\bar{v}_0 \approx \bar{v}_1$, then the bias of $\hat{\beta}_{1C}$ is approximately:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}] 
  \approx \left[H(1) - H(0)\right] (v + \tau^2) \psi_1 
\label{eq:cc-bias-b1-simp}
\end{equation}
The expression in \@ref(eq:cc-bias-b1-simp) depends on three quantities. 
Like $\hat{\beta}_{0C}$, the bias of $\hat{\beta}_{1C}$ is an increasing function of $\tau^2 + v$ and $\psi_1$.
The bias of $\hat{\beta}_{1C}$ also increases as a function of $H(1) - H(0)$, which can be thought of as a difference in missingness rates between cases where $X_i = 1$ and $X_i = 0$.
Recall that $H$ is an approximation of the probability $X$ is missing given $X$ and $T$: $P[R \neq \mathbbm{1} | T, X]$. 
Viewed this way, the difference $H(1) - H(0)$, and hence the bias of $\hat{\beta}_{1C}$ will be greater if $R$ is strongly correlated with $X$ or if it is strongly correlated with $T$.
Taken together, the bias of $\hat{\beta}_{1C}$ will be greatest when there are fewer complete cases, missingness is strongly related to the size of effects or the value of the covariate $X$. 

To gain insight into the bias of $\hat{\beta}_{1C}$, consider the values of $\psi_1 \in [0.4, 7.5]$ and $\tau^2 + v = 4(1 + r)/n$ discussed above. 
The difference $H(1) - H(0) = p(R = 0 | X = 1, \eta) - p(R = 0 | X = 0, \eta)$ is just a difference in conditional probabilities. 
For reference, because both $R_i$ and $X_i$ are binary, then $p(R = 0 | X = 1) - p(R = 0 | X = 0)$ would be equal to the correlation between $R$ and $X$ (assuming equal marginals in a $2 \times 2$ table).
Thus, $|p(R = 0 | X = 1) - p(R = 0 | X = 0)|$ could be as small as 0, but could possibly be as large as 1, though conventions on the size of correlations suggest that $|p(R = 0 | X = 1) - p(R = 0 | X = 0)| = 0.5$ would be a "large" value [@cohenStatisticalPowerAnalysis1988].

Figure \@ref(fig:b1) shows the potential bias of $\hat{\beta}_{1C}$ for this example assuming the values discussed above.
Each panel corresponds to a given amount of heterogeneity $\tau^2 + v$, and within panels the bias is shown as a function of the difference $p(R = 0 | X = 1) - p(R = 0 | X = 0)$ ($x$-axis) and $\psi_1$ (color).
Figure \@ref(fig:b1) highlights that the relationship between $R_i$ and $T_i$ ($\psi_1$) and between $R_i$ and $X_i$ ($x$-axes) can affect the magnitude of the bias. 
If $R_i$ is strongly correlated with both $X_i$ and $T_i$ the bias can be as large as $d$ = 0.3 or 0.4. 
However, the less $R_i$ depends on $T_i$ or $X_i$, the lower the bias is. 

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/bias_beta1_ex1}
\end{center}
\caption{This figure plots the bias of $\hat{\beta}_{1C}$ ($y$-axis). Each panel corresponds to a given value of residual heterogeneity $\tau^2$ and estimation error variance $v$. Within panels, the bias of $\hat{\beta}_{1C}$ is plotted as function of differential missingness rates ($p(R = 0 | X = 1) - p(R = 0 | X = 0)$), which is analogous to the correlation between the value of $X$ and whether it is observed. Bias is also shown as a function of $\psi_1$ which is the relationship between the probability of observing $X$ and the effect size $T$. Bias is shown on the scale of Cohen's $d$ and $\psi_1$ is on the scale of a log-odds ratio.}
\label{fig:b1}
\end{figure}

It is worth noting that the missingness mechanism in this example is MNAR. 
However, the bias is largely a function of $\psi_1$, which is the coefficient in the selection model for the effect size $T_i$. 
The data would be MAR under a selection model where $\psi_2 = 0$ in \@ref(eq:cc-loglinear).
In that case, the bias of $\hat{\beta}_C$ would be identical to the biases presented above. 

For context, suppose the analysis of interest in the substance abuse example involved only the indicator for whether Group 1 received a high-intensity intervention. 
The missingness rate for Group 1's intensity is 27%.
Using a logit model analogous to \@ref(eq:cc-loglinear), we get an estimate of $\psi_1 = 2.08$. 
Based on the heterogeneity $\tau^2 = 0.08$ estimated in the example and the mean within-study variance $v = 0.08$, this would give a bias of about $d = 0.09$ in the intercept $\beta_0$.



<!-- - Continuous covariates? -->

<!-- For a single continuous covariate, note that  -->
<!-- \[ -->
<!-- \hat{\beta}_1 = \frac{\sum w_i (T_i - \bar{T}_\cdot)(X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- \] -->
<!-- Given the selection model above, the bias of $\hat{\beta}_1$ can be expressed as: -->
<!-- \[ -->
<!-- \psi_1 \frac{\sum (1 - G(X_i))(X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- + \psi_3 \frac{\sum (1 - G(X_i)) X_i (X_i - \bar{X}_{\cdot})}{\sum w_i (X_i - \bar{X}_{\cdot})^2} -->
<!-- \] -->

<!-- It is not immediately clear that these simplify. Perhaps we leave this result out? -->
<!-- [Comment-J] I agree! -->



# Bias in Shifting-Case Analyses

Shifting-case analyses (SCA) are a common approach in meta-regression when there are very few complete cases across multiple covariates. 
These analyses involve fitting multiple regression models, where each model omits some of the covariates of interest.
In this sense, shifting-case analyses can be thought of as a set of regression models. 
Consider one model from that set, which estimates regression coefficients for some subset $S$ of the relevant covariates using the estimator $\hat{\beta}_S$ in equation \@ref(eq:beta-s).
Recall that $E$ refers to the set of covariates omitted from the model, and that the estimator $\hat{\beta}_S$ conditions on a set of missingness patterns $R_i \in \mathcal{R}_j$. 
The set of missingness patterns $\mathcal{R}_j$ is such that $R_{iS} = 1$ so that all included covariates are observed.



To understand the conditions under which $\hat{\beta}_S$ is unbiased, we can write a shifting-case model as:
\begin{equation}
p(T_{i} | X_{iS}, v_i, R_i \in \mathcal{R}_j, \eta, \psi) = \frac{p(R_i \in \mathcal{R}_j | T_i, X_{iS}, v_i, \psi) p(T_i | X_{iS}, v_i, \eta)}{p(R_i \in \mathcal{R}_j | X_{iS}, v_i, \eta, \psi)}
\label{eq:sca-model}
\end{equation}

The model in \@ref(eq:sca-model) is slightly different from the models in the previous sections in that all of the functions depend on the covariates included in a given regression $X_{iS}$ rather than the complete set of relevant covariates $X_i$.
Thus, the function $p(T_i | X_{iS}, v_i)$ can be thought of as a partial-data model, since it omits relevant covariates. 
The partial-data model $p(T_i | X_{iS}, v_i)$ need not be equivalent to the complete-data model $p(T_i | X_i, v_i)$ because the former conditions only on $X_{iS}$ and not the full set of covariates $X_i$.
These models would only be equivalent if $T_i \perp X_{iE} | X_{iS}, v_i$. 
That is, unless the excluded covariates are completely unrelated to effect size (given the covariates included in the SCA model), then $\hat{\beta}_S$ will be biased even if $X_{iS}$ are completely observed.

The model in \@ref(eq:sca-model) suggests a very strict set of conditions for which $\hat{\beta}_S$ is unbiased. 
First, missingness must be independent of effect sizes. 
This arises if $R_i \perp T_i | X_{iS}, v_i$ or $R_i \perp (T_i, X_{iS}) | v_i$. 
This is a similar assumption as that made for unbiased complete-case analyses, and amounts to effect sizes (and potentially covariates) being independent of missingness.

Second, any excluded covariates must be completely irrelevant given the included covariates: $T_i \perp X_{iE} | X_{iS}, v_i$. 
This amounts to $\beta_j = 0$ for all $j \in E$.
A related assumption is that $(T, X_{iS}) \perp X_{iE} | v$, which would imply that the complete data likelihood involves no interactions between $X_{iS}$ and $X_{iE}$ and that $X_{iS}$ and $X_{iE}$ are orthogonal.
Note that conditions on omitted covariates *and* omitted observations must hold in order for $\hat{\beta}_S$ to be unbiased.

When the assumptions about omitted variables and cases are not met, $\hat{\beta}_S$ will be biased. 
Just how biased will depend on a number of factors, including the amount of missingness, the missingness mechanism, and the relevance of any excluded covariates.
The bias can be expressed as:
\begin{equation}
\text{Bias}[\hat{\beta}_S] = (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{UE} \beta_E + (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \Delta_{jU}
\label{eq:bias-sca}
\end{equation}
where $\mathbf{X}_{UE}$ is the matrix of omitted covariates and $\beta_E$ comprises the coefficients for the omitted covariates. The term $\Delta_j$ is a vector of biases due to missingness $\Delta_j = [\delta_{1j}, \ldots, \delta_{kj}]$ and $\Delta_{jU}$ is the subset of $\Delta_j$ for which $R_i \in \mathcal{R}_j$.
Note that the $\delta_{ij}$ are the biases due solely to missingness as in equation \@ref(eq:conditional-bias):
\[
\delta_{ij} = E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] - X_i \beta
\]

The expression in \@ref(eq:bias-sca) shows that a shifting-case analysis suffers from two sources of bias. 
The first source, captured in the first term in \@ref(eq:bias-sca), is a function of the coefficients for the excluded covariates $\beta_E$.
This is referred to in the statistical and econometric literature as *omitted variable bias* [e.g., @farrarMulticollinearityRegressionAnalysis1967; @melaImpactCollinearityRegression2002]. 
Omitted variable bias arises even if no $X_{iS}$ are missing, and is related to the issue of multicollinearity in linear models.
In fact, if the columns in $\mathbf{X}_{US}$ and $\mathbf{X}_{UE}$ are orthogonal, so that the omitted variables are independent of the included variables, then the omitted variable bias will be zero. 
When the omitted variables are not orthogonal to the included variables, the bias will be nonzero, and it will depend in large part on the contribution of the omitted variables $\mathbf{X}_{UE} \beta_E$. 
The estimator $\hat{\beta}_S$ will have greater bias if the coefficients for the omitted variables $\beta_E$ are larger and the omitted covariates $\mathbf{X}_{UE}$ are correlated with the included covariates $\mathbf{X}_{US}$.

The second term in \@ref(eq:bias-sca) captures the bias due to ignoring observations missing $X_{iS}$.
This *missingness bias* is a function of $\Delta_{jU}$, which is itself a vector of biases for each effect, and it can be understood in terms of its individual components $\delta_{ij}$.
Because the $\delta_{ij}$ are of the same form for the complete-case versus shifting-case models, the missing data bias for a shifting-case analysis is governed by similar factors as the complete-case analyses, and are quite possibly similar in magnitude. 
Based on \@ref(eq:conditional-bias), $\delta_{ij}$ will be positive if $T_i$ is positively correlated with whether $R_i \in \mathcal{R}_j$, and $\delta_{ij}$ will be greater in magnitude when that correlation is larger.

Taken together, the bias of shifting-case analysis can be greater than the bias of a complete-case analysis.
This occurs if the omitted variable bias and the missingness bias are in the same direction (e.g., both are positive).
For both biases to be in the same direction, correlation between $T_i$ and the omitted variables $X_{iE}$ must be in the same direction as the correlation between $T_i$ the probability that $X_{iS}$ is observed.
If, however, the omitted variable and missingness biases are in opposite directions, this can reduce the bias of a shifting-case estimator.
It is worth noting, however, that it will almost always be impossible to confirm the direction of biases, since they depend on potentially unobserved covariates.




## Example: Shifting-Cases Analysis with Two Binary Covariates

Suppose $X_i = [1, X_{i1}, X_{i2}]$ and $X_{i1}$ and $X_{i2}$ are binary covariates such that 
\begin{equation}
T_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + u_i + e_i
\label{eq:sca-ex}
\end{equation}
If there is missingness in both $X_{i1}$ and $X_{i2}$, then $R_i \in \{0, 1\}^2$ so that $R_i = [1,1]$ indicates both covariates are observed, and $R_i = [1, 0]$ indicates only $X_{i1}$ is observed. 
If missingness is such that $R_i = [1, 1]$ for very few effect estimates, then a shifting-case analysis might involve regressing $T_i$ on the observed values of $X_{i1}$ and then on the observed values of $X_{i2}$. 

The first regression would take only rows for which $X_{i1}$ is observed, so that $R \in \mathcal{R}_{1} = \{[1,1], [1,0]\}$ and the excluded $X_{i2}$ could be either 0 or 1. 
The shifting-case estimators follow from equation \@ref(eq:beta-s):
\[
\hat{\beta}_{0S} = \frac{\sum_{X_1 = 0} w_i T_i}{\sum_{X_1 = 0} w_i}, 
\qquad \hat{\beta}_{1S} = \frac{\sum_{X_1 = 1} w_i T_i}{\sum_{X_1 = 1} w_i} - \hat{\beta}_{0S}
\]

Assume that missigness follows the following log-linear model:
\begin{equation}
\text{logit}[p(R_i \in \mathcal{R}_{1} | T_i, X_{i1}, v_i)] = \psi_{0} + \psi_{1} T_i + \psi_{2} X_{i1} 
\label{eq:logit-sca}
\end{equation}
Note that this gives the log-odds that an effect is included in the model given $T_i$ and $X_{i1}$, and that $X_{i2}$ is not involved.
Further, because the distribution of $R_i$ depends on $X_{i1}$, the mechanism is MNAR.

Given the selection model in \@ref(eq:logit-sca), the bias of the coefficient estimators can be written as:
\begin{align}
\text{Bias}[\hat{\beta}_{0S}]
  & = \beta_2 \frac{\sum_{X_1 = 0, X_2 = 1} w_i}{\sum_{X_i = 0} w_i} + \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 0} w_i}  \\
\text{Bias}[\hat{\beta}_{1S}]
  & = \beta_2 \left(\frac{\sum_{X_{i1} = 1, X_{i2} = 1} w_i}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0, X_{i2} = 1} w_i}{\sum_{X_{i1} = 0} w_i}\right) + \left(\frac{\sum_{X_{i1} = 1} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 0} w_i}\right)
\end{align}
Here $\tilde{\delta}_{i1}$ are the missingness biases as defined above, and whose approximate values is given in \@ref(eq:conditional-bias). 
To distinguish from the $\delta_{i1}$ from the complete-case example, we use the $\tilde{\delta}$ notation.

Both the bias of $\hat{\beta}_{0S}$ and $\hat{\beta}_{1S}$ depend on two terms. 
The first term in each expression is the omitted variable bias, and the second term in each expression is the missingness bias.
Consider the omitted variable biases.
When effects are estimated with roughly the same precision, so that $w_i \approx w$, then the omitted variable biases reduce to 
\begin{align}
\text{Omitted Var. Bias}[\hat{\beta}_{0S}]
  & = \beta_2 p(X_2 = 1 | X_1 = 0) \label{eq:omvar-b0}\\
\text{Omitted Var. Bias}[\hat{\beta}_{1S}]
  & = \beta_2 \left[p(X_2 = 1 | X_1 = 1) - p(X_2 = 1 | X_1 = 0) \right] \label{eq:omvar-b1}
\end{align}

The omitted variable biases for each coefficient can be seen as depending on two quantities. 
Both \@ref(eq:omvar-b0) and \@ref(eq:omvar-b1) are increasing in $\beta_2$, which is the contribution of $X_{i2}$ to the complete-data model.
The omitted variable bias for $\hat{\beta}_{0S}$ is also increasing in $p(X_2 = 1 | X_1 = 0)$.
The bias for $\hat{\beta}_{1S}$ in \@ref(eq:omvar-b1) is increasing in $p(X_2 = 1 | X_1 = 1) - p(X_2 = 1 | X_1 = 0)$. 
Because both $X_{i1}$ and $X_{i2}$ are binary, this difference is roughly equivalent to their Pearson correlation (assuming equal marginals). 
If $X_{i1} \perp X_{i2}$, then their correlation is zero, and the omitted variable bias will be zero.
But if $X_{i1}$ and $X_{i2}$ are correlated, the bias of $\hat{\beta}_1$ will depend on how strongly correlated $X_{i1}$ and $X_{i2}$ are, and how big $\beta_2$ is.

Figure \@ref(fig:omitted-bias) shows the omitted variable bias of $\hat{\beta}_0$ (left plot) and $\hat{\beta}_1$ as a function of $\beta_2$. 
Both the bias and $\beta_2$ are shown on the scale of Cohen's $d$. 
In the left plot $\pi_{01} = p(X_{i2} = 1 | X_{i1} = 0)$ is the proportion of $X_{i2} = 1$ when $X_{i1} = 0$. 
In the right plot, $\rho_{12} = p(X_{i2} = 1 | X_{i1} = 1) - p(X_{i2} = 1 | X_{i1} = 0)$, which is roughly the correlation between $X_{i1}$ and $X_{i2}$.
Note that because $\rho_{12}$ can be intuited as (roughly) a Pearson correlation, the values in the figure include 0, 0.1 (i.e., a "small" correlation), 0.3 (medium correlation), and 0.5 (large correlation) [@cohenStatisticalPowerAnalysis1988].

The figure shows that if $\beta_2 = 0$ so that $X_{i2}$ is independent of $T_i$ given $X_{i1}$, that both $\hat{\beta}_{0S}$ and $\hat{\beta}_{1S}$ will be unbiased.
However, when $\beta_2$ is nonzero, both estimators will be biased.
If $X_{i1}$ and $X_{i2}$ are highly correlated, or if $X_{i2} = 1$ when $X_{i1} = 0$ with high probability, the bias of both estimators will about as large as a "small" effect (i.e., $d = 0.2$) when $\beta_2$ is larger than 0.2. 
For $\hat{\beta}_{1S}$ the bias will be less than about $d = 0.05$ when $|\beta_2| \leq 0.1$ or if $\rho_{12} < 0.5$. 

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/omitted_var_bias}
\end{center}
\caption{This figure shows the omitted variable bias of $\hat{\beta}_{0S}$ and $\hat{\beta}_{1S}$ for the model in \@ref(eq:sca-ex) as a function of the omitted variable coefficient $\beta_2$. The bias ($y$-axis) and $\beta_2$ ($x$-axis) are on the scale of Cohen's $d$. The bias displayed is solely due to omitting $X_{i2}$ from \@ref(eq:sca-ex). In the left plot, lines are colored according to $\pi_{01} = p(X_{i2} = 1 | X_{i1} = 0)$. In the right plot, lines are colored according to $\rho_{12} = p(X_{i2} = 1 | X_{i1} = 1) - p(X_{i2} = 1 | X_{i1} = 0)$.}
\label{fig:omitted-bias}
\end{figure}

Figure \@ref(fig:omitted-bias) does not take into account any bias induced by missingness. 
However, because the missingness mechanism in \@ref(eq:logit-sca) is the same as the mechanism for the complete-case example \@ref(eq:cc-loglinear), the missingness bias for $\hat{\beta}_{0S}$ is the same as that for $\hat{\beta}_{0C}$ in \@ref(eq:cc-bias-b0), which is shown in Figure \@ref(fig:delta). 
Likewise, the missingness bias for $\hat{\beta}_{1S}$ is the same as that for $\hat{\beta}_{1C}$ in \@ref(eq:cc-bias-beta1-example), which is shown in Figure \@ref(fig:b1).

Thus, the total bias of $\hat{\beta}_{0S}$ will be the sum of the omitted variable biases shown in Figure \@ref(fig:delta) and the missingness biases shown in Figure \@ref(fig:omitted-bias).
If both the omitted and missingness biases are on the higher end, the total bias of $\hat{\beta}_0$ might be as large as $d = 0.6$ to over 1.0.
Likewise, the total bias of $\hat{\beta}_{1S}$ will be the sum of the omitted variable biases shown in Figure \@ref(fig:b1) and the missingness biases shown in Figure \@ref(fig:omitted-bias), and can be larger than $d = 0.6$.

As noted above, the missingness bias and omitted variable bias can be in the different directions. 
For instance, if $\beta_2 < 0$ but $\tilde{\delta}_{ij} > 0$, then the omitted variable bias for $\hat{\beta}_{0S}$ will be negative, but the missingness bias will be positive. 
In such cases, the bias of the shifting case estimators could be smaller than the bias of the complete-case estimators.
However, because the biases depend on unknown (and potentially unobserved) quantities, it will often be impossible to empirically verify the magnitude or direction of the bias. 

In the substance abuse example, consider the regression that omits Group 2's covariates. 
Recall that the estimate for $\beta_2$ on complete cases was $\hat{\beta}_2 = -0.21$.
Among the observed data, we find that $p(X_2 = 1 | X_1 = 0) = 0.08$ and $p(X_2 = 1 | X_1 = 1) = 0.55$. 
Given these values, the omitted variable bias for the intercept $\beta_0$ would be $d = -0.02$, and the omitted variable bias for $\beta_1$ would be $d = -0.1$. 
Note, however, that the total bias further depends on the bias due to missingness.
Based on empirical estimates in the previous section, this would give a bias in the estimate of $\beta_0$ of about $d = 0.07$. 


# Discussion

This article described a selection model approach to studying the bias of two common methods for conducting meta-regressions with missing covariates: complete-case and shifting-case analyses. 
Under certain assumptions regarding the selection model, we obtained expressions for the approximate bias of coefficient estimators.
These expressions were presented in a general form, which was then unpacked by way of examples. 

We found that both complete-case and shifting-case analyses will produce biased coefficient estimates unless certain conditions are met. 
Complete-case estimators are unbiased if the probability that all relevant covariates are observed is (conditionally) independent of the effect size estimate.
Shifting-estimators are unbiased if, in addition to effect sizes being independent of missingness, the covariates omitted from a model have no relationship with the effect size. 
When these conditions are not met, the bias of coefficient estimates can be substantial---as large as $d = 0.4$ to $d = 0.8$---depending on the missingness mechanism, the missingness rate, an the relevance of any omitted covariates.

An important aspect of these results is that bias will depend on unknown parameters and unobserved data. 
This means that it will be impossible to empirically verify the magnitude or direction of the bias.
Even the estimated biases from the substance abuse data, which were on the order of about $d = \pm 0.1$ may not be entirely accurate, as so much of that data is missing.
Further, it will require strong assumptions regarding the missingness mechanism to correct any bias. 
These assumptions may be buttressed by theory about scientific report, data collection, and data curation.

It is not immediately clear how commonly the conditions required for unbiased complete- and shifting-case estimators arise.
Recent empirical work on examining missingness in meta-analytic datasets found that effect sizes can be strongly correlated with missingness, though this is not always the case [@schauerExploratoryAnalysesMissingunderreview]. 
Further, the issues of multicollinearity and confounding in meta-regression, including those discussed by @lipseyThoseConfoundedModerators2003, would suggest that omitting variables in a shifting-case analysis are likely to induce bias.

Based on these results, our primary recommendation is that analysts attempt to understand the missingness mechanisms and patterns in their data.
This can leverage knowledge about standard reporting and coding practices, as well as exploratory analyses [@schauerExploratoryAnalysesMissingunderreview].
If there is very little missingness, or if there is good reason to assume that missingness is uncorrelated with effect size estimates, a complete-case analysis may be a reasonable option.
However, we would discourage analysts from continuing to use shifting-case analyses because it would seem unlikely that omitted variable biases are zero in practice.

We would also suggest analysts investigate the feasibility of alternative estimation methods. 
@ibrahimIncompleteDataGeneralized1990 describes an EM algorithm for generalized linear models with missing covariates, and @ibrahimMissingCovariatesGeneralized1999 extend that algorithm when covariates are MNAR.
In addition, full-information maximum likelihood (FIML) has long been used in linear models [@grahamMissingDataAnalysis2009; @grahamMissingData2012], and has shown some promise for meta-regression involving continuous covariates. 
Finally, multiple imputation (MI) has become something of a standard approach for handling missing data across a number of fields [@rubinMultipleImputationNonresponse1987; @littleStatisticalAnalysisMissing2002; @vanbuurenFlexibleImputationMissing2018]. 

However, employing any of these alternative strategies is not necessarily straightforward for meta-analysts.
To our knowledge, the EM algorithm for missing covariates has yet to be implemented in standard meta-analytic software.
Although FIML for meta-regression model is available in SEM framework [@cheungHandlingMissingCovariates2019], the approach has not been empirically validated under various conditions.
How best to specify quality imputation models for MI analyses is something of an open question for meta-regression, as is the potential inaccuracies incurred by using poor imputation models.
Research on and clear implementation of these methods for meta-regression model would seem to be of great use for meta-analysts.



\clearpage

\appendix


# Approximate Bias for Log-Linear Selection Models

**Proposition:** Suppose $p(T_i | X_i, v_i)$ is the standard fixed- or random effects meta-regression model in equation \@ref(eq:full-data-prob), and suppose $p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i) = H_j(T_i, X_i, v_i)$ follows the log-linear model in \@ref(eq:r-loglinear). Then:

\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] \approx X_i\beta + H_j(X_i\beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional_expectation}
\end{equation}
where $f_{mj}'(X_i \beta, X_i, v_i) = \frac{\partial}{\partial T_i} f_{mj}(T_i, X_i, v_i)\rvert_{T_i = X_i^T \beta}$.
Therefore, the bias of the conditional expectation is given by:
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional-bias-appendix}
\end{equation}

**Proof:**

In this proof, we drop the subscript $i$ for sake of simplicity. 
Denote
\begin{align*}
H_j(X\beta, X, v) \equiv H_j(X, v) 
  & = P[R =\not\in \mathcal{R}_j | T, X, v] \rvert_{T = X\beta} \\
G_j(X, v) 
  & = 1 - H_j(X, v) \\
g_j(X, v)
  & = P[R \in \mathcal{R}_j | X, v]
\end{align*}

Then an approximation for $E[T | X, v, R \in \mathcal{R}_j]$ is as follows:
\begin{align*}
  & \text{omitting subscripts, Taylor series for exponents in } P[R \in \mathcal{R}_j | T, X, v] \text{ at } T = X  \beta\\
E[T | X, v, R \in \mathcal{R}_j] 
  & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij}(T, X, v)\right\}}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)} \left(1 + e^{\sum_i \psi_{ij} f_{ij}(T, X, v)}\right)} dT \\
  & = \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij} (X \beta, X, v) \right. \\
  & \qquad\qquad + \sum_i \psi_{ij} f_{ij}'(X \beta, X, v)(T - X \beta) - \log\left(1 + e^{\sum_i \psi_{ij} f_{ij} (X  \beta, X, v)}\right) \\
  & \qquad\qquad \left. - G_j(X, v)(\sum_i \psi_{ij} f_{ij}'(X  \beta, X, v))(T - X\beta) + O(T^2)\right\} dT \\
  & \approx \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{1}{2(\tau^2 + v)}\left(T^2 - 2TX\beta - 2T (\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X \beta, X, v)\right.\right. \\
  & \qquad\qquad \left.\left.+ 2T(\tau^2 + v) G_j(X, v)(\sum_{i \in \mathcal{D}} \psi_{ij} f_{ij}'(X  \beta, X, v)) + \ldots\right)\right\} dT \\
  & = X\beta + H_j(X, v)(\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X  \beta, X, v)  \qquad\qquad\qquad \blacksquare
\end{align*}

Note that this uses a first order Taylor expansion of the log-linear model at $T = X \beta$, and thus assumes the $f_{mj}$ are differentiable.
The approximation will be more accurate if $\tau^2 + v_i$ are small.
A more accurate approximation is possible if the $f_{mj}$ are linear in $T_i$. 
In that case, only an approximation of the denominator of the log-linear model is required.




\clearpage

# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent