% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={On the Bias of Complete- and Shifting-Case Meta-Regressions with Missing Covariates},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
%%%

%-----------------------------------------------% 
% Math Type
%-----------------------------------------------% 
% \usepackage{mathspec}
% \usepackage{amsmath,amsthm}
% \allowdisplaybreaks
\usepackage{bbm}


%-----------------------------------------------% 
% Text flow
%-----------------------------------------------% 
\usepackage{ragged2e}
\RaggedRight

\setlength{\parindent}{.5in}
\setlength{\parskip}{0em}

\usepackage{setspace}\doublespacing

\usepackage[nofiglist, notablist, tablesfirst]{endfloat}


%-----------------------------------------------% 
% Captions
%-----------------------------------------------% 
% \usepackage{floatrow}
% \floatsetup[table]{capposition=top}
% \floatsetup[figure]{capposition=top}
% \captionsetup{width=.75\textwidth}
\usepackage{subfig}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{On the Bias of Complete- and Shifting-Case Meta-Regressions with Missing Covariates}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Meta-regression is a useful tool for studying important sources of variation between effects in a meta-analysis.\textsuperscript{1,2}
Analyses of these models in the absence of missing data have been studied thoroughly in the literature.\textsuperscript{3--7}
However, it is common for meta-analytic datasets to be missing data.\textsuperscript{8}
In the context of meta-regression, issues with missing data frequently involve missing covariates.\textsuperscript{9,10}

Precisely how to proceed with a meta-regression when missing covariates remains something of an open question.
Statistical guidance suggests that analyses ought to consider the mechanism that causes covariates to be missing.\textsuperscript{9,11}
However, it appears that doing so is less common in practice for meta-analyses.
A recent review found that meta-regressions with missing data tend to take one of two strategies.\textsuperscript{10}
An analyst may conduct a \emph{complete-case analysis} (CCA) that excludes any effects for which a relevant covariate is missing (i.e., only analyze complete cases).
This is often referred to as ``listwise deletion'' in data analyses.
However, if there are very few such effects, a common approach is to use \emph{shifting units of analysis}, which we refer to in this article as a \emph{shifting-case analysis} (SCA).\textsuperscript{12}
In an SCA, analysts fit a series of meta-regression models on subsets of relevant covariates, so that each model selectively omits certain covariates.
This is equivalent to ``pairwise deletion'' in data analyses.

Both CCA and SCA ignore effects for which a covariate is missing.
Ignoring missing data can potentially lead to biased estimates of parameters of interest.\textsuperscript{13,14}
Despite authors pointing out such issues in meta-analysis, these methods continue to enjoy widespread use.\textsuperscript{11}
Existing meta-analysis literature on this discussion has yet to detail precisely how much bias can arise in a complete- or shifting-case analysis, nor is there exhaustive guidance on when these methods produce unbiased estimates.
In short, there is an understanding that these methods \emph{can} induce bias, but less is known about \emph{how much} and \emph{under what conditions}.

This article examines the potential bias of complete- and shifting-case analyses.
The following section provides a demonstration of these methods on data concerning a meta-analysis of substance abuse interventions.\textsuperscript{15}
We then introduce a statistical framework for studying bias for incomplete data meta-regressions that incorporates a model for whether or not a covariate is observed.
Using this framework, we describe conditions under which CCA and SCA are unbiased.
When these conditions are not met, we derive an approximation for the bias of CCA and SCA using standard models for missingness and examine the magnitude of bias.
We find that bias is highly dependent on the precise mechanism by which data are missing, and is less reliant on more traditional missingness mechanism classifications (e.g., missing at random vs.~not at random).

\hypertarget{example-substance-abuse-interventions}{%
\section{Example: Substance Abuse Interventions}\label{example-substance-abuse-interventions}}

Tanner-Smith et al.\textsuperscript{15} conducted a meta-analysis that examined the effects of substance abuse interventions on future substance use among adolescents.
The studies included in this meta-analysis involved a variety of different treatment types (e.g., cognitive behavioral therapy, family therapy, and pharmacological therapy) and treatment intensities (measured in hours per week), and were carried out in a variety of contexts, including in-patient and out-patient centers.
Tanner-Smith et al.~used meta-regression models to study potential moderators of these effects, and their analyses had to contend with a number of effects that were missing covariates.
While in practice, models were estimated via the expectation-maximization (EM) algorithm rather than complete- or shifting-case methods, we use a subset of this data in order to illustrate complete- and shifting-case analyses.

Consider a subset of the Tanner-Smith et al.~data comprising 74 effect estimates of substance abuse interventions from 46 studies.
These effect estimates involve contrasts between groups in a study that are subjected to different treatment conditions, denoted in the data as \emph{Group 1} and \emph{Group 2}, so that each treatment effect can be thought of as Group 1 minus Group 2.
Typically, researchers avoided no-treatment or placebo conditions in studies over ethical concerns surrounding the failure to treat adolescents with substance abuse disorders.
Thus, contrasts within studies (i.e., effect estimates) tended to focus on a specific treatment of interest to the researcher versus some alternate treatment.
Effect estimates are reported on the scale of bias-corrected standardized mean differences.

Suppose the analysis of interest involves the impact of high- versus low-intensity interventions on treatment effects, where a high-intensity intervention consisted of more than 1.5 hours per week of treatment.
Then this analysis might use a pair of binary covariates for each effect: one would indicate whether group 1 received a high-intensity intervention (i.e., \(X_1 = 1\) if group 1 treatment was high-intensity) and the other would indicate whether group 2 received a high intensity (i.e., \(X_{2} = 1\) if group 2 treatment was high-intensity).
The relevant meta-regression model would regress the effect estimates on these two covariates.

In the data, the treatment intensity is missing for some of the effects, and Table \ref{tab:misspat} summarizes missingness for these covariates.
Table \ref{tab:misspat} shows that only 37 of the 74 (50\%) have a reported treatment intensity for both groups (i.e., \(X_{1}\) and \(X_{2}\) are both observed), but that 54 (73\%) of effects report Group 1's treatment intensity (i.e., \(X_{1}\) is observed) and 41 (55\%) effects report Group 2's treatment intensity.

\begin{table}

\caption{\label{tab:unnamed-chunk-2}\label{tab:misspat} \textit{This table displays the total number and percentage of effect sizes that are missing covariates regarding whether Group 1 or Group 2 received high-intensity interventions in the substance abuse intervention meta-analysis.}}
\centering
\begin{tabular}[t]{cccc}
\toprule
Group 1 Hi-Intensity & Group 2 Hi-Intensity & Count & Percent\\
\midrule
Observed & Observed & 37 & 0.50\\
Observed & Missing & 17 & 0.23\\
Missing & Observed & 4 & 0.05\\
Missing & Missing & 16 & 0.22\\
\bottomrule
\end{tabular}
\end{table}

A complete-case analysis would include only the 37 effects for which both covariates were observed.
Using robust variance estimation to account for dependence between effect sizes, a CCA would result in the coefficient estimates and standard errors displayed in the first column of Table \ref{tab:ccadtexample}.
Based on these estimates, when Group 1 receives a high-intensity treatment, we would expect an effect to be larger by \(d = 0.44\) (in standard deviation units) than when Group 1 receives a low-intensity treatment, which is statistically significant at the \(\alpha = 0.10\) level.
Note that the estimated between-effect variance is \(\hat{\tau}^2 = 0.08\).

\begin{table}

\caption{\label{tab:unnamed-chunk-3}\label{tab:ccadtexample} \textit{This table displays the meta-regression results for the model regressing effect sizes on high-intensity indicator variables when using complete- and shifting-case analyses.}}
\centering
\begin{tabular}[t]{lccc}
\toprule
Term & Complete-Case & Shifting-Case Group 1 & Shifting-Case Group 2\\
\midrule
Intercept & 0.11 (SE = 0.06, p = 0.11) & 0.14 (SE = 0.06, p = 0.02) & 0.15 (SE = 0.06, p = 0.03)\\
Group 1 Hi-Int. & 0.44 (SE = 0.16, p = 0.06) & 0.27 (SE = 0.15, p = 0.07) & --\\
Group 2 Hi-Int. & -0.21 (SE = 0.26, p = 0.46) & -- & 0.16 (SE = 0.26, p = 0.54)\\
Variance Comp. $\tau^2$ & 0.08 & 0.06 & 0.09\\
\bottomrule
\end{tabular}
\end{table}

However, the model above is estimated on only half of the data.
Concern over using a small proportion of the data, or a relatively few number of effects often leads meta-analysts to opt for a shifting-case analysis.
An example of an SCA would use the 54 effects for which Group 1's treatment intensity is observed (i.e., \(X_{1}\) is observed), but only including \(X_{1}\) in the model.
Doing so leads to the estimates in second column of Table \ref{tab:ccadtexample}.
Note that the coefficient estimate for Group 1's treatment intensity is still positive, but is roughly 60\% the magnitude of the estimate in the complete-case model.

Finally, an analogous model in an SCA would include the 41 effects for which Group 2's intensity is observed, and include only that covariate in the model.
The third column of Table \ref{tab:ccadtexample} shows that this results in a coefficient estimate for Group 2's treatment intensity (0.16) that is in the opposite direction of the estimate from the CCA (-0.21).

It should be noted that all of these estimates and comparisons between them ought to be interpreted with caution.
The complete-case analysis includes only half of the effect sizes, which comprises a missingness rate well beyond what might be considered negligible.\textsuperscript{16,17}
The shifting-case analyses include more of the data, but because each shifting-case model omits one of the covariates, these models are not equivalent to the model that includes both covariates.\textsuperscript{18}
It could even be argued that the parameters in the model with both covariates are not comparable to parameters in models with only one covariate; coefficients in a model with multiple covariates must be interpreted in relation to other variables in the model.
The remainder of this article quantifies the bias induced by omitting effect sizes and/or covariates from meta-regressions.

\hypertarget{model-and-notation}{%
\section{Model and Notation}\label{model-and-notation}}

Suppose a meta-analysis involves \(k\) effects estimated from collection of studies.
For the \(i\)th effect, let \(T_i\) be the estimate of the effect parameter \(\theta_i\), and let \(v_i\) be the estimation error variance of \(T_i\).
Denote a vector of covariates that pertain to effect estimate \(T_i\) as \(X_i =[1, X_{i1}, \ldots, X_{ip}]\). Note that the first element of \(X_i\) is a 1, which corresponds to an intercept term in a meta-regression model, and that \(X_{ij}\) for \(j = 1, \ldots p\) corresponds to different covariates.
The meta-regression model can be expressed as:
\begin{equation}
T_i | X_i, v_i, \eta = X_i \beta + u_i + e_i 
\label{eq:full-data-reg}
\end{equation}
Here, \(\beta \in \mathbb{R}^{p+1}\) is the vector of regression coefficients.
The estimation errors \(e_i\) are typically assumed to be normally distributed with mean zero and variance \(V[e_i] = v_i\).
This assumption is true of some effect size indices and is a very accurate large-sample approximation for others.\textsuperscript{19}
The term \(u_i\) represents the random effect such that \(u_i \perp e_i\) and \(V[u_i] = \tau^2\).
This model is equivalent to the standard mixed-effects meta-regression model, and it is also consistent with subgroup analysis models.\textsuperscript{19,20}
The vector \(\eta = [\beta, \tau^2]\) refers to the parameters of model.
Under a fixed-effects model, it is assumed that \(\tau^2 = 0\), in which case \(\eta = \beta\), and \(u_i \equiv 0\).

A common assumption in random effects meta-regression is that the random effects \(u_i\) are independent and normally distributed with mean zero and variance \(\tau^2\):\textsuperscript{20--23}
\[
u_i \sim N(0, \tau^2).
\]
This could correspond to a scenario of \(k\) independent effect estimates presumably from \(k\) different studies.
In that case, the distribution \(p(T | X, v, \eta)\) can be written as
\begin{equation}
p(T_i | X_i, v_i, \eta) = \frac{1}{\sqrt{2\pi(\tau^2 + v_i)}} e^{-\frac{(T_i - X_i\beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full-data-prob}
\end{equation}
Thus, the joint likelihood for all \(k\) effects can be wrtiten as:
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = (2\pi)^{-k/2} \left[\prod_{i = 1}^k (\tau^2 + v_i)\right] e^{-\sum_{i=1}^k \frac{(T_i - X_i \beta)^2}{2(\tau^2 + v_i)}}
\label{eq:full-data-prob-vec}
\end{equation}
where \(\mathbf{T} \in \mathbb{R}^k\) is the vector of effect estimates, \(\mathbf{v} \in \mathbb{R}^k\) is the vector of estimation variances, and \(\mathbf{X} \in \mathbb{R}^{k \times (p+1)}\) is the matrix of covariates where each row of \(\mathbf{X}\) is simply the row vector \(X_i\).
Note that the functions in both \eqref{eq:full-data-prob} and \eqref{eq:full-data-prob-vec} assume that \emph{all} of the \(p\) covariates are observed.
Equation \eqref{eq:full-data-prob-vec} is referred to as the \emph{complete-data likelihood function}.\textsuperscript{13,24}
We note that a meta-regression with no missing data will be accurate if the complete-data model is correctly specified.
Thus, to illustrate the properties of incomplete data meta-regression, we assume that the complete-data model is correctly specified.

The vector of regression coefficient estimates for the complete-data model when there is no missing data is typically estimated by
\begin{equation}
\hat{\beta} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{T}
\label{eq:betahat-cd}
\end{equation}
Here, \(\mathbf{W} = \text{diag}[w_i]\) is the diagonal matrix of weights such that \(w_i = 1/(v_i + \tau^2)\).
The covariance matrix of \(\hat{\beta}\) is given by
\begin{equation}
V[\hat{\beta}] = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} 
\label{eq:v-betahat-cd}
\end{equation}
Note that the weights involve the true variance component \(\tau^2\).
In practice, \(\tau^2\) must be estimated by \(\hat{\tau}^2\), and the resulting weights used in analyses can be written \(\hat{w}_i = 1/(v_i + \hat{\tau}^2)\).
For the sake of simplicity, we use \(w_i\) to derive results in this article, and so results do not depend on variance component estimators.
Presumably, use of \(\hat{w}_i\) would induce additional variation into analyses.

The substance abuse data contains multiple effect estimates per study that are likely correlated.
This differs from the model above.
However, we can expand this model to account for dependent effect sizes by assuming that \(T_i \in \mathbb{R}^{k_i}\) is a vector of \(k_i\) effects from the same study, \(e_i\) is vector of estimation errors, \(u_i\) is a vector of random effects, and \(e_i + u_i\) has covariance matrix \(\Sigma_i\).
In this model, \(X_i\) is a matrix of covariates for each effect in \(T_i\).
The resulting formulas for the complete-data likelihood function and coefficient estimators will be more complex (including a variance-covariance weight matrix), but they will have a similar form as the independent effect size model.

Not all relevant variables may be observed in a meta-analytic dataset.
Let \(R_i\) be a vector of response indicators that correspond with effect \(i\).
This article concerns missing covariates, and we assume that \(T_i\) and \(v_i\) are observed for every effect of interest in a meta-analysis.
Thus, each element \(R_{ij}\) of \(R_i\) corresponds to a covariate \(X_{ij}\).
The \(R_{ij}\) take a value of either 0 or 1: \(R_{ij} = 1\) indicates the corresponding \(X_{ij}\) is observed and \(R_{ij} = 0\), indicates a that the corresponding \(X_{ij}\) is not observed.
Note that \(R_i \in \mathcal{R} \equiv \{0,1\}^p\) is a vector of 0s and 1s of length \(p\).
For instance, \(X_{i2}\) were missing, this would be indicated by \(R_{i2} = 0\).

Denote \(O = \{(i, j): R_{ij} = 1\}\) as the indices of covariates that are observed and \(M = \{(i, j): R_{ij} = 0\}\) be the set of indices for missing covariates.
Then, the complete-data model can be written as
\begin{equation}
p(\mathbf{T} | \mathbf{X}, \mathbf{v}, \eta) = p(\mathbf{T} | \mathbf{X}_{O}, \mathbf{X}_{M}, \mathbf{v}, \eta). 
\label{eq:full-data-prob_mis}
\end{equation}
Note that the complete-data model depends on entries of \(\mathbf{X}_{M}\), which are unobserved.
It is worth pointing out that the \emph{complete-data model}, which refers to the model with no missing data, is distinct from the \emph{complete-case analysis}, which is an estimation procedure that conditions only on observed data.

\hypertarget{complete-case-estimators}{%
\subsection{Complete-Case Estimators}\label{complete-case-estimators}}

A common approach in meta-regression with missing covariates is to use a complete-case analysis.\textsuperscript{10,11}
This approach simply omits rows in the data for which any covariate is missing.
Thus, this analysis method only uses effects and covariates for which \(R_i = [1, \ldots, 1] = \mathbbm{1}\).

Let \(C = \{i : R_i = \mathbbm{1}\}\) index all relevant effects \(i\) such that \(R_i = \mathbbm{1}\), so that \(\mathbf{X}_C\) is the matrix of covariates such \(R_i = \mathbbm{1}\), \(\mathbf{T}_C\) is the corresponding subset of effect estimates, and \(\mathbf{W}_C\) is the corresponding subset of weights.
The CCA estimates the coefficients \(\beta\) with:
\begin{equation}
\hat{\beta}_C
  = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \mathbf{T}_C
\label{eq:beta-c}
\end{equation}

\hypertarget{shifting-case-estimators}{%
\subsection{Shifting-Case Estimators}\label{shifting-case-estimators}}

When there are multiple covariates of interest, each of which has some missingness, there may only be a few effects for which all covariates of interest are observed.
When that happens, a complete-case analysis can be unfeasible.
A common solution to this in meta-analysis is to use an available-case analysis.\textsuperscript{11}
In practice, an \emph{available-case} meta-regression is often equivalent to a shifting-case analysis, referred to in the literature as \emph{shifting units of analysis}.\textsuperscript{10,12}

Shifting-case analyses involve fitting multiple regression models, each including a subset of the covariates of interest.
Sometimes this even takes the form of regressing effect estimates on one covariate at a time.\textsuperscript{10,11}
In the substance abuse data example, we focused on two covariates of interest \(X_{i1}\) and \(X_{i2}\).
The SCA first regressed \(T_i\) on observed values of \(X_{i1}\).
This regression included observations for which both \(X_{i1}\) and \(X_{i2}\) are observed (i.e., \(R_i = [1, 1]\)) and observations for which \(X_{i1}\) is observed but \(X_{i2}\) is missing (i.e., \(R_i = [1, 0])\).
We then regressed \(T_i\) on \(X_{i2}\), which included effects for which \(R_i \in \{[1, 1], [0, 1]\}\).
In sum, the SCA demonstrated in the previous section involved two regressions, each of which conditioned on different sets of missingness patterns.

To formalize SCA estimators, consider a single regression in an SCA, and let \(S\) index the component of \(X_i\) (i.e., the intercept term and relevant covariates) included in that model \(S = \{j : j = 0 \text{ or } X_{ij} \text{ in analysis}\}\).
Let \(E\) be the complement of \(S\) so that \(E\) indexes the covariates excluded from the regression.
Then, the regression is used to estimate and make inferences about coefficients \(\beta_S\).
In the following section, we discuss \(\beta_S\) and its relationship to \(\beta\), but here assume that the target of inference for an SCA is \(\beta\) and hence \(\beta_S\) comprises a subset of the components of \(\beta\).
For instance, in the first substance abuse SCA regression, \(T_i\) was regressed on only \(X_{i1}\), so that \(\beta_S = [\beta_0, \beta_1]\).

Denote \(\mathcal{R}_j\) as the set of missingness patterns such that all included covariates are observed: \(\mathcal{R}_j = \{R \in \mathcal{R}: R_S = \mathbbm{1}\}\).
Note that \(\mathcal{R}_j\) contains missingness patterns such that all the included covariates are observed, but any excluded covariates may be either observed or unobserved.
For instance, in the first substance abuse SCA regression of \(T_i\) on \(X_{i1}\), the analysis included effects such that \(R_i \in \mathcal{R}_1 = \{[1,1], [1,0]\}\).
Finally, let \(U\) denote the indices \(i\) of effects for which \(X_{iS}\) are observed; note that \(U\) depends on \(S\), so we may write \(U(S) = \{i : R_i \in \mathcal{R}_j\}\).
Then, the shifting-case estimators for \(\beta_S\) are given by:
\begin{equation}
\hat{\beta}_S = 
(\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1} \mathbf{X}_{US}^T \mathbf{W}_U \mathbf{T}_U
\label{eq:beta-s}
\end{equation}
where \(\mathbf{X}_{US}\) contains the columns (\(S\)) of \(\mathbf{X}\) that pertain to the covariates that are included in the SCA regression, and the rows (\(U\)) for which all of those covariates are observed.
The matrix \(\mathbf{W}_U\) is a square matrix containing the relevant rows and columns of \(\mathbf{W}\) for which \(X_{iS}\) are observed, while \(\mathbf{T}_U\) contains the effect sizes in \(\mathbf{T}\) for which \(X_{iS}\) is observed.

\hypertarget{omitted-variables}{%
\subsection{Omitted Variables}\label{omitted-variables}}

A common concern in meta-regression is that models may not be able to account for all relevant covariates, either due to sample size constraints or because some covariates were not observed.\textsuperscript{25}
Such concerns pertain to meta-regressions both with and without missing data.
In contrast to primary data analysis, covariates or moderators in meta-regression are frequently ad-hoc or difficult to measure consistently.
Even with key conceptually or theoretically important moderators, meta-regression models must often contend with variation in reporting of such moderators across studies.

The implication of omitting observed variables in SCA can be understood via the parameter \(\beta_S\).
It has been noted that there are various conditions under which components of \(\beta_S\) are unequal to their counterparts in \(\beta\).\textsuperscript{26}
For instance, it can be the case that \(\beta_S = [\tilde{\beta}_0, \tilde{\beta}_1] \neq [\beta_0, \beta_1] \subset \beta\).
The difference between \(\beta_S\) and components of \(\beta\) is often referred to as \emph{omitted variable bias} in the statistical and econometric literature.\textsuperscript{27,28}
This conception inherently assumes that \(\beta\) in the full model is of interest to the analyst, which may not necessarily be the case.
Indeed, one may assume that \(\beta_S\) is of interest, rather than \(\beta\), so that the components of \(\beta_S\) comprise parameters distinct from \(\beta\).
In this approach, \(\beta_S\) characterizes the relationship between \(X_{iS}\) and \(T_i\) in a more restricted model that does not account for \(X_{iE}\).
This would be consistent with analyses that focus on specific subgroups of studies.

We refer to the difference between \(\beta_S\) and \(\beta\) as omitted variable bias, in keeping with the literature on linear models.
In doing so, we treat SCA as a missing data analytic strategy, wherein the target of inference is \(\beta\).
Subsequent sections present findings on bias induced by omitting observed covariates in an SCA, which reflect the findings of Lipsey (2003), who points out that interpretation of meta-regression coefficients when covariates are omitted can lead to misleading interpretations about the correlates of effective interventions.
However, if the intent of the analysis is to examine restricted models or specific subgroups of effects/studies, the omitted variable bias presented in this article may be less applicable, though Lipsey's caveats for interpreting such models may still apply.

\hypertarget{missingness-mechanisms}{%
\subsection{Missingness Mechanisms}\label{missingness-mechanisms}}

Both the complete- and shifting-case estimators are analyses of incomplete data.
Analyses of incomplete data require some assumption about why data are missing, which is referred to as the missingness \emph{mechanism}.
The mechanism by which missingness arises is typically modeled through the distribution of \(R\).
Let \(\psi\) denote the parameter (or vector of parameters) that index the distribution of \(R\) so that the probability mass function of \(R\) can be written as \(p(R | T, X, v, \psi)\).
Assumptions about the missingness mechanism are therefore equivalent to assumptions about \(p(R | T, X, v, \psi)\).

Rubin\textsuperscript{29} defined three types of mechanisms in terms of the distribution of \(R\).
Data could be missing completely at random (MCAR), which means that the probability that a given value is missing is independent of all of the observed or unobserved data:
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) = p(\mathbf{R} | \psi)
\]
MCAR implies that probability that a given value is missing depends only on the missingness parameter \(\psi\).

Covariates could be missing at random (MAR), which implies the distribution of missingness depends only on observed data and the missingness parameter:
\[
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O, \mathbf{X}_M, \mathbf{v}, \psi) = 
  p(\mathbf{R} | \mathbf{T}, \mathbf{X}_O,\mathbf{v}, \psi)
\]
MAR differs from MCAR in that missingness might be related to observed values.
As an example, if studies with larger standard errors are less likely to report the racial composition of their samples, then missingness would depend on the (observed) estimation error variances.
Data missing according to this mechanism would violate an assumption of MCAR, since missingness is related to an observed value.

Finally, data are said to be missing not at random (MNAR) if the distribution of \(R\) depends on unobserved data in some way.
In the context of the meta-regression data, this would imply that \(R\) is related to \(\mathbf{X}_M\), so that the probability of a covariate not being observed depends on the value of the covariate itself.
For instance, data would be MNAR if studies with larger standard errors and a greater proportion of minorities are less likely to report the racial composition of their samples because the likelihood that racial composition is not reported will depend on the composition itsef.

A related concept in missing data is that of \emph{ignorability}, which means that the missingness pattern does not contribute any additional information.
When missing data are ignorable, it is not necessary to know (or estimate) \(\psi\) in order to conduct inference on \(\eta\).\textsuperscript{13,14,24,30}
In practice, missing data are ignorable if they are MAR and if \(\psi\) and \(\eta\) are distinct.

\hypertarget{conditional-incomplete-data-meta-regression}{%
\section{Conditional Incomplete Data Meta-Regression}\label{conditional-incomplete-data-meta-regression}}

Because both complete- and available-case analyses depend on the value of \(R_i\), they can be seen as models that condition on missingness.
Models that condition on missingness are not necessarily identical to the complete-data model, which is the model of interest, because the complete-data model does not condition on \(R_i\).
Yet, CCA and SCA proceed as if the complete-data and conditional models on missingness are equivalent.
Doing so ignores the missingness mechanism and its potential impact on the accuracy of analytic results.

The complete-data model can be related to the conditional models through the distribution of missingness \(R_i\).
This approach is referred to as a \emph{selection model} in the missing data literature.\textsuperscript{13,24,30}
We can write the selection model for meta-regression with missing covariates as:
\begin{equation}
p(T_i | X_i, v_i, R_i \in \mathcal{R}_j, \eta, \psi) 
  = \frac{p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta)}{p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta)} 
\label{eq:selection-model}
\end{equation}
where \(\psi\) indexes the distribution of \(R | T, X, v\).
Here, \(\mathcal{R}_j\) refers to the relevant subset of \(\mathcal{R}\) on which the analysis conditions; for a CCA, \(\mathcal{R}_j = \{\mathbbm{1}\}\).

Equation \eqref{eq:selection-model} describes the conditional model as a function of the complete-data model \(p(T_i | X_i, v_i, \eta)\) and a selection model \(p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)\) that gives the probability that a given set of covariates are observed.
The denominator on the right hand side of \eqref{eq:selection-model} is a normalizing factor that is equivalent to the probability of observing the missingness pattern \(\mathcal{R}_j\) given the estimation error variance \(v_i\) and the observed and unobserved covariates in the vector \(X_i\), and can be written as
\begin{equation}
p(R_i \in \mathcal{R}_j | X_i, v_i, \psi, \eta) = \int p(R_i \in \mathcal{R}_j | T_i, X_i, v_i, \psi)p(T_i | X_i, v_i, \eta) dT_i
\label{eq:pr-xv}
\end{equation}

Note that when the complete-data model in \eqref{eq:full-data-prob} is not equivalent to the conditional model in \eqref{eq:selection-model}, the resulting coefficient estimators in a meta-regression can be biased.
To see this, we can write:
\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] 
  = E[T_i | X_i, v_i] + \delta_{ij} 
  = X_i \beta + \delta_{ij}
\label{eq:bias-delta}
\end{equation}
Here, we see that the expectation of \(T_i\) given \(X_i\) and \(R_i\) can be written as the complete-data expectation \(X_i \beta\) (i.e., the regression model) plus a bias term \(\delta_{ij}\).
The bias term \(\delta_{ij}\) refers to the bias induced in the regression model due to conditioning on missingness pattern \(\mathcal{R}_j\), which can affect individual components of \(\eta\).
If \(\delta_{ij} \neq 0\), it follows that conditioning on \(R_i\) induces bias in the distribution of \(T_i\) used in an analysis.
Because the CCA estimator \eqref{eq:beta-c} and SCA estimator in \eqref{eq:beta-s} are weighted averages of the \(T_i\), they can be biased if \(\delta_{ij} \neq 0\).
The precise magnitude of the \(\delta_{ij}\) will depend on the selection model in \eqref{eq:selection-model} and hence on the missingness mechanism.
It is worth noting that the subsequent sections show that bias depends on the precise selection model rather than the class of mechanism (MCAR or MAR).

A standard approach for modeling missingness mechanisms for covariates is to assume \(R_i\) follows some log-linear distribution.\textsuperscript{31}
Various authors have described approaches to modelling \(R\) for missing covariates in generalized linear models that include logistic and multinomial logistic models.\textsuperscript{32--34}
Thus, one class of models for missingness would involve the logit probability of observing some missingness patterns \(R_i \in \mathcal{R}_j \subset \mathcal{R}\):
\begin{equation}
\text{logit}[p(R_i \in \mathcal{R}_j | T_i, X_i, v_i)] = \sum_{m = 0}^{m_j} \psi_{mj} f_{mj}(T_i, X_i, v_i) 
\label{eq:r-loglinear}
\end{equation}
Here, \(f_{mj}(T_i, X_i, v_i)\) are assumed to be differentiable basis functions of the data and \(m_j\) is the number of terms in the selection model.
In theory, \(m_j\) could be arbitrarily large, but the model is only estimable if \(m_j < k\).
Finally, we assume \(f_{0j}(T_i, X_i, v_i) = 1\), so that \(\psi_{0j}\) would be the intercept term for the logit model for the set of missingness patterns \(\mathcal{R}_j\).

In general, it is impossible to know whether a selection model is correctly specified, but the formulation in \eqref{eq:r-loglinear} offers a few important advantages.
First, it is fairly general: the only assumption made of the basis functions \(f_{mj}\) is that they are differentiable, which means model \eqref{eq:r-loglinear} allows for nonlinear or interaction terms.
Second, it expresses the relationships between the probability of the event \(\{R_i \in \mathcal{R}_j\}\) and observed variables on the scale of the log odds ratio, a well-understood scale in meta-analysis.
Third, it allows for closed-form expressions for the approximate bias of coefficient estimates by virtue of the the logit link function.
Thus, it comprises a large class of models for selection that can be more clearly interpreted.

\hypertarget{approximate-bias-for-log-linear-selection-models}{%
\subsection{Approximate Bias for Log-linear Selection Models}\label{approximate-bias-for-log-linear-selection-models}}

As argued above, the bias of complete-case estimators \(\hat{\beta}_C\) or shifting-case estimators \(\hat{\beta}_S\) will depend in some way on the bias \(\delta_{ij}\) induced in \(T_i\) by conditioning on \(R_i \in \mathcal{R}_j\).
The magnitude and direction of \(\delta_{ij}\) will in turn depend on the selection model.

It is possible to derive an approximation for \(\delta_{ij}\) under certain conditions.
If \(p(T_i | X_i, v_i)\) is the standard fixed- or random effects meta-regression model in equation \eqref{eq:full-data-prob}, and \(p(R_i \in \mathcal{R}_j | T_i, X_i, v_i)\) follows the log-linear model in \eqref{eq:r-loglinear}, and the \(f_{mj}\) are differentiable with respect to \(T_i\), then
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional-bias}
\end{equation}
where \(H_j(X_i \beta, X_i, v_i)\) is equivalent to \(p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i)\) evaluated at \(T_i = X_i\beta\) and
\[
f_{mj}'(X_i\beta, X_i, v_i) = \left.\frac{\partial f_{mj}}{\partial T_i}\right\rvert_{T_i = X_i\beta}
\]
is the derivative of \(f_{mj}\) with respect to \(T_i\) evaluated at \(T_i = X_i\beta\).
A more detailed proof is presented in Appendix A.

While the following sections will examine possible values that \(\delta_{ij}\) may take under different selection models, we can gain some insight on bias by examining \eqref{eq:conditional-bias}.
The expression for \(\delta_{ij}\) depends on three main quantities.
First, \(\delta_{ij}\) is an increasing of \(H_j(X_i\beta, X_i, v_i)\), which is the probability that \(R_i \not\in \mathcal{R}_j\).
This implies that the bias will be greater as the probability of omitting an observation increases.
Second, \(\delta_{ij}\) increases in the sum of variance components \(\tau^2 + v_i\), which means that the bias will be larger when \(T_i\) vary more around the regression line.
Finally, \(\delta_{ij}\) depends on \(\psi_{mj} f_{mj}'(X_i\beta, X_i, v_i)\).
Since \(f_{mj}'\) is the derivative of \(f_{mj}\) with respect to \(T\), when \(f_{mj}\) does not depend on \(T\), then \(f_{mj}' = 0\), and hence \(\psi_{mj} f_{mj}' = 0\).
Thus, \(\delta_{ij}\) depends on the components of the selection model that are functions of \(T_i\) and how strongly those components are related to the probability of observing \(X_i\) via the parameter \(\psi\).

\hypertarget{bias-in-complete-case-analyses}{%
\section{Bias in Complete-Case Analyses}\label{bias-in-complete-case-analyses}}

Complete-case analyses only include effects for which all relevant covariates are observed.
The complete-case coefficient estimator \(\hat{\beta}_C\) given in equation \eqref{eq:beta-c} conditions on \(R_i = \mathbbm{1}\).
As noted above, conditioning on \(R_i\) can induce bias, however there are conditions under which the CCA will lead to unbiased coefficient estimates.
These conditions largely amount to whether or not \(R_i\) is independent of the effect size estimate \(T_i\), the outcome of meta-regression model.
When the distribution of \(R_i\) depends on \(T_i\), then complete-case estimators will be biased.

The general condition under which CCA estimators are unbiased is that \(R_i \perp T_i\), which occurs for different types of selection models.
First, if the covariates are MCAR, then \(R_i \perp (T_i, X_i, v_i)\).
Alternatively, if the selection model depends only on \(v_i\), but not \(X_i\) or \(T_i\), then \(R_i \perp (T_i, X_i) | v_i\); this would constitute a MAR mechanism.
Finally, if the selection model depends only on \(v_i\) and \(X_i\), but not \(T_i\), then \(R_i \perp T_i | (X_i, v_i)\), which would correspond to an MNAR mechanism.
Under each of these assumptions, it can be shown that the model that conditions on complete cases \(R_i = \mathbbm{1}\) is identical to the complete-data model, and hence CCA estimators will be unbiased:
\begin{equation}
p(T_i | X_i, v_i, R_i = \mathbbm{1}, \eta, \psi) 
  = \frac{p(R_i = \mathbbm{1} | T_i, X_i, v_i, \psi) p(T_i | X_i, v_i, \eta)}{p(R_i = \mathbbm{1} | X_i, v_i, \eta, \psi)}
  = p(T_i | X_i, v_i, \eta)
\label{eq:cc-unbiased}
\end{equation}
This result is consistent with prior work regarding linear regression models with missing covariates.\textsuperscript{35,36}

An important aspect of this result is that whether or not a CCA produces unbiased coefficient estimates depends more on the role of \(T_i\) in the selection model rather than traditional mechanism classifications of MCAR, MAR, or MNAR.
Though various selection models satisfy the conditions of MAR, and similarly with MNAR, the key factor for bias in CCA estimators is the relationship between \(R_i\) and \(T_i\).
Should \(R_i \not\perp T_i\), then CCA estimators can be biased, regardless of whether the mechanism is MAR or MNAR.
Similarly, if \(R_i \perp T_i\), CCA estimators can be unbaised, regardless of MAR or MNAR.

When \(R_i\) is not independent of \(T_i\) (given \(X_i\) or \(v_i\)), then CCA can be biased.
Let \(\mathcal{R}_1 = \{\mathbbm{1}\}\) so that the CCA conditions on \(R_i \in \mathcal{R}_1\).
Based on equation \eqref{eq:bias-delta}, the bias of \(\hat{\beta}_C\) will depend on the \(\delta_{i1}\).
If we let \(\Delta = [\delta_{11}, \ldots, \delta_{k1}]\) be the vector of \(\delta_{i1}\) and let \(\Delta_C\) be the subset of \(\Delta\) for which all covariates are observed (i.e., \(R_i = \mathbbm{1}\)).
Then the bias of the complete-case analysis can be written as
\begin{equation}
\text{Bias}[\hat{\beta}_C] = (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \Delta_C
\label{eq:cc-bias-delta}
\end{equation}
The bias in equation \eqref{eq:cc-bias-delta} is a weighted average of individual biases \(\delta_{i1}\).
Hence, the bias will be larger if the \(\delta_{i1}\) are larger (and in the same direction).

Precisely, how large the bias in \eqref{eq:cc-bias-delta} is will depend on the distribution of \(R_i\) and its relationship to effect estimates \(T_i\) and their covariates \(X_i\).
When \(R_i\) follows the log-linear model in \eqref{eq:r-loglinear}, the approximate bias can be written as
\begin{equation}
\text{Bias}[\hat{\beta}_C]
  \approx (\mathbf{X}_C^T \mathbf{W}_C \mathbf{X}_C)^{-1} \mathbf{X}_C^T \mathbf{W}_C \mathbf{H}_{1C}  \mathbf{f}_{1C} \psi_1
\label{eq:cc-bias-loglinear}
\end{equation}
where
\[
\mathbf{H}_1 = \text{diag}[H_1(X_i\beta, X_i, v_i)]
\]
is a \(k \times k\) diagonal matrix where entries refer to the probability that an observation is \emph{not} a complete case,
\[
\mathbf{f}_1 = \left[f_{01}'(X_i^T\beta, X_i, v_i), \ldots, f_{m_1 1}'(X_i^T\beta, X_i, v_i) \right]
\]
is a \(k \times m_1\) matrix of derivatives, and \(\psi_1 = [\psi_{01}, \ldots, \psi_{m_1 1}]^T\) is a vector of parameters that index the selection model.
Note that the bias in \eqref{eq:cc-bias-loglinear} involves \(\mathbf{H}_{1C}\) which contains the rows of \(\mathbf{H}_1\) for which \(R_i = \mathbbm{1}\); similarly for \(\mathbf{f}_{1C}\).

While \eqref{eq:cc-bias-loglinear} provides a general expression for the approximate bias of \(\hat{\beta}_C\), it can be a little difficult to interpret.
Loosely, we can see that the bias depends on the probability that covariates are missing, reflected in \(\mathbf{H}_{1C}\), as well as some function of the components of the log-linear selection model \(\mathbf{f}_{1C} \psi_1\).
To better intuit this bias, we provide a simple example in the following section.

\hypertarget{example-complete-case-analysis-with-a-single-binary-covariate}{%
\subsection{Example: Complete-Case Analysis with a Single Binary Covariate}\label{example-complete-case-analysis-with-a-single-binary-covariate}}

Suppose the model of interest includes a single binary covariate \(X_{i1} \equiv X_i \in \{0, 1\}\), so that the complete data model is
\begin{equation}
T_i = \beta_0 + \beta_1 X_i + u_i + e_i
\label{eq:cc-example}
\end{equation}
where \(\beta_0\) and \(\beta_1\) are the regression coefficients of interest.
Note that \(\beta_0\) is the average effect when \(X_i = 0\) and \(\beta_1\) is the contrast in mean effects for when \(X_i = 1\) versus when \(X_i = 0\).

Because \(X_i\) is a scalar, so is \(R_i\); \(R_i = 0\) indicates that \(X_i\) is missing, \(R_i = 1\) indicates that \(X_i\) is observed.
A CCA would include only effects \(i\) for which \(X_i\) is observed (i.e., \(R_i = 1\)).
The complete-case estimator for \(\beta_0\) is given by a weighted sum of \(T_i\) among the effects for which \(X_i = 0\) and \(R_i = 1\):
\begin{equation}
\hat{\beta}_{0C} = \frac{\sum_{i: X_i = 0, R_i = 1} w_i T_i}{\sum_{i: X_i = 0, R_i = 1} w_i}
\label{eq:b0c-ex}
\end{equation}
The complete-case estimator for \(\beta_1\) is given by the difference between the (weighted) mean effect for \(X_i = 1\) versus \(X_i = 0\):
\begin{equation}
\hat{\beta}_{1C} = \frac{\sum_{i: X_i = 1, R_i = 1} w_i T_i}{\sum_{i: X_i = 1, R_i = 1} w_i} - \hat{\beta}_{0C}
\label{eq:b1c-ex}
\end{equation}

Assume that the selection model is log-linear, and that for the sake of simplicity the probability of observing \(X_i\) depends on the size of the effect \(T_i\) and the value of \(X_i\):
\begin{equation}
\text{logit}[p(R_i = 1 | T_i, X_i, v_i)] 
  = \psi_0  + \psi_1 T_i + \psi_2 X_i
\label{eq:cc-loglinear}
\end{equation}
Note that this is an MNAR mechanism, since the probability \(X_i\) is observed depends on \(X_i\) itself; a MAR mechanism would involve \(\psi_2 = 0\) in equation \eqref{eq:cc-loglinear}.
Because \eqref{eq:cc-loglinear} depends on \(T_i\), \(\delta_{ij} \neq 0\) for this selection model regardless of MAR or MNAR (i.e., regardless of whether \(\psi_2 = 0\) or not), the CCA estimators may be biased.

Under this model, \(H_{1}(X_i\beta, X_i, v_i)\) depends only on \(X_i\) and not \(v_i\), so we can write \(H_1(X_i) = p(R \neq \mathbbm{1} | T_i, X_i, v_i)\rvert_{T_i = X_i^T \beta}\).
As well, \(f_{11}(T_i, X_i, v_i) = T_i\) and \(f_{21}(T_i, X_i, v_i) = X_i\).
Given the result in equation \eqref{eq:conditional-bias}, we can write
\begin{equation}
\delta_{i1}
   \approx H_1(X_i)(v_i + \tau^2)\psi_1
\label{eq:cc-ex-delta}
\end{equation}

Given the selection model in \eqref{eq:cc-loglinear}, the bias of the complete-case estimator for the intercept, \(\hat{\beta}_{0C}\), is:
\begin{equation}
\text{Bias}[\hat{\beta}_{0C}] 
  \approx H_1(0)(\bar{v}_0 + \tau^2)\psi_1
\label{eq:cc-bias-b0}
\end{equation}
where \(\bar{v}_0\) is the average estimation error variance \(v_i\) among effects for which \(X_i = 0\) and \(R_i = 1\).
The expression in \eqref{eq:cc-bias-b0} depends on three key quantities, and is an increasing function of each of those quantities.
First, the bias increases in \(H_1(0)\), which is an approximation of the probability that \(X_i\) is missing among studies for which \(X_i = 0\).
While under model \eqref{eq:cc-loglinear}, this probability is a function of \(T_i\) and \(X_i\), we can intuit \(H_1(0)\) loosely as a missingness rate in \(X_i\) among effects for which \(X_i = 0\).
Second, the bias in \eqref{eq:cc-bias-b0} is increasing in \(\bar{v}_0 + \tau^2\), the average variation of \(T_i\) for which \(X_i = 0\); the greater the variation, the greater the bias.
Because the \(v_i\) are typically decreasing in sample size, if studies have smaller samples, the bias will be greater.
Finally, the bias depends on \(\psi_1\), which characterizes the relationship between an \(X_i\) being observed (i.e., \(R_i\)) and \(T_i\).
When \(\psi_1\) is positive, larger effect estimates \(T_i\) are more likely to have observed \(X_i\) and the bias will be positive; if \(\psi_1\) is negative, so that larger effect sizes are more likely to be missing the covariate \(X_i\), then the bias will be negative.

To gain better insight into equation \eqref{eq:cc-bias-b0}, suppose \(v_i \approx v = \bar{v}_0\) so that each study has roughly the same estimation error variance.
If we assume \(T_i\) is on the scale of a standardized mean difference, \(v_i \approx 4/n_i\) where \(n_i\) is the total sample size used to compute \(T_i\).
Various researchers have described conventions for the magnitude of \(\tau^2\) that range from \(\tau^2 = v/4\) to \(\tau^2 = v\).\textsuperscript{37--39}
Thus, we can write \(\tau^2 + v = 4(1 + r)/n\) from some constant \(r\) that ranges from 0 to 1.

Further, the parameter \(\psi_1\) is a log-odds ratio, which reflects the odds of a complete case for \(T_i\) versus \(T_i - 1\).
There are various conventions for the size of an odds ratio that depend on base rates \(P[R = \mathbbm{1} | T]\) that could be interpreted as ranging from 1.5 to as large as 9.0, though various researchers have noted that odds ratios greater than 3.0 or 4.0 could be considered large.\textsuperscript{40--43}
Thus, we consider a range of odds ratios from about 1.5 to 4.5.
However, the actual size of \(\psi_1\) will depend on the scale of a change in effect size \(D_T = |T_i - \tilde{T}_i|\).
Since it corresponds to a difference, \(D_T\) should be no larger than an individual \(|T_i|\).
Based on conventions in the social and medical sciences (some arbitrary, some empirical), meaningful values of \(D_T\) might feasibly range from 0.2 to 1.0.\textsuperscript{40,44}
These conventions for odds ratios and \(D_T\) would imply that relevant values of \(|\psi_1|\) might range from 0.4 (large \(D_T\) with small odds ratio) to over 7.5 (small \(D_T\) with large odds ratio).

Based on these conventions, Figure \ref{fig:delta} shows the potential (approximate) bias of \(\hat{\beta}_{0C}\) for this example.
Each panel corresponds to a given within-study variance \(v = 4/n\) and residual heterogeneity \(\tau^2\).
Panels plot the bias contributed by a single case \(\delta_i\) as a function of the probability of missingness \(H_1(0)\) (\(x\)-axis) and \(\psi_1\) (color).
The panels on the bottom few rows and left most columns show that if both \(\psi_1\) is small and \(\tau^2 + v\) is small, then \(\delta_i\) will be less than 0.05.
However if \(\tau^2 + v_i\) is larger and the probability of a complete case is strongly related to \(T_i\) (i.e., \(\psi_1\) is large), then the bias can be greater than \(d\) = 0.2 or even 0.5.

It is worth noting that Figure \ref{fig:delta} gives the bias for when \(T_i\) is positively correlated with \(R_i\), and hence \(\psi_1 > 0\).
When \(\psi_1 < 0\), then the bias of \(\hat{\beta}_{0C}\) is negative, and would be be a mirror image of those in Figure \ref{fig:delta}.
Larger, more negative values of \(\psi_1\) would lead to a greater downward bias.

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/delta_plot_cts}
\end{center}
\caption{This figure plots the bias of the intercept estimate $\hat{\beta}_{0C}$ ($y$-axis) of the example. Bias is shown as a function of the average sampling variance $v$, residual heterogeneity $\tau^2$, the probability of missingness when $X_1 = 0$, $H_1(0)$ ($x$-axis), and the correlation between missingness and the effect size as measured by $\psi_1$ (color). Note that $\psi_1$ is a log-odds ratio for effect sizes on the scale of Cohen's $d$.}
\label{fig:delta}
\end{figure}

The bias of the slope coefficient, \(\hat{\beta}_{1C}\), under selection model \eqref{eq:cc-loglinear} is given by:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}]
  \approx \left[H_1(1)(\bar{v}_1 + \tau^2) - H_1(0)(\bar{v}_0 + \tau^2)\right]\psi_1 
\label{eq:cc-bias-beta1-example}
\end{equation}
where \(\bar{v}_1\) is the mean \(v_i\) among effects for which \(X_i = 1\) and \(R_i = 1\).
As with \(\hat{\beta}_{0C}\), the bias of \(\hat{\beta}_{1C}\) is an increasing function of \(\psi_1\).
If \(T_i\) has a strong positive correlation with \(R_i\), then \(\psi_1\) will be larger and so will the bias of \(\hat{\beta}_{1C}\).

When all studies have approximately the same estimation error variance so that \(v_i \approx v\) and \(\bar{v}_0 \approx \bar{v}_1\), then the bias of \(\hat{\beta}_{1C}\) is approximately:
\begin{equation}
\text{Bias}[\hat{\beta}_{1C}] 
  \approx \left[H_1(1) - H_1(0)\right] (v + \tau^2) \psi_1 
\label{eq:cc-bias-b1-simp}
\end{equation}
The expression in \eqref{eq:cc-bias-b1-simp} is similar to \eqref{eq:cc-bias-b0}, and both expressions depend on similar quantities.
Like \(\hat{\beta}_{0C}\), the bias of \(\hat{\beta}_{1C}\) is an increasing function of \(\tau^2 + v\) and \(\psi_1\).
The bias of \(\hat{\beta}_{1C}\) also increases as a function of \(H_1(1) - H_1(0)\), which can be thought of as a difference in missingness rates between cases where \(X_i = 1\) and \(X_i = 0\).
Note, however, that this does not imply that MAR data necessarily leads to an unbiased slope estimate.
Recall that \(H_1\) is an approximation of the probability \(X_i\) is missing given \(X_i\) and \(T_i\) in \eqref{eq:cc-loglinear}: \(P[R_i \neq \mathbbm{1} | T_i, X_i]\).
Even if \(X_i\) were MAR (i.e., assuming \(\psi_1 \neq 0\) but \(\psi_2 = 0\)), the slope estimate would be unbiased only if the slope was zero: \(\beta_1 = 0\).
This is because when \(\beta_1 \neq 0\), we would expect different rates of missingness among studies for which \(X_i = 1\) than \(X_i = 0\) because of the relationship between \(R_i\) and \(T_i\), as well as the relationship between \(T_i\) and \(X_i\).
Viewed this way, the bias of \(\hat{\beta}_{1C}\) will be greatest when there are fewer complete cases, missingness is strongly related the value of the covariate \(X_i\) or to the size of effects (assuming that effects are correlated with \(X_i\)).

To gain insight into the magnitude of bias in \eqref{eq:cc-bias-b1-simp}, consider the values of \(\psi_1 \in [0.4, 7.5]\) and \(\tau^2 + v = 4(1 + r)/n\) discussed above.
Note that the difference \(H_1(1) - H_1(0) = p(R = 0 | X = 1, \eta) - p(R = 0 | X = 0, \eta)\) is a difference in conditional probabilities.
For reference, because both \(R_i\) and \(X_i\) are binary, then \(p(R = 0 | X = 1) - p(R = 0 | X = 0)\) would be equal to the correlation between \(R_i\) and \(X_i\) (assuming equal marginals in a \(2 \times 2\) table).
Thus, \(|p(R = 0 | X = 1) - p(R = 0 | X = 0)|\) could be as small as 0, but could possibly be as large as 1; arbitrary conventions on the size of correlations suggest that \(|p(R = 0 | X = 1) - p(R = 0 | X = 0)| = 0.5\) would be a ``large'' value.\textsuperscript{40}

Figure \ref{fig:b1} shows the potential bias of \(\hat{\beta}_{1C}\) for this example assuming the values of \(\tau^2 + v\), \(\psi_1\), and \(H_1(1) - H_1(0)\) discussed above.
Each panel corresponds to a given amount of heterogeneity \(\tau^2 + v\), and within panels the bias is shown as a function of the difference \(H_1(1) - H_1(0)\) (\(x\)-axis) and \(\psi_1\) (color).
Figure \ref{fig:b1} highlights that the relationship between \(R_i\) and \(T_i\) (\(\psi_1\)) and between \(R_i\) and \(X_i\) (\(x\)-axes) can affect the magnitude of the bias.
If \(R_i\) is strongly correlated with both \(X_i\) and \(T_i\) the bias can be as large as \(d\) = 0.3 or 0.4.
However, the less \(R_i\) depends on \(T_i\) or \(X_i\), the lower the bias is.

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/bias_beta1_ex1}
\end{center}
\caption{This figure plots the bias of $\hat{\beta}_{1C}$ ($y$-axis). Each panel corresponds to a given value of residual heterogeneity $\tau^2$ and estimation error variance $v$. Within panels, the bias of $\hat{\beta}_{1C}$ is plotted as function of differential missingness rates ($p(R = 0 | X = 1) - p(R = 0 | X = 0)$), which is analogous to the correlation between the value of $X$ and whether it is observed. Bias is also shown as a function of $\psi_1$ which is the relationship between the probability of observing $X$ and the effect size $T$. Bias is shown on the scale of Cohen's $d$ and $\psi_1$ is on the scale of a log-odds ratio.}
\label{fig:b1}
\end{figure}

Recall that the mechanism in these computations is assumed to be MNAR, since \(\psi_2\) in \eqref{eq:cc-loglinear} is nonzero.
A MAR mechanism would require \(\psi_2 = 0\).
In that case, the bias for the CCA intercept estimator \(\hat{\beta}_{0C}\) is identical to that given in \eqref{eq:cc-bias-b0}.
However, the bias in the slope will be slightly different when \(\psi_2 = 0\).
This is because, as noted noted in \eqref{eq:cc-bias-b1-simp}, the bias in the slope depends (loosely) on the correlation between \(R\) and \(X\).
Given the form of \(H_1(X)\) in this example, it is possible for the bias of \(\hat{\beta}_{1C}\) to be greater when \(\psi_2 \neq 0\) (MNAR) than when \(\psi_2 = 0\) (MAR), which can occur if the correlation between \(R\) and \(T\), and \(R\) and \(X\) are in the same direction (i.e., \(\psi_1\) and \(\psi_2\) are in the same direction).
However, when \(\psi_2 \neq 0\) (MNAR), the bias of \(\hat{\beta}_{1C}\) can also decrease in magnitude relative to when \(\psi_2 = 0\) if \(\psi_1\) and \(\psi_2\) are in the opposite directions.

A key implication of this example is that under the relatively simple selection model in \eqref{eq:cc-loglinear}, CCA intercept estimators can have substantial bias.
This bias does not change even if \(\psi_2 = 0\) and the data are MAR.
Thus, inferences for the group of studies for which \(X_i = 0\) will be biased.
Moreover, because inference for the group of studies for which \(X_i = 1\) will depend on the intercept estimate, those inferences will also be biased even if the slope estimator \(\hat{\beta}_{1C}\) is unbiased.

\hypertarget{bias-in-shifting-case-analyses}{%
\section{Bias in Shifting-Case Analyses}\label{bias-in-shifting-case-analyses}}

Shifting-case analyses (SCA) are a common approach in meta-regression when there are very few complete cases across multiple covariates.
These analyses involve fitting multiple regression models, where each model omits some of the covariates of interest.
In this sense, SCA can be thought of as a set of regression models.
Consider one model from that set, which estimates regression coefficients for some subset \(S\) of the relevant covariates using the estimator \(\hat{\beta}_S\) in equation \eqref{eq:beta-s}.
Recall that \(E\) refers to the set of covariates omitted from the model, and that the estimator \(\hat{\beta}_S\) conditions on a set of missingness patterns \(R_i \in \mathcal{R}_j\).
The set of missingness patterns \(\mathcal{R}_j\) is such that \(R_{iS} = 1\) so that all included covariates are observed.

To understand the conditions under which \(\hat{\beta}_S\) is unbiased, we can write a shifting-case model as:
\begin{equation}
p(T_{i} | X_{iS}, v_i, R_i \in \mathcal{R}_j, \eta, \psi) = \frac{p(R_i \in \mathcal{R}_j | T_i, X_{iS}, v_i, \psi) p(T_i | X_{iS}, v_i, \eta)}{p(R_i \in \mathcal{R}_j | X_{iS}, v_i, \eta, \psi)}
\label{eq:sca-model}
\end{equation}
The model in \eqref{eq:sca-model} is slightly different from the models in the previous sections in that all of the functions depend on the covariates included in a given regression \(X_{iS}\) rather than the complete set of relevant covariates \(X_i\).
Thus, the function \(p(T_i | X_{iS}, v_i)\) can be thought of as a partial-data model, since it omits some of the relevant covariates.
The partial-data model \(p(T_i | X_{iS}, v_i)\) need not be equivalent to the complete-data model \(p(T_i | X_i, v_i)\) because the former conditions only on \(X_{iS}\) and not the full set of covariates \(X_i\).
These models would only be equivalent if \(T_i \perp X_{iE} | X_{iS}, v_i\).
That is, unless the excluded covariates are completely unrelated to effect size (given the covariates included in the SCA model), then \(\hat{\beta}_S\) will be biased even if \(X_{iS}\) are completely observed.

The model in \eqref{eq:sca-model} suggests a very strict set of conditions for which \(\hat{\beta}_S\) is unbiased which concern the missingness mechanism and the relevance of excluded covariates in a given shifting-case regression.
First, missingness must be independent of effect sizes.
This arises if \(R_i \perp T_i | X_{iS}, v_i\) or \(R_i \perp (T_i, X_{iS}) | v_i\), which is a similar assumption as that made for unbiased CCA.
In effect, this assumption implies that missingness is independent of effect sizes \(T_i\) (and potentially covariates), but could be correlated with estimation error variances \(v_i\).

Second, any excluded covariates must be completely irrelevant to effect sizes given the included covariates: \(T_i \perp X_{iE} | X_{iS}, v_i\).
This assumption is equivalent to assuming that \(\beta_j = 0\) for all \(j \in E\), so that any omitted variables in a given shifting-case regression are assumed to have a coefficient of zero.
A related assumption is that \((T_i, X_{iS}) \perp X_{iE} | v_i\), which would imply that the complete-data likelihood involves no interactions between \(X_{iS}\) and \(X_{iE}\) and that \(X_{iS}\) and \(X_{iE}\) are orthogonal.
Given the nature of many meta-analyses wherein included studies and effects are ostensibly ``found objects,'' correlation among multiple covariates is a common issue in meta-regression.\textsuperscript{25}
Note that conditions on omitted covariates \emph{and} omitted observations must hold in order for \(\hat{\beta}_S\) to be unbiased.

When the assumptions about omitted variables and effect sizes are not met, \(\hat{\beta}_S\) will be biased.
The magnitude of the bias will depend on a number of factors, including the amount of missingness, the missingness mechanism, and the relevance of any excluded covariates.
The bias can be expressed as:
\begin{equation}
\text{Bias}[\hat{\beta}_S] = (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{UE} \beta_E + (\mathbf{X}_{US}^T \mathbf{W}_U \mathbf{X}_{US})^{-1}\mathbf{X}_{US}^T \mathbf{W}_U \Delta_{jU},
\label{eq:bias-sca}
\end{equation}
where \(\mathbf{X}_{UE}\) is the matrix of omitted covariates and \(\beta_E\) comprises the coefficients for the omitted covariates. The term \(\Delta_j\) is a vector of biases due to missingness \(\Delta_j = [\delta_{1j}, \ldots, \delta_{kj}]\) and \(\Delta_{jU}\) is the subset of \(\Delta_j\) for which \(R_i \in \mathcal{R}_j\).
Note that the \(\delta_{ij}\) are the biases due solely to missingness as in equation \eqref{eq:bias-delta}.

The expression in \eqref{eq:bias-sca} shows that a shifting-case analysis suffers from two sources of bias.
The first source, captured in the first term in \eqref{eq:bias-sca}, is a function of the coefficients for the excluded covariates \(\beta_E\), which we refer to as as omitted variable bias.
Discussion in a previous section argued that the term \emph{omitted variable bias} assumes that \(\beta\) is the target of inference in an SCA, which may or may not be the case.
If \(\beta\) is the target of inference, omitted variable bias arises even if no \(X_{iS}\) are missing, and is related to the issue of multicollinearity in linear models.
In fact, if the columns in \(\mathbf{X}_{US}\) and \(\mathbf{X}_{UE}\) are orthogonal, so that the omitted variables are independent of the included variables, then the omitted variable bias will be zero.
When the omitted variables are not orthogonal to the included variables, the bias will be nonzero, and it will depend in large part on the contribution of the omitted variables in the complete-data model \(\mathbf{X}_{UE} \beta_E\).
The estimator \(\hat{\beta}_S\) will have greater bias if the coefficients for the omitted variables \(\beta_E\) are larger and the omitted covariates \(\mathbf{X}_{UE}\) are correlated with the included covariates \(\mathbf{X}_{US}\).

The second term in \eqref{eq:bias-sca} captures the bias due to ignoring observations missing \(X_{iS}\).
This \emph{missingness bias} is a function of \(\Delta_{jU}\), which is itself a vector of biases for each effect, and it can be understood in terms of its individual components \(\delta_{ij}\).
Because the \(\delta_{ij}\) are of the same form for the complete-case and shifting-case models, the missing data bias for an SCA is governed by similar factors as the CCA, and are quite possibly similar in magnitude.
Based on \eqref{eq:conditional-bias}, \(\delta_{ij}\) will be positive if \(T_i\) is strongly correlated with whether \(R_i \in \mathcal{R}_j\), and \(\delta_{ij}\) will be greater in magnitude when that correlation is larger.

Taken together, shifting-case estimators can be even more biased than complete-case estimators.
This occurs if the omitted variable and the missingness biases are in the same direction (e.g., both are positive).
For both biases to be in the same direction, correlation between \(T_i\) and the omitted variables \(X_{iE}\) must be in the same direction as the correlation between \(T_i\) the probability that \(X_{iS}\) is observed.
If, however, the omitted variable and missingness biases are in opposite directions, this can reduce the bias of a shifting-case estimator.
It is worth noting, however, that it will almost always be impossible to confirm the direction of biases, since they depend on potentially unobserved covariates.

\hypertarget{example-shifting-cases-analysis-with-two-binary-covariates}{%
\subsection{Example: Shifting-Cases Analysis with Two Binary Covariates}\label{example-shifting-cases-analysis-with-two-binary-covariates}}

Suppose \(X_i = [1, X_{i1}, X_{i2}]\) and \(X_{i1}\) and \(X_{i2}\) are binary covariates such that
\begin{equation}
T_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + u_i + e_i
\label{eq:sca-ex}
\end{equation}
If there is missingness in both \(X_{i1}\) and \(X_{i2}\), then \(R_i \in \{0, 1\}^2\) so that \(R_i = [1,1]\) indicates both covariates are observed, and \(R_i = [1, 0]\) indicates only \(X_{i1}\) is observed.
If missingness is such that \(R_i = [1, 1]\) for very few effect estimates, then an SCA might involve regressing \(T_i\) on the observed values of \(X_{i1}\) and then on the observed values of \(X_{i2}\).

The first regression would take only rows for which \(X_{i1}\) is observed, so that \(R \in \mathcal{R}_{1} = \{[1,1], [1,0]\}\) and the excluded \(X_{i2}\) could be either 0 or 1.
The shifting-case estimators follow from equation \eqref{eq:beta-s}:
\[
\hat{\beta}_{0S} = \frac{\sum_{X_1 = 0} w_i T_i}{\sum_{X_1 = 0} w_i}, 
\qquad \hat{\beta}_{1S} = \frac{\sum_{X_1 = 1} w_i T_i}{\sum_{X_1 = 1} w_i} - \hat{\beta}_{0S}
\]

Assume that missigness follows the following log-linear model:
\begin{equation}
\text{logit}[p(R_i \in \mathcal{R}_{1} | T_i, X_{i1}, v_i)] = \psi_{0} + \psi_{1} T_i + \psi_{2} X_{i1} 
\label{eq:logit-sca}
\end{equation}
Note that this gives the log-odds that an effect is included in the model given \(T_i\) and \(X_{i1}\), and that \(X_{i2}\) is not involved.
Further, because the distribution of \(R_i\) depends on \(X_{i1}\), the mechanism is MNAR.

Given the selection model in \eqref{eq:logit-sca}, the bias of the coefficient estimators can be written as:
\begin{align}
\text{Bias}[\hat{\beta}_{0S}]
  & = \beta_2 \frac{\sum_{X_1 = 0, X_2 = 1} w_i}{\sum_{X_i = 0} w_i} + \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 0} w_i}  \\
\text{Bias}[\hat{\beta}_{1S}]
  & = \beta_2 \left(\frac{\sum_{X_{i1} = 1, X_{i2} = 1} w_i}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0, X_{i2} = 1} w_i}{\sum_{X_{i1} = 0} w_i}\right) + \left(\frac{\sum_{X_{i1} = 1} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 1} w_i} - \frac{\sum_{X_{i1} = 0} w_i \tilde{\delta}_{i1}}{\sum_{X_{i1} = 0} w_i}\right)
\end{align}
Here \(\tilde{\delta}_{i1}\) are the missingness biases as defined above, and whose approximate values is given in \eqref{eq:conditional-bias}.
To distinguish from the \(\delta_{i1}\) from the complete-case example, we use the \(\tilde{\delta}\) notation.

Both the bias of \(\hat{\beta}_{0S}\) and \(\hat{\beta}_{1S}\) depend on two terms.
The first term in each expression is the omitted variable bias, and the second term in each expression is the missingness bias.
Consider the omitted variable biases.
When effects are estimated with roughly the same precision, so that \(w_i \approx w\), then the omitted variable biases reduce to
\begin{align}
\text{Omitted Var. Bias}[\hat{\beta}_{0S}]
  & = \beta_2 p(X_2 = 1 | X_1 = 0) \label{eq:omvar-b0}\\
\text{Omitted Var. Bias}[\hat{\beta}_{1S}]
  & = \beta_2 \left[p(X_2 = 1 | X_1 = 1) - p(X_2 = 1 | X_1 = 0) \right] \label{eq:omvar-b1}
\end{align}

The omitted variable biases for each coefficient can be seen as depending on two quantities.
Both \eqref{eq:omvar-b0} and \eqref{eq:omvar-b1} are increasing in \(\beta_2\), which is the contribution of \(X_{i2}\) to the complete-data model.
The omitted variable bias for \(\hat{\beta}_{0S}\) is also increasing in \(p(X_2 = 1 | X_1 = 0)\).
The bias for \(\hat{\beta}_{1S}\) in \eqref{eq:omvar-b1} is increasing in \(p(X_2 = 1 | X_1 = 1) - p(X_2 = 1 | X_1 = 0)\).
Because both \(X_{i1}\) and \(X_{i2}\) are binary, this difference is roughly equivalent to their Pearson correlation (assuming equal marginals).
If \(X_{i1} \perp X_{i2}\), then their correlation is zero, and the omitted variable bias will be zero.
But if \(X_{i1}\) and \(X_{i2}\) are correlated, the bias of \(\hat{\beta}_1\) will depend on how strongly correlated \(X_{i1}\) and \(X_{i2}\) are, and how big \(\beta_2\) is.

Figure \ref{fig:omitted-bias} shows the omitted variable bias of \(\hat{\beta}_0\) (left plot) and \(\hat{\beta}_1\) (right plot) as a function of \(\beta_2\).
Both the bias and \(\beta_2\) are shown on the scale of Cohen's \(d\).
In the left plot \(\pi_{01} = p(X_{i2} = 1 | X_{i1} = 0)\) is the proportion of \(X_{i2} = 1\) when \(X_{i1} = 0\).
In the right plot, \(\rho_{12} = p(X_{i2} = 1 | X_{i1} = 1) - p(X_{i2} = 1 | X_{i1} = 0)\), which is roughly the correlation between \(X_{i1}\) and \(X_{i2}\).
Note that because \(\rho_{12}\) can be intuited as (roughly) a Pearson correlation, the values in the figure include 0, 0.1 (i.e., a ``small'' correlation), 0.3 (medium correlation), and 0.5 (large correlation).\textsuperscript{40}

The figure shows that if \(\beta_2 = 0\) so that \(X_{i2}\) is independent of \(T_i\) given \(X_{i1}\), that both \(\hat{\beta}_{0S}\) and \(\hat{\beta}_{1S}\) will be unbiased.
However, when \(\beta_2\) is nonzero, both estimators will be biased.
If \(X_{i1}\) and \(X_{i2}\) are highly correlated, or if \(X_{i2} = 1\) when \(X_{i1} = 0\) with high probability, the bias of both estimators will about as large as a ``small'' effect (i.e., \(d = 0.2\)) when \(\beta_2\) is larger than 0.2.
For \(\hat{\beta}_{1S}\) the bias will be less than about \(d = 0.05\) when \(|\beta_2| \leq 0.1\) or if \(\rho_{12} < 0.5\).

\begin{figure}
\begin{center}
\includegraphics[width = \textwidth]{./graphics/omitted_var_bias}
\end{center}
\caption{This figure shows the omitted variable bias of $\hat{\beta}_{0S}$ and $\hat{\beta}_{1S}$ for model (27) as a function of the omitted variable coefficient $\beta_2$. The bias ($y$-axis) and $\beta_2$ ($x$-axis) are on the scale of Cohen's $d$. The bias displayed is solely due to omitting $X_{i2}$ from model (27). In the left plot, lines are colored according to $\pi_{01} = p(X_{i2} = 1 | X_{i1} = 0)$. In the right plot, lines are colored according to $\rho_{12} = p(X_{i2} = 1 | X_{i1} = 1) - p(X_{i2} = 1 | X_{i1} = 0)$.}
\label{fig:omitted-bias}
\end{figure}

Figure \ref{fig:omitted-bias} does not take into account any bias induced by missingness.
However, because the missingness mechanism in \eqref{eq:logit-sca} is the same as the mechanism for the complete-case example \eqref{eq:cc-loglinear}, the missingness bias for \(\hat{\beta}_{0S}\) is the same as that for \(\hat{\beta}_{0C}\) in \eqref{eq:cc-bias-b0}, which is shown in Figure \ref{fig:delta}.
Likewise, the missingness bias for \(\hat{\beta}_{1S}\) is the same as that for \(\hat{\beta}_{1C}\) in \eqref{eq:cc-bias-beta1-example}, which is shown in Figure \ref{fig:b1}.

Thus, the total bias of \(\hat{\beta}_{0S}\) will be the sum of the omitted variable biases shown in Figure \ref{fig:omitted-bias} and the missingness biases shown in Figures \ref{fig:delta} and \ref{fig:b1}.
If both the omitted and missingness biases are on the higher end, the total bias of \(\hat{\beta}_0\) might be as large as \(d = 0.6\) to over 1.0.
Likewise, the total bias of \(\hat{\beta}_{1S}\) will be the sum of the omitted variable biases shown in Figure \ref{fig:b1} and the missingness biases shown in Figure \ref{fig:omitted-bias}, and can be larger than \(d = 0.6\).

As noted above, the missingness bias and omitted variable bias can be in the different directions.
For instance, if \(\beta_2 < 0\) but \(\tilde{\delta}_{ij} > 0\), then the omitted variable bias for \(\hat{\beta}_{0S}\) will be negative, but the missingness bias will be positive.
In such cases, the bias of the shifting-case estimators could be smaller than the bias of the complete-case estimators.
However, because the biases depend on unknown (and potentially unobserved) quantities, it will often be impossible to empirically verify the magnitude or direction of the bias.

\hypertarget{implications-for-empirical-example}{%
\section{Implications for Empirical Example}\label{implications-for-empirical-example}}

The theoretical results above suggest that there are conditions under which the coefficient estimates from the CCA and SCA of the substance abuse data in Table \ref{tab:ccadtexample} are substantially biased.
However, it will be difficult, if not impossible, to determine just how biased those estimates are, even given the simplified examples in the previous sections.
First, the missingness mechanism is not known for the substance abuse data.
Even if we assume that the mechanism follows a log-linear model like that in \eqref{eq:cc-loglinear} or \eqref{eq:logit-sca}, the resulting formulas for the bias depend on quantities, such as \(\psi\) and \(\eta\) that are not known, and cannot be estimated in the presence of missing data without further assumptions.

However, one approach to examining bias in the estimates presented in Table \ref{tab:ccadtexample} would involve stochastically imputing the missing \(X_{ij}\) in the data.
In the same vein as multiple imputation (MI), each set of imputed values constitutes a ``complete'' dataset from which we can compute the parameters relevant to bias.\textsuperscript{13,45}
Given an imputed dataset, we can compute (a) the difference in the resulting \(\hat{\beta}^{(i)}\) for the \(i\)th imputed dataset and \(\hat{\beta}_S\), (b) the quantities that govern bias in the formulas above, including \(\psi\), \(H(X)\), and \(\tau^2\).
This allows us not only to assess the bias, but also to examine which aspects of the missing data are driving it.

As with MI, the accuracy of the resulting estimated quantities depends on the validity of assumptions regarding missingness and the accuracy of the imputation model.
Thus, we would urge interpretation of the following results as \emph{potential} biases in the CCA and SCA estimators presented earlier in this article, rather than a precise estimate of the bias.
We generated \(m = 1,000\) imputations using the \texttt{mice} software in the \texttt{R} programming language.\textsuperscript{46}
Estimates of \(\eta\) were computed using \texttt{metafor}, specifying a Paule-Mandel estimator for the variance component \(\tau^2\).\textsuperscript{47}
To estimate the log-linear model selection parameters \(\psi\) in \eqref{eq:logit-sca}, as well as \(H(X)\), we used a logistic regression with the missingness indicator \(R_{ij}\) and \(T_i\) and \(X_{ij}\) as the predictors.

Here, we focus on results for \(\beta_0\) and \(\beta_1\).
Consider the regression of \(T_i\) on \(X_{i1}\) reported in Table \ref{tab:ccadtexample}.
We can view this as a single regression in an SCA that includes only observations for which \(X_{i1}\) is observed.
As noted above, the resulting estimators of the intercept \(\beta_0\) and slope \(\beta_1\) will exhibit bias due to missingness given in \eqref{eq:cc-bias-b0} and \eqref{eq:cc-bias-beta1-example} and bias due to omitting variables as in \eqref{eq:omvar-b0} and \eqref{eq:omvar-b1}.
Recall that the bias due to missingness in an SCA under this model will be similar to the bias derived for a CCA.

Figure \ref{fig:bias-box} plots the omitted variable bias, missingness bias, and total bias for both \(\hat{\beta}_{0S}\) and \(\hat{\beta}_{1S}\).
Results are reported on the scale of Cohen's \(d\).
Omitted variable biases for \(\hat{\beta}_{0S}\) range from -0.05 to 0.02 with a mean of -0.01; omitted variable bias for \(\hat{\beta}_{1S}\) ranges from -0.31 to 0.11 with a mean of -0.05.
Similarly the bias due to missingness could feasibly range from 0.04 to 0.08 with a mean of 0.06 for \(\hat{\beta}_{0S}\), while the missingness bias of \(\hat{\beta}_{1S}\) might range from 0 to 0.12 with a mean of 0.06.
Note that while the omitted variable bias and missingness bias are in opposite directions in this example, this need not be the case in general; both biases could feasibly be in the same direction for other data.
In sum, this amounts to a total bias of -0.01 to 0.10 for \(\hat{\beta}_{0S}\) and from -0.31 to 0.23 for \(\hat{\beta}_{1S}\).

\begin{figure}
\begin{center}
\includegraphics[width = .8\textwidth]{../../writeup/cca_paper/graphics/bias_boxplot.jpg}
\end{center}
\caption{Bias in SCA regression of $T_i$ on $X_{i1}$. For both the intercept $\beta_0$ and slope $\beta_1$, these boxplots show the total potential bias of the SCA estimators, as well as the omitted variable and missingness bias. Units are shown on the scale of Cohen's $d$.}
\label{fig:bias-box}
\end{figure}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

This article described a selection model approach to study the bias of two common methods for conducting meta-regressions with missing covariates: complete-case and shifting-case analyses.
Under certain assumptions regarding the selection model, we obtained expressions for the approximate bias of coefficient estimators.
These expressions were presented in a general form, which was then unpacked by way of examples.

We found that both CCA and SCA will produce biased coefficient estimates unless certain conditions are met.
While discussion regarding potential bias of these analyses have largely focused on traditional mechanism taxonomy of MCAR, MAR, and MNAR, we found that bias depends more on the precise model for missingness rather than these broader classifications.
Certain mechanisms that are MAR or MNAR can lead to unbiased estimates with CCA and SCA, while other MAR or MNAR mechanisms can induce substantial bias.
Complete-case estimators are unbiased if the probability that all relevant covariates are observed is (conditionally) independent of the effect size estimate.
Shifting-case estimators are unbiased if, in addition to effect sizes being independent of missingness, the covariates omitted from a model have no relationship with the effect size.
When these conditions are not met, the bias of coefficient estimates can be substantial---as large as \(d = 0.4\) to \(d = 0.8\)---depending on the missingness mechanism (i.e., parameters in the selection model), the missingness rate, an the relevance of any omitted covariates.

Results for both CCA and SCA suggest that bias due to missingness will tend to increase in magnitude as a function of the total variation in the data.
This means that if studies have small sample sizes (i.e., \(v_i\) are large) or there is substantial residual between-effect heterogeneity \(\tau^2\), the bias of a CCA or SCA will be greater.
Because meta-regression is used to explain between-effect variation \(\tau^2\), models capable of explaining much of that variation will have lower bias in CCA and SCA estimates.
However, even very modest amounts of residual variation can still result in substantial bias.

An important aspect of these findings is that bias will depend on unknown parameters and unobserved data.
This means that it will be impossible to empirically verify the magnitude or direction of the bias.
Even the estimated biases from the substance abuse data, which were on the order of about \(d = \pm 0.1\) may not be entirely accurate, as so much of that data is missing.
Further, it will require strong assumptions regarding the missingness mechanism to correct any bias.
These assumptions may be buttressed by theory about scientific reporting, data collection, and data curation.

In addition, it is not immediately clear how commonly the conditions required for unbiased complete- and shifting-case estimators arise.
Recent empirical work on examining missingness in meta-analytic datasets found that effect sizes can be strongly correlated with missingness, though this is not always the case.\textsuperscript{48}
Further, the issues of multicollinearity and confounding in meta-regression, including those discussed by Lipsey,\textsuperscript{25} would suggest that omitting variables in an SCA are likely to induce bias.

Based on these results, our primary recommendation is that analysts attempt to understand the missingness mechanisms and patterns in their data.
This can leverage knowledge about standard reporting and coding practices, as well as exploratory analyses.\textsuperscript{48}
If there is very little missingness, or if there is a good reason to assume that missingness is uncorrelated with effect size estimates, a CCA may be a reasonable option.
However, we would discourage analysts from continuing to use SCA because it would seem unlikely that omitted variable biases are zero in practice.

We would also suggest analysts investigate the feasibility of alternative estimation methods. Ibrahim\textsuperscript{32} describes an EM algorithm for generalized linear models with missing covariates, and Ibrahim, lipsitz, and Chen\textsuperscript{33} extend that algorithm when covariates are MNAR.
In addition, full-information maximum likelihood (FIML) has long been used in linear models,\textsuperscript{14,49} and has shown some promise for meta-regression involving continuous covariates.
Finally, multiple imputation has become something of a standard approach for handling missing data across a number of fields.\textsuperscript{13,30,45}

However, employing any of these alternative strategies is not necessarily straightforward for meta-analysts.
To our knowledge, the EM algorithm for missing covariates has yet to be implemented in standard meta-analytic software.
Although FIML for meta-regression model is available in SEM framework,\textsuperscript{50} the approach has not been empirically validated under various conditions.
How best to specify quality imputation models for MI analyses is something of an open question for meta-regression, as is the potential inaccuracies incurred by using poor imputation models.
Research on and clear implementation of these methods for meta-regression model would seem to be of great use for meta-analysts.

\clearpage

\hypertarget{highlights}{%
\section{Highlights}\label{highlights}}

Missing covariates are a common problem when conducting meta-regressions.
A common practice for meta-regression analyses has been to ignore effects for which covariates are missing.
However, a vast statistical literature suggests that analyses that ignore missing data can only provide accurate estimates of relevant quantitites under certain conditions.
In this article, we examine conditions under which ignoring missing covariates in a meta-regression can still lead to unbiased estimation of regression coefficients.
We also investigate the possible magnitude and sources of bias when those conditions do not hold.
Our findings highlight that substantial bias can be induced by ignoring missing data in a meta-regression.

\hypertarget{data-availability}{%
\section{Data Availability}\label{data-availability}}

The data that support the findings of this study are available from the corresponding author upon reasonable request. Note that the key findings of this article concern statistical properties of various incomplete data estimators, while the data were used for demonstration purposes only.

\clearpage

\appendix

\hypertarget{approximate-bias-for-log-linear-selection-models-1}{%
\section{Approximate Bias for Log-Linear Selection Models}\label{approximate-bias-for-log-linear-selection-models-1}}

\textbf{Proposition:} Suppose \(p(T_i | X_i, v_i)\) is the standard fixed- or random effects meta-regression model in equation \eqref{eq:full-data-prob}, and suppose \(p(R_i \not\in \mathcal{R}_j | T_i, X_i, v_i) = H_j(T_i, X_i, v_i)\) follows the log-linear model in \eqref{eq:r-loglinear}. Then:

\begin{equation}
E[T_i | X_i, v_i, R_i \in \mathcal{R}_j] \approx X_i\beta + H_j(X_i\beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional_expectation}
\end{equation}
where \(f_{mj}'(X_i \beta, X_i, v_i) = \frac{\partial}{\partial T_i} f_{mj}(T_i, X_i, v_i)\rvert_{T_i = X_i^T \beta}\).
Therefore, the bias of the conditional expectation is given by:
\begin{equation}
\delta_{ij} \approx H_j(X_i \beta, X_i, v_i)(\tau^2 + v_i)\sum_{m = 0}^{m_j} \psi_{mj} f_{mj}'(X_i \beta, X_i, v_i)
\label{eq:conditional-bias-appendix}
\end{equation}

\textbf{Proof:}

In this proof, we drop the subscript \(i\) for sake of simplicity.
Denote
\begin{align*}
H_j(X\beta, X, v) \equiv H_j(X, v) 
  & = P[R =\not\in \mathcal{R}_j | T, X, v] \rvert_{T = X\beta} \\
G_j(X, v) 
  & = 1 - H_j(X, v) \\
g_j(X, v)
  & = P[R \in \mathcal{R}_j | X, v]
\end{align*}

Then an approximation for \(E[T | X, v, R \in \mathcal{R}_j]\) is as follows:
\begin{align*}
  & \text{omitting subscripts, Taylor series for exponents in } P[R \in \mathcal{R}_j | T, X, v] \text{ at } T = X  \beta\\
E[T | X, v, R \in \mathcal{R}_j] 
  & = \int \frac{T \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij}(T, X, v)\right\}}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)} \left(1 + e^{\sum_i \psi_{ij} f_{ij}(T, X, v)}\right)} dT \\
  & = \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{(T - X\beta)^2}{2(\tau^2 + v)} + \sum_i \psi_{ij} f_{ij} (X \beta, X, v) \right. \\
  & \qquad\qquad + \sum_i \psi_{ij} f_{ij}'(X \beta, X, v)(T - X \beta) - \log\left(1 + e^{\sum_i \psi_{ij} f_{ij} (X  \beta, X, v)}\right) \\
  & \qquad\qquad \left. - G_j(X, v)(\sum_i \psi_{ij} f_{ij}'(X  \beta, X, v))(T - X\beta) + O(T^2)\right\} dT \\
  & \approx \int \frac{T}{g_j(X, v)\sqrt{2 \pi (\tau^2 + v)}} \exp\left\{-\frac{1}{2(\tau^2 + v)}\left(T^2 - 2TX\beta - 2T (\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X \beta, X, v)\right.\right. \\
  & \qquad\qquad \left.\left.+ 2T(\tau^2 + v) G_j(X, v)(\sum_{i \in \mathcal{D}} \psi_{ij} f_{ij}'(X  \beta, X, v)) + \ldots\right)\right\} dT \\
  & = X\beta + H_j(X, v)(\tau^2 + v)\sum_{i} \psi_{ij} f_{ij}'(X  \beta, X, v)  \qquad\qquad\qquad \blacksquare
\end{align*}

Note that this uses a first order Taylor expansion of the log-linear model at \(T = X \beta\), and thus assumes the \(f_{mj}\) are differentiable.
The approximation will be more accurate if \(\tau^2 + v_i\) are small.
A more accurate approximation is possible if the \(f_{mj}\) are linear in \(T_i\).
In that case, only an approximation of the denominator of the log-linear model is required.

\clearpage

\hypertarget{references}{%
\section{References}\label{references}}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}

\noindent

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\hypertarget{ref-borensteinIntroductionMetaanalysis2009}{}%
\CSLLeftMargin{1. }
\CSLRightInline{Borenstein M. \emph{Introduction to meta-analysis}. Chichester, U.K.: John Wiley \& Sons; 2009. \url{http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=427912}. Accessed May 18, 2020.}

\leavevmode\hypertarget{ref-tiptonHistoryMetaregressionTechnical2019}{}%
\CSLLeftMargin{2. }
\CSLRightInline{Tipton E, Pustejovsky JE, Ahmadi H. A history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018. \emph{Res Syn Meth}. 2019;10(2):161-179. doi:\href{https://doi.org/10.1002/jrsm.1338}{10.1002/jrsm.1338}}

\leavevmode\hypertarget{ref-berkeyRandomeffectsRegressionModel1995}{}%
\CSLLeftMargin{3. }
\CSLRightInline{Berkey CS, Hoaglin DC, Mosteller F, Colditz GA. A random-effects regression model for meta-analysis. \emph{Statist Med}. 1995;14(4):395-411. doi:\href{https://doi.org/10.1002/sim.4780140406}{10.1002/sim.4780140406}}

\leavevmode\hypertarget{ref-hedgesCombiningIndependentEstimators1983}{}%
\CSLLeftMargin{4. }
\CSLRightInline{Hedges LV. Combining independent estimators in research synthesis. \emph{British Journal of Mathematical and Statistical Psychology}. 1983;36(1):123-131. doi:\href{https://doi.org/10.1111/j.2044-8317.1983.tb00768.x}{10.1111/j.2044-8317.1983.tb00768.x}}

\leavevmode\hypertarget{ref-hedgesRobustVarianceEstimation2010}{}%
\CSLLeftMargin{5. }
\CSLRightInline{Hedges LV, Tipton E, Johnson MC. Robust variance estimation in meta-regression with dependent effect size estimates. \emph{Res Synth Method}. 2010;1(1):39-65. doi:\href{https://doi.org/10.1002/jrsm.5}{10.1002/jrsm.5}}

\leavevmode\hypertarget{ref-konstantopoulosFixedEffectsVariance2011}{}%
\CSLLeftMargin{6. }
\CSLRightInline{Konstantopoulos S. Fixed effects and variance components estimation in three-level meta-analysis: Three-level meta-analysis. \emph{Res Syn Meth}. 2011;2(1):61-76. doi:\href{https://doi.org/10.1002/jrsm.35}{10.1002/jrsm.35}}

\leavevmode\hypertarget{ref-viechtbauerAccountingHeterogeneityRandomeffects2007}{}%
\CSLLeftMargin{7. }
\CSLRightInline{Viechtbauer W. Accounting for heterogeneity via random-effects models and moderator analyses in meta-analysis. \emph{Zeitschrift fr Psychologie / Journal of Psychology}. 2007;215(2):104-121. doi:\href{https://doi.org/10.1027/0044-3409.215.2.104}{10.1027/0044-3409.215.2.104}}

\leavevmode\hypertarget{ref-pigottReviewMethodsMissing2001}{}%
\CSLLeftMargin{8. }
\CSLRightInline{Pigott TD. A review of methods for missing data. \emph{Educational Research and Evaluation}. 2001;7(4):353-383. doi:\href{https://doi.org/10.1076/edre.7.4.353.8937}{10.1076/edre.7.4.353.8937}}

\leavevmode\hypertarget{ref-pigottMissingPredictorsModels2001}{}%
\CSLLeftMargin{9. }
\CSLRightInline{Pigott TD. Missing predictors in models of effect size. \emph{Eval Health Prof}. 2001;24(3):277-307. doi:\href{https://doi.org/10.1177/01632780122034920}{10.1177/01632780122034920}}

\leavevmode\hypertarget{ref-tiptonCurrentPracticesMetaregression2019}{}%
\CSLLeftMargin{10. }
\CSLRightInline{Tipton E, Pustejovsky JE, Ahmadi H. Current practices in meta-regression in psychology, education, and medicine. \emph{Res Syn Meth}. 2019;10(2):180-194. doi:\href{https://doi.org/10.1002/jrsm.1339}{10.1002/jrsm.1339}}

\leavevmode\hypertarget{ref-pigottHandlingMissingData2019}{}%
\CSLLeftMargin{11. }
\CSLRightInline{Pigott TD. Handling missing data. In: Harris Cooper, Larry V. Hedges, Jeffrey C. Valentine, eds. \emph{The Handbook for Research Synthesis and Meta-Analysis}. 3rd ed. New York: Russell Sage; 2019.}

\leavevmode\hypertarget{ref-cooperResearchSynthesisMetaanalysis2017}{}%
\CSLLeftMargin{12. }
\CSLRightInline{Cooper HM. \emph{Research Synthesis and Meta-Analysis: A Step-by-Step Approach}. Fifth Edition. Los Angeles: SAGE; 2017.}

\leavevmode\hypertarget{ref-littleStatisticalAnalysisMissing2002}{}%
\CSLLeftMargin{13. }
\CSLRightInline{Little RJA, Rubin DB. \emph{Statistical Analysis with Missing Data}. Hoboken, NJ, USA: John Wiley \& Sons, Inc.; 2002. doi:\href{https://doi.org/10.1002/9781119013563}{10.1002/9781119013563}}

\leavevmode\hypertarget{ref-grahamMissingData2012}{}%
\CSLLeftMargin{14. }
\CSLRightInline{Graham JW. \emph{Missing Data}. New York, NY: Springer New York; 2012. doi:\href{https://doi.org/10.1007/978-1-4614-4018-5}{10.1007/978-1-4614-4018-5}}

\leavevmode\hypertarget{ref-tanner-smithAdolescentSubstanceUse2016}{}%
\CSLLeftMargin{15. }
\CSLRightInline{Tanner-Smith EE, Steinka-Fry KT, Kettrey HH, Lipsey MW. Adolescent substance use treatment effectiveness: A systematic review and meta-analysis. December 2016.}

\leavevmode\hypertarget{ref-schaferMultipleImputationPrimer1999}{}%
\CSLLeftMargin{16. }
\CSLRightInline{Schafer JL. Multiple imputation: a primer. \emph{Stat Methods Med Res}. 1999;8(1):3-15. doi:\href{https://doi.org/10.1177/096228029900800102}{10.1177/096228029900800102}}

\leavevmode\hypertarget{ref-bennettHowCanDeal2001}{}%
\CSLLeftMargin{17. }
\CSLRightInline{Bennett DA. How can I deal with missing data in my study? \emph{Aust N Z J Public Health}. 2001;25(5):464-469.}

\leavevmode\hypertarget{ref-cooperSynthesizingResearchGuide1998}{}%
\CSLLeftMargin{18. }
\CSLRightInline{Cooper HM. \emph{Synthesizing Research: A Guide for Literature Reviews}. 3rd ed. Thousand Oaks, Calif: Sage Publications; 1998.}

\leavevmode\hypertarget{ref-cooperHandbookResearchSynthesis2019}{}%
\CSLLeftMargin{19. }
\CSLRightInline{Cooper HM, Hedges LV, Valentine JC, eds. \emph{Handbook of Research Synthesis and Meta-Analysis}. 3rd edition. New York: Russell Sage Foundation; 2019.}

\leavevmode\hypertarget{ref-hedgesFixedRandomeffectsModels1998}{}%
\CSLLeftMargin{20. }
\CSLRightInline{Hedges LV, Vevea JL. Fixed- and random-effects models in meta-analysis. \emph{Psychological Methods}. 1998;3(4):486-504. doi:\href{https://doi.org/10.1037/1082-989X.3.4.486}{10.1037/1082-989X.3.4.486}}

\leavevmode\hypertarget{ref-hedgesRandomEffectsModel1983}{}%
\CSLLeftMargin{21. }
\CSLRightInline{Hedges LV. A random effects model for effect sizes. \emph{Psychological Bulletin}. 1983;93(2):388-395. doi:\href{https://doi.org/10.1037/0033-2909.93.2.388}{10.1037/0033-2909.93.2.388}}

\leavevmode\hypertarget{ref-lairdStatisticalMethodsCombining1990}{}%
\CSLLeftMargin{22. }
\CSLRightInline{Laird NM, Mosteller F. Some statistical methods for combining experimental results. \emph{Int J Technol Assess Health Care}. 1990;6(1):5-30. doi:\href{https://doi.org/10.1017/S0266462300008916}{10.1017/S0266462300008916}}

\leavevmode\hypertarget{ref-viechtbauerBiasEfficiencyMetaanalytic2005}{}%
\CSLLeftMargin{23. }
\CSLRightInline{Viechtbauer W. Bias and efficiency of meta-analytic variance estimators in the random-effects model. \emph{Journal of Educational and Behavioral Statistics}. 2005;30(3):261-293. doi:\href{https://doi.org/10.3102/10769986030003261}{10.3102/10769986030003261}}

\leavevmode\hypertarget{ref-gelmanBayesianDataAnalysis2014}{}%
\CSLLeftMargin{24. }
\CSLRightInline{Gelman A. \emph{Bayesian Data Analysis}. Third edition. Boca Raton: CRC Press; 2014.}

\leavevmode\hypertarget{ref-lipseyThoseConfoundedModerators2003}{}%
\CSLLeftMargin{25. }
\CSLRightInline{Lipsey MW. Those confounded moderators in meta-analysis: Good, bad, and ugly. \emph{The ANNALS of the American Academy of Political and Social Science}. 2003;587(1):69-81. doi:\href{https://doi.org/10.1177/0002716202250791}{10.1177/0002716202250791}}

\leavevmode\hypertarget{ref-farrarMulticollinearityRegressionAnalysis1967}{}%
\CSLLeftMargin{26. }
\CSLRightInline{Farrar DE, Glauber RR. Multicollinearity in regression analysis: The problem revisited. \emph{The Review of Economics and Statistics}. 1967;49(1):92. doi:\href{https://doi.org/10.2307/1937887}{10.2307/1937887}}

\leavevmode\hypertarget{ref-melaImpactCollinearityRegression2002}{}%
\CSLLeftMargin{27. }
\CSLRightInline{Mela CF, Kopalle PK. The impact of collinearity on regression analysis: The asymmetric effect of negative and positive correlations. \emph{Applied Economics}. 2002;34(6):667-677. doi:\href{https://doi.org/10.1080/00036840110058482}{10.1080/00036840110058482}}

\leavevmode\hypertarget{ref-angristpischke2009}{}%
\CSLLeftMargin{28. }
\CSLRightInline{Angrist JD, Pischke J-S. \emph{Mostly harmless econometrics}. Princeton, N.J.: Princeton University Press; 2009.}

\leavevmode\hypertarget{ref-rubinInferenceMissingData1976}{}%
\CSLLeftMargin{29. }
\CSLRightInline{Rubin DB. Inference and missing data. \emph{Biometrika}. 1976;63(3):581-592. doi:\href{https://doi.org/10.1093/biomet/63.3.581}{10.1093/biomet/63.3.581}}

\leavevmode\hypertarget{ref-vanbuurenFlexibleImputationMissing2018}{}%
\CSLLeftMargin{30. }
\CSLRightInline{van Buuren S. \emph{Flexible Imputation of Missing Data, Second Edition}. 2nd ed. Second edition. \textbar{} Boca Raton, Florida : CRC Press, {[}2019{]} \textbar: Chapman and Hall/CRC; 2018. doi:\href{https://doi.org/10.1201/9780429492259}{10.1201/9780429492259}}

\leavevmode\hypertarget{ref-agrestiCategoricalDataAnalysis2013}{}%
\CSLLeftMargin{31. }
\CSLRightInline{Agresti A. \emph{Categorical Data Analysis}. 3rd ed. Hoboken, NJ: Wiley; 2013.}

\leavevmode\hypertarget{ref-ibrahimIncompleteDataGeneralized1990}{}%
\CSLLeftMargin{32. }
\CSLRightInline{Ibrahim JG. Incomplete data in generalized linear models. \emph{Journal of the American Statistical Association}. 1990;85(411):765-769. doi:\href{https://doi.org/10.1080/01621459.1990.10474938}{10.1080/01621459.1990.10474938}}

\leavevmode\hypertarget{ref-ibrahimMissingCovariatesGeneralized1999}{}%
\CSLLeftMargin{33. }
\CSLRightInline{Ibrahim JG, Lipsitz SR, Chen M-H. Missing covariates in generalized linear models when the missing data mechanism is non-ignorable. \emph{J Royal Statistical Soc B}. 1999;61(1):173-190. doi:\href{https://doi.org/10.1111/1467-9868.00170}{10.1111/1467-9868.00170}}

\leavevmode\hypertarget{ref-lipsitzConditionalModelIncomplete1996}{}%
\CSLLeftMargin{34. }
\CSLRightInline{Lipsitz S. A conditional model for incomplete covariates in parametric regression models. \emph{Biometrika}. 1996;83(4):916-922. doi:\href{https://doi.org/10.1093/biomet/83.4.916}{10.1093/biomet/83.4.916}}

\leavevmode\hypertarget{ref-glynnRegressionEstimatesMissing1986}{}%
\CSLLeftMargin{35. }
\CSLRightInline{Glynn RJ, Laird NM. Regression estimates and missing data: Complete case analysis. \emph{Cambridge, MA: Harvard School of Public Health, Department of Biostatistics}. 1986.}

\leavevmode\hypertarget{ref-littleRegressionMissingReview1992}{}%
\CSLLeftMargin{36. }
\CSLRightInline{Little RJA. Regression with missing \emph{X} 's: A review. \emph{Journal of the American Statistical Association}. 1992;87(420):1227-1237. doi:\href{https://doi.org/10.1080/01621459.1992.10476282}{10.1080/01621459.1992.10476282}}

\leavevmode\hypertarget{ref-hedgesPowerStatisticalTests2001}{}%
\CSLLeftMargin{37. }
\CSLRightInline{Hedges LV, Pigott TD. The power of statistical tests in meta-analysis. \emph{Psychological Methods}. 2001;6(3):203-217. doi:\href{https://doi.org/10.1037/1082-989X.6.3.203}{10.1037/1082-989X.6.3.203}}

\leavevmode\hypertarget{ref-hedgesPowerStatisticalTests2004}{}%
\CSLLeftMargin{38. }
\CSLRightInline{Hedges LV, Pigott TD. The power of statistical tests for moderators in meta-analysis. \emph{Psychological Methods}. 2004;9(4):426-445. doi:\href{https://doi.org/10.1037/1082-989X.9.4.426}{10.1037/1082-989X.9.4.426}}

\leavevmode\hypertarget{ref-hedgesStatisticalAnalysesStudying2019}{}%
\CSLLeftMargin{39. }
\CSLRightInline{Hedges LV, Schauer JM. Statistical analyses for studying replication: Meta-analytic perspectives. \emph{Psychological Methods}. 2019;24(5):557-570. doi:\href{https://doi.org/10.1037/met0000189}{10.1037/met0000189}}

\leavevmode\hypertarget{ref-cohenStatisticalPowerAnalysis1988}{}%
\CSLLeftMargin{40. }
\CSLRightInline{Cohen J. \emph{Statistical Power Analysis for the Behavioral Sciences}. 2nd ed. Hillsdale, N.J: L. Erlbaum Associates; 1988.}

\leavevmode\hypertarget{ref-fergusonEffectSizePrimer2009}{}%
\CSLLeftMargin{41. }
\CSLRightInline{Ferguson CJ. An effect size primer: A guide for clinicians and researchers. \emph{Professional Psychology: Research and Practice}. 2009;40(5):532-538. doi:\href{https://doi.org/10.1037/a0015808}{10.1037/a0015808}}

\leavevmode\hypertarget{ref-chenHowBigBig2010}{}%
\CSLLeftMargin{42. }
\CSLRightInline{Chen H, Cohen P, Chen S. How big is a big odds ratio? Interpreting the magnitudes of odds ratios in epidemiological studies. \emph{Communications in Statistics - Simulation and Computation}. 2010;39(4):860-864. doi:\href{https://doi.org/10.1080/03610911003650383}{10.1080/03610911003650383}}

\leavevmode\hypertarget{ref-haddockUsingOddsRatios1998}{}%
\CSLLeftMargin{43. }
\CSLRightInline{Haddock CK, Rindskopf D, Shadish WR. Using odds ratios as effect sizes for meta-analysis of dichotomous data: A primer on methods and issues. \emph{Psychological Methods}. 1998;3(3):339-353. doi:\href{https://doi.org/10.1037/1082-989X.3.3.339}{10.1037/1082-989X.3.3.339}}

\leavevmode\hypertarget{ref-hillBenchmarks2008}{}%
\CSLLeftMargin{44. }
\CSLRightInline{Hill CJ, Bloom HS, Black AR, Lipsey MW. Empirical benchmarks for interpreting effect sizes in research. \emph{Child Development Perspectives}. 2008;2(3):172-177. doi:\href{https://doi.org/10.1111/j.1750-8606.2008.00061.x}{10.1111/j.1750-8606.2008.00061.x}}

\leavevmode\hypertarget{ref-rubinMultipleImputationNonresponse1987}{}%
\CSLLeftMargin{45. }
\CSLRightInline{Rubin DB. \emph{Multiple Imputation for Nonresponse in Surveys}. New York: Wiley; 1987.}

\leavevmode\hypertarget{ref-vanbuurenMiceMultivariateImputation2011}{}%
\CSLLeftMargin{46. }
\CSLRightInline{van Buuren S van, Groothuis-Oudshoorn K. \textbf{mice} : Multivariate Imputation by Chained Equations in \emph{R}. \emph{J Stat Soft}. 2011;45(3). doi:\href{https://doi.org/10.18637/jss.v045.i03}{10.18637/jss.v045.i03}}

\leavevmode\hypertarget{ref-viechtbauerConductingMetaanalysesMetafor2010}{}%
\CSLLeftMargin{47. }
\CSLRightInline{Viechtbauer W. Conducting meta-analyses in R with the metafor package. \emph{J Stat Soft}. 2010;36(3). doi:\href{https://doi.org/10.18637/jss.v036.i03}{10.18637/jss.v036.i03}}

\leavevmode\hypertarget{ref-schauerExploratoryAnalysesMissingunderreview}{}%
\CSLLeftMargin{48. }
\CSLRightInline{Schauer JM, Daz K, Pigott TD, Lee J. Exploratory analyses for missing data in meta-analyses. \emph{Alcohol and Alcoholism}. 2021:1-12. doi:\href{https://doi.org/10.1093/alcalc/agaa144}{10.1093/alcalc/agaa144}}

\leavevmode\hypertarget{ref-grahamMissingDataAnalysis2009}{}%
\CSLLeftMargin{49. }
\CSLRightInline{Graham JW. Missing data analysis: Making it work in the real world. \emph{Annu Rev Psychol}. 2009;60(1):549-576. doi:\href{https://doi.org/10.1146/annurev.psych.58.110405.085530}{10.1146/annurev.psych.58.110405.085530}}

\leavevmode\hypertarget{ref-cheungHandlingMissingCovariates2019}{}%
\CSLLeftMargin{50. }
\CSLRightInline{Cheung MW-L. Handling missing covariates in mixed-effects meta-analysis with full-information maximum likelihood. Presented at the: Society for Research Synthesis Methods; 2019; Chicago, IL. \url{http://www.srsm.org/uploads/4/6/1/3/46138157/abstract_-_mike_cheung.pdf}. Accessed September 8, 2020.}

\end{CSLReferences}

\end{document}
